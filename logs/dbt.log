[0m01:52:38.834674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019359096670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001935C08B340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001935C08BC10>]}


============================== 01:52:38.837675 | b9fe0243-fbf5-48c9-9fd4-5e4d75d0c6e6 ==============================
[0m01:52:38.837675 [info ] [MainThread]: Running with dbt=1.7.4
[0m01:52:38.838674 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt test', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:52:39.925512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b9fe0243-fbf5-48c9-9fd4-5e4d75d0c6e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019357F79850>]}
[0m01:52:39.997520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b9fe0243-fbf5-48c9-9fd4-5e4d75d0c6e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001937D205F10>]}
[0m01:52:39.998520 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m01:52:40.007520 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m01:52:40.008524 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m01:52:40.009524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b9fe0243-fbf5-48c9-9fd4-5e4d75d0c6e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001937D34A6D0>]}
[0m01:52:41.277503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b9fe0243-fbf5-48c9-9fd4-5e4d75d0c6e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001937D7A9970>]}
[0m01:52:41.293248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b9fe0243-fbf5-48c9-9fd4-5e4d75d0c6e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001937D34A190>]}
[0m01:52:41.294249 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m01:52:41.295250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9fe0243-fbf5-48c9-9fd4-5e4d75d0c6e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001937D7A9160>]}
[0m01:52:41.296261 [info ] [MainThread]: 
[0m01:52:41.297256 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (18652, 8524), compute: ``
[0m01:52:41.298260 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:52:41.298260 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (18652, 8524), compute: ``, acquire_release_count: 0, idle time: 0s
[0m01:52:41.298260 [debug] [MainThread]: Databricks adapter: Thread (18652, 8524) using default compute resource.
[0m01:52:41.300259 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (18652, 16740), compute: ``
[0m01:52:41.300259 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m01:52:41.301260 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (18652, 16740), compute: ``, acquire_release_count: 0, idle time: 0s
[0m01:52:41.301260 [debug] [ThreadPool]: Databricks adapter: Thread (18652, 16740) using default compute resource.
[0m01:52:41.305260 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m01:52:41.305260 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m01:52:41.306260 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:52:44.030065 [debug] [ThreadPool]: Databricks adapter: Error while running:
GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m01:52:44.031058 [debug] [ThreadPool]: Databricks adapter: Database Error
  HTTPSConnectionPool(host='https', port=None): Max retries exceeded with url: //adb-6516581332475033.13.azuredatabricks.net/:443/sql/protocolv1/o/6516581332475033/1219-052851-untr82n3 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001937D7CF310>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))
[0m01:52:44.032057 [debug] [ThreadPool]: Databricks adapter: Error while running:
macro list_relations_without_caching
[0m01:52:44.032057 [debug] [ThreadPool]: Databricks adapter: Runtime Error
  Database Error
    HTTPSConnectionPool(host='https', port=None): Max retries exceeded with url: //adb-6516581332475033.13.azuredatabricks.net/:443/sql/protocolv1/o/6516581332475033/1219-052851-untr82n3 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001937D7CF310>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))
[0m01:52:44.033056 [debug] [ThreadPool]: Databricks adapter: Error while retrieving information about `hive_metastore`.`saleslt`: Runtime Error
  Database Error
    HTTPSConnectionPool(host='https', port=None): Max retries exceeded with url: //adb-6516581332475033.13.azuredatabricks.net/:443/sql/protocolv1/o/6516581332475033/1219-052851-untr82n3 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001937D7CF310>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))
[0m01:52:44.034057 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (18652, 16740), compute: ``, acquire_release_count: 1, idle time: 0s
[0m01:52:44.035270 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (18652, 8524), compute: ``, acquire_release_count: 1, idle time: 0s
[0m01:52:44.036061 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:52:44.037058 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m01:52:44.037058 [debug] [MainThread]: On list_hive_metastore_saleslt: No close available on handle
[0m01:52:44.038057 [info ] [MainThread]: 
[0m01:52:44.038057 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 2.74 seconds (2.74s).
[0m01:52:44.039063 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      HTTPSConnectionPool(host='https', port=None): Max retries exceeded with url: //adb-6516581332475033.13.azuredatabricks.net/:443/sql/protocolv1/o/6516581332475033/1219-052851-untr82n3 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001937D7CF310>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))
[0m01:52:44.042060 [debug] [MainThread]: Command `dbt test` failed at 01:52:44.041060 after 5.24 seconds
[0m01:52:44.042060 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019359096670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001937D1B2C10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001937D7B97C0>]}
[0m01:52:44.043058 [debug] [MainThread]: Flushing usage events
[0m01:52:55.706645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A01A14730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A049F0370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A049F0B20>]}


============================== 01:52:55.709647 | 53b14590-f848-4acc-81db-7f031b39fca9 ==============================
[0m01:52:55.709647 [info ] [MainThread]: Running with dbt=1.7.4
[0m01:52:55.709647 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt test', 'send_anonymous_usage_stats': 'True'}
[0m01:52:56.706534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '53b14590-f848-4acc-81db-7f031b39fca9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A049F0E20>]}
[0m01:52:56.781056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '53b14590-f848-4acc-81db-7f031b39fca9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A25B75DF0>]}
[0m01:52:56.782057 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m01:52:56.790052 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m01:52:56.808516 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m01:52:56.809515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '53b14590-f848-4acc-81db-7f031b39fca9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A25C80430>]}
[0m01:52:58.159122 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '53b14590-f848-4acc-81db-7f031b39fca9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A261D20A0>]}
[0m01:52:58.172121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '53b14590-f848-4acc-81db-7f031b39fca9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A26199550>]}
[0m01:52:58.172121 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m01:52:58.173123 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '53b14590-f848-4acc-81db-7f031b39fca9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A261D2790>]}
[0m01:52:58.174123 [info ] [MainThread]: 
[0m01:52:58.175119 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (17468, 20348), compute: ``
[0m01:52:58.175119 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:52:58.176120 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (17468, 20348), compute: ``, acquire_release_count: 0, idle time: 0s
[0m01:52:58.176120 [debug] [MainThread]: Databricks adapter: Thread (17468, 20348) using default compute resource.
[0m01:52:58.177120 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (17468, 23432), compute: ``
[0m01:52:58.178123 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m01:52:58.178123 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (17468, 23432), compute: ``, acquire_release_count: 0, idle time: 0s
[0m01:52:58.178123 [debug] [ThreadPool]: Databricks adapter: Thread (17468, 23432) using default compute resource.
[0m01:52:58.182122 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m01:52:58.183123 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m01:52:58.183123 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:52:58.464159 [debug] [ThreadPool]: Databricks adapter: <class 'databricks.sql.exc.RequestError'>: Error during request to server
[0m01:52:58.465159 [debug] [ThreadPool]: Databricks adapter: attempt: 1/30
[0m01:52:58.465159 [debug] [ThreadPool]: Databricks adapter: bounded-retry-delay: None
[0m01:52:58.466159 [debug] [ThreadPool]: Databricks adapter: elapsed-seconds: 0.26503467559814453/900.0
[0m01:52:58.466159 [debug] [ThreadPool]: Databricks adapter: error-message: 
[0m01:52:58.467158 [debug] [ThreadPool]: Databricks adapter: http-code: 404
[0m01:52:58.467158 [debug] [ThreadPool]: Databricks adapter: method: OpenSession
[0m01:52:58.468158 [debug] [ThreadPool]: Databricks adapter: no-retry-reason: non-retryable error
[0m01:52:58.468158 [debug] [ThreadPool]: Databricks adapter: original-exception: 
[0m01:52:58.469158 [debug] [ThreadPool]: Databricks adapter: query-id: None
[0m01:52:58.470157 [debug] [ThreadPool]: Databricks adapter: session-id: None
[0m01:52:58.471158 [debug] [ThreadPool]: Databricks adapter: Error while running:
GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m01:52:58.472161 [debug] [ThreadPool]: Databricks adapter: Database Error
  Error during request to server
[0m01:52:58.473162 [debug] [ThreadPool]: Databricks adapter: Error while running:
macro list_relations_without_caching
[0m01:52:58.473162 [debug] [ThreadPool]: Databricks adapter: Runtime Error
  Database Error
    Error during request to server
[0m01:52:58.474162 [debug] [ThreadPool]: Databricks adapter: Error while retrieving information about `hive_metastore`.`saleslt`: Runtime Error
  Database Error
    Error during request to server
[0m01:52:58.474162 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (17468, 23432), compute: ``, acquire_release_count: 1, idle time: 0s
[0m01:52:58.475161 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (17468, 20348), compute: ``, acquire_release_count: 1, idle time: 0s
[0m01:52:58.476160 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:52:58.476160 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m01:52:58.476160 [debug] [MainThread]: On list_hive_metastore_saleslt: No close available on handle
[0m01:52:58.477159 [info ] [MainThread]: 
[0m01:52:58.477159 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.30 seconds (0.30s).
[0m01:52:58.478155 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      Error during request to server
[0m01:52:58.480156 [debug] [MainThread]: Command `dbt test` failed at 01:52:58.479157 after 2.81 seconds
[0m01:52:58.480156 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A01A14730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A25C90AF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A261E3DC0>]}
[0m01:52:58.480156 [debug] [MainThread]: Flushing usage events
[0m01:53:09.678193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A6F8E6670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A728DA340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A728DAC10>]}


============================== 01:53:09.681460 | 1fc882ed-e8d4-446a-8a21-286f33524a5c ==============================
[0m01:53:09.681460 [info ] [MainThread]: Running with dbt=1.7.4
[0m01:53:09.682460 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt test', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:53:10.662327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1fc882ed-e8d4-446a-8a21-286f33524a5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A6E7C9850>]}
[0m01:53:10.740838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1fc882ed-e8d4-446a-8a21-286f33524a5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A13BA8BB0>]}
[0m01:53:10.740838 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m01:53:10.750839 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m01:53:10.773361 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m01:53:10.774361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1fc882ed-e8d4-446a-8a21-286f33524a5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A13A6F400>]}
[0m01:53:12.050644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1fc882ed-e8d4-446a-8a21-286f33524a5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A140ACEB0>]}
[0m01:53:12.061644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1fc882ed-e8d4-446a-8a21-286f33524a5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A13BA8B80>]}
[0m01:53:12.061644 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m01:53:12.062644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1fc882ed-e8d4-446a-8a21-286f33524a5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A140AC1F0>]}
[0m01:53:12.063644 [info ] [MainThread]: 
[0m01:53:12.064644 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (5688, 2856), compute: ``
[0m01:53:12.064644 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:53:12.064644 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (5688, 2856), compute: ``, acquire_release_count: 0, idle time: 0s
[0m01:53:12.065644 [debug] [MainThread]: Databricks adapter: Thread (5688, 2856) using default compute resource.
[0m01:53:12.066644 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (5688, 6028), compute: ``
[0m01:53:12.067644 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m01:53:12.067644 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (5688, 6028), compute: ``, acquire_release_count: 0, idle time: 0s
[0m01:53:12.067644 [debug] [ThreadPool]: Databricks adapter: Thread (5688, 6028) using default compute resource.
[0m01:53:12.071644 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m01:53:12.071644 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m01:53:12.072644 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:53:12.253591 [debug] [ThreadPool]: Databricks adapter: <class 'databricks.sql.exc.RequestError'>: Error during request to server
[0m01:53:12.253591 [debug] [ThreadPool]: Databricks adapter: attempt: 1/30
[0m01:53:12.254590 [debug] [ThreadPool]: Databricks adapter: bounded-retry-delay: None
[0m01:53:12.254590 [debug] [ThreadPool]: Databricks adapter: elapsed-seconds: 0.1699211597442627/900.0
[0m01:53:12.255617 [debug] [ThreadPool]: Databricks adapter: error-message: 
[0m01:53:12.255617 [debug] [ThreadPool]: Databricks adapter: http-code: 404
[0m01:53:12.256590 [debug] [ThreadPool]: Databricks adapter: method: OpenSession
[0m01:53:12.256590 [debug] [ThreadPool]: Databricks adapter: no-retry-reason: non-retryable error
[0m01:53:12.257590 [debug] [ThreadPool]: Databricks adapter: original-exception: 
[0m01:53:12.257590 [debug] [ThreadPool]: Databricks adapter: query-id: None
[0m01:53:12.258590 [debug] [ThreadPool]: Databricks adapter: session-id: None
[0m01:53:12.258590 [debug] [ThreadPool]: Databricks adapter: Error while running:
GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m01:53:12.259590 [debug] [ThreadPool]: Databricks adapter: Database Error
  Error during request to server
[0m01:53:12.259590 [debug] [ThreadPool]: Databricks adapter: Error while running:
macro list_relations_without_caching
[0m01:53:12.260590 [debug] [ThreadPool]: Databricks adapter: Runtime Error
  Database Error
    Error during request to server
[0m01:53:12.261586 [debug] [ThreadPool]: Databricks adapter: Error while retrieving information about `hive_metastore`.`saleslt`: Runtime Error
  Database Error
    Error during request to server
[0m01:53:12.261586 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (5688, 6028), compute: ``, acquire_release_count: 1, idle time: 0s
[0m01:53:12.263617 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (5688, 2856), compute: ``, acquire_release_count: 1, idle time: 0s
[0m01:53:12.264589 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:53:12.264589 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m01:53:12.264589 [debug] [MainThread]: On list_hive_metastore_saleslt: No close available on handle
[0m01:53:12.265587 [info ] [MainThread]: 
[0m01:53:12.265587 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m01:53:12.266587 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      Error during request to server
[0m01:53:12.267585 [debug] [MainThread]: Command `dbt test` failed at 01:53:12.267585 after 2.62 seconds
[0m01:53:12.267585 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A6F8E6670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A13BCC8E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A140C4400>]}
[0m01:53:12.268585 [debug] [MainThread]: Flushing usage events
[0m01:53:37.243916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C2A905730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C2D8CD160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C2D8CDD00>]}


============================== 01:53:37.247126 | ecd39b5c-7273-4edd-94df-d3dcfcc2ae6e ==============================
[0m01:53:37.247126 [info ] [MainThread]: Running with dbt=1.7.4
[0m01:53:37.247126 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m01:53:37.248125 [info ] [MainThread]: dbt version: 1.7.4
[0m01:53:37.248125 [info ] [MainThread]: python version: 3.9.0
[0m01:53:37.249124 [info ] [MainThread]: python path: D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\Scripts\python.exe
[0m01:53:37.249124 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m01:53:38.175547 [info ] [MainThread]: Using profiles dir at C:\Users\EliteSniper\.dbt
[0m01:53:38.176554 [info ] [MainThread]: Using profiles.yml file at C:\Users\EliteSniper\.dbt\profiles.yml
[0m01:53:38.176554 [info ] [MainThread]: Using dbt_project.yml file at D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\dbt_spark_modeling\dbt_project.yml
[0m01:53:38.177553 [info ] [MainThread]: adapter type: databricks
[0m01:53:38.177553 [info ] [MainThread]: adapter version: 1.7.3
[0m01:53:38.241214 [info ] [MainThread]: Configuration:
[0m01:53:38.241712 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m01:53:38.242231 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:53:38.242231 [info ] [MainThread]: Required dependencies:
[0m01:53:38.243242 [debug] [MainThread]: Executing "git --help"
[0m01:53:38.296603 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:53:38.296603 [debug] [MainThread]: STDERR: "b''"
[0m01:53:38.297604 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:53:38.298606 [info ] [MainThread]: Connection:
[0m01:53:38.298606 [info ] [MainThread]:   host: adb-6516581332475033.13.azuredatabricks.net/
[0m01:53:38.299605 [info ] [MainThread]:   http_path: sql/protocolv1/o/6516581332475033/1219-052851-untr82n3
[0m01:53:38.300605 [info ] [MainThread]:   catalog: hive_metastore
[0m01:53:38.300605 [info ] [MainThread]:   schema: saleslt
[0m01:53:38.301605 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m01:53:38.302605 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: debug, thread: (23500, 3936), compute: ``
[0m01:53:38.303605 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m01:53:38.304112 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: debug, thread: (23500, 3936), compute: ``, acquire_release_count: 0, idle time: 0s
[0m01:53:38.304619 [debug] [MainThread]: Databricks adapter: Thread (23500, 3936) using default compute resource.
[0m01:53:38.306628 [debug] [MainThread]: Using databricks connection "debug"
[0m01:53:38.306628 [debug] [MainThread]: On debug: select 1 as id
[0m01:53:38.306628 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:53:38.497248 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.RequestError'>: Error during request to server
[0m01:53:38.498247 [debug] [MainThread]: Databricks adapter: attempt: 1/30
[0m01:53:38.498247 [debug] [MainThread]: Databricks adapter: bounded-retry-delay: None
[0m01:53:38.499248 [debug] [MainThread]: Databricks adapter: elapsed-seconds: 0.17103981971740723/900.0
[0m01:53:38.499248 [debug] [MainThread]: Databricks adapter: error-message: 
[0m01:53:38.499248 [debug] [MainThread]: Databricks adapter: http-code: 404
[0m01:53:38.500248 [debug] [MainThread]: Databricks adapter: method: OpenSession
[0m01:53:38.500248 [debug] [MainThread]: Databricks adapter: no-retry-reason: non-retryable error
[0m01:53:38.501248 [debug] [MainThread]: Databricks adapter: original-exception: 
[0m01:53:38.501248 [debug] [MainThread]: Databricks adapter: query-id: None
[0m01:53:38.502249 [debug] [MainThread]: Databricks adapter: session-id: None
[0m01:53:38.502249 [debug] [MainThread]: Databricks adapter: Error while running:
select 1 as id
[0m01:53:38.502249 [debug] [MainThread]: Databricks adapter: Database Error
  Error during request to server
[0m01:53:38.504275 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: debug, thread: (23500, 3936), compute: ``, acquire_release_count: 1, idle time: 0s
[0m01:53:38.504275 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m01:53:38.505288 [info ] [MainThread]: [31m1 check failed:[0m
[0m01:53:38.506288 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    Error during request to server

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m01:53:38.508813 [debug] [MainThread]: Command `dbt debug` failed at 01:53:38.507798 after 1.30 seconds
[0m01:53:38.509807 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m01:53:38.509807 [debug] [MainThread]: On debug: No close available on handle
[0m01:53:38.510810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C2A905730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C4EAF55E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C4EAF5A30>]}
[0m01:53:38.511809 [debug] [MainThread]: Flushing usage events
[0m01:55:16.458257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000247FAD26130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000247FDD12610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000247FDD12850>]}


============================== 01:55:16.462265 | 68fd3554-3b66-4293-bd62-a78b2ab06d8d ==============================
[0m01:55:16.462265 [info ] [MainThread]: Running with dbt=1.7.4
[0m01:55:16.463265 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt debug', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m01:55:16.463265 [info ] [MainThread]: dbt version: 1.7.4
[0m01:55:16.464264 [info ] [MainThread]: python version: 3.9.0
[0m01:55:16.465267 [info ] [MainThread]: python path: D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\Scripts\python.exe
[0m01:55:16.465267 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m01:55:17.387746 [info ] [MainThread]: Using profiles dir at C:\Users\EliteSniper\.dbt
[0m01:55:17.387746 [info ] [MainThread]: Using profiles.yml file at C:\Users\EliteSniper\.dbt\profiles.yml
[0m01:55:17.388752 [info ] [MainThread]: Using dbt_project.yml file at D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\dbt_spark_modeling\dbt_project.yml
[0m01:55:17.388752 [info ] [MainThread]: adapter type: databricks
[0m01:55:17.388752 [info ] [MainThread]: adapter version: 1.7.3
[0m01:55:17.453751 [info ] [MainThread]: Configuration:
[0m01:55:17.454748 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m01:55:17.454748 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:55:17.455751 [info ] [MainThread]: Required dependencies:
[0m01:55:17.455751 [debug] [MainThread]: Executing "git --help"
[0m01:55:17.482128 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:55:17.483130 [debug] [MainThread]: STDERR: "b''"
[0m01:55:17.483130 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:55:17.484130 [info ] [MainThread]: Connection:
[0m01:55:17.484130 [info ] [MainThread]:   host: adb-6516581332475033.13.azuredatabricks.net/
[0m01:55:17.484130 [info ] [MainThread]:   http_path: sql/protocolv1/o/6516581332475033/1219-052851-untr82n3
[0m01:55:17.485130 [info ] [MainThread]:   catalog: hive_metastore
[0m01:55:17.485130 [info ] [MainThread]:   schema: saleslt
[0m01:55:17.486129 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m01:55:17.487129 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: debug, thread: (320, 7928), compute: ``
[0m01:55:17.488128 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m01:55:17.488128 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: debug, thread: (320, 7928), compute: ``, acquire_release_count: 0, idle time: 0s
[0m01:55:17.489128 [debug] [MainThread]: Databricks adapter: Thread (320, 7928) using default compute resource.
[0m01:55:17.489128 [debug] [MainThread]: Using databricks connection "debug"
[0m01:55:17.489128 [debug] [MainThread]: On debug: select 1 as id
[0m01:55:17.490128 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:55:17.678661 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.RequestError'>: Error during request to server
[0m01:55:17.679663 [debug] [MainThread]: Databricks adapter: attempt: 1/30
[0m01:55:17.679663 [debug] [MainThread]: Databricks adapter: bounded-retry-delay: None
[0m01:55:17.680663 [debug] [MainThread]: Databricks adapter: elapsed-seconds: 0.17552852630615234/900.0
[0m01:55:17.680663 [debug] [MainThread]: Databricks adapter: error-message: 
[0m01:55:17.680663 [debug] [MainThread]: Databricks adapter: http-code: 404
[0m01:55:17.681663 [debug] [MainThread]: Databricks adapter: method: OpenSession
[0m01:55:17.681663 [debug] [MainThread]: Databricks adapter: no-retry-reason: non-retryable error
[0m01:55:17.682662 [debug] [MainThread]: Databricks adapter: original-exception: 
[0m01:55:17.682662 [debug] [MainThread]: Databricks adapter: query-id: None
[0m01:55:17.683663 [debug] [MainThread]: Databricks adapter: session-id: None
[0m01:55:17.683663 [debug] [MainThread]: Databricks adapter: Error while running:
select 1 as id
[0m01:55:17.684661 [debug] [MainThread]: Databricks adapter: Database Error
  Error during request to server
[0m01:55:17.684661 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: debug, thread: (320, 7928), compute: ``, acquire_release_count: 1, idle time: 0s
[0m01:55:17.684661 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m01:55:17.685665 [info ] [MainThread]: [31m1 check failed:[0m
[0m01:55:17.685665 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    Error during request to server

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m01:55:17.686660 [debug] [MainThread]: Command `dbt debug` failed at 01:55:17.686660 after 1.28 seconds
[0m01:55:17.687661 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m01:55:17.687661 [debug] [MainThread]: On debug: No close available on handle
[0m01:55:17.687661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000247FAD26130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002479EF01DC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002479EF010D0>]}
[0m01:55:17.688662 [debug] [MainThread]: Flushing usage events
[0m01:56:04.945149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002508AB262E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002508DB1C370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002508DB1C6A0>]}


============================== 01:56:04.948148 | 9601deae-7d40-4c64-a07e-56981e421401 ==============================
[0m01:56:04.948148 [info ] [MainThread]: Running with dbt=1.7.4
[0m01:56:04.949147 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:56:04.949147 [info ] [MainThread]: dbt version: 1.7.4
[0m01:56:04.950154 [info ] [MainThread]: python version: 3.9.0
[0m01:56:04.950154 [info ] [MainThread]: python path: D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\Scripts\python.exe
[0m01:56:04.951150 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m01:56:05.842746 [info ] [MainThread]: Using profiles dir at C:\Users\EliteSniper\.dbt
[0m01:56:05.843706 [info ] [MainThread]: Using profiles.yml file at C:\Users\EliteSniper\.dbt\profiles.yml
[0m01:56:05.844225 [info ] [MainThread]: Using dbt_project.yml file at D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\dbt_spark_modeling\dbt_project.yml
[0m01:56:05.844225 [info ] [MainThread]: adapter type: databricks
[0m01:56:05.845236 [info ] [MainThread]: adapter version: 1.7.3
[0m01:56:05.908236 [info ] [MainThread]: Configuration:
[0m01:56:05.908236 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m01:56:05.909242 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:56:05.909242 [info ] [MainThread]: Required dependencies:
[0m01:56:05.910361 [debug] [MainThread]: Executing "git --help"
[0m01:56:05.938102 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:56:05.939104 [debug] [MainThread]: STDERR: "b''"
[0m01:56:05.939104 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:56:05.939104 [info ] [MainThread]: Connection:
[0m01:56:05.940108 [info ] [MainThread]:   host: adb-6516581332475033.13.azuredatabricks.net
[0m01:56:05.940108 [info ] [MainThread]:   http_path: sql/protocolv1/o/6516581332475033/1219-052851-untr82n3
[0m01:56:05.940108 [info ] [MainThread]:   catalog: hive_metastore
[0m01:56:05.941102 [info ] [MainThread]:   schema: saleslt
[0m01:56:05.941102 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m01:56:05.942102 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: debug, thread: (22648, 12336), compute: ``
[0m01:56:05.942102 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m01:56:05.942102 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: debug, thread: (22648, 12336), compute: ``, acquire_release_count: 0, idle time: 0s
[0m01:56:05.943106 [debug] [MainThread]: Databricks adapter: Thread (22648, 12336) using default compute resource.
[0m01:56:05.943106 [debug] [MainThread]: Using databricks connection "debug"
[0m01:56:05.943106 [debug] [MainThread]: On debug: select 1 as id
[0m01:56:05.944106 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:56:08.596972 [debug] [MainThread]: SQL status: OK in 2.6500000953674316 seconds
[0m01:56:08.597988 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: debug, thread: (22648, 12336), compute: ``, acquire_release_count: 1, idle time: 0s
[0m01:56:08.598988 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m01:56:08.599982 [info ] [MainThread]: [32mAll checks passed![0m
[0m01:56:08.600988 [debug] [MainThread]: Command `dbt debug` succeeded at 01:56:08.600988 after 3.69 seconds
[0m01:56:08.601989 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m01:56:08.601989 [debug] [MainThread]: On debug: Close
[0m01:56:09.108988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002508AB262E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250AEBC51C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250AEC6FF40>]}
[0m01:56:09.110005 [debug] [MainThread]: Flushing usage events
[0m02:02:34.904780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B1EAC35520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B1EDC12850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B1EDC12760>]}


============================== 02:02:34.908816 | d918290c-a93a-4109-9d7a-ee9b5c488324 ==============================
[0m02:02:34.908816 [info ] [MainThread]: Running with dbt=1.7.4
[0m02:02:34.909816 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt snapshot', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m02:02:35.928972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd918290c-a93a-4109-9d7a-ee9b5c488324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B1E9B09970>]}
[0m02:02:36.004622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd918290c-a93a-4109-9d7a-ee9b5c488324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B18EDE8190>]}
[0m02:02:36.004622 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m02:02:36.015267 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m02:02:36.030866 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m02:02:36.031868 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m02:02:36.031868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'd918290c-a93a-4109-9d7a-ee9b5c488324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B18EDB8640>]}
[0m02:02:37.430355 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_customers' in the 'models' section of file 'models\marts\customer\dim_customer.yml'
[0m02:02:37.516106 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_products' in the 'models' section of file 'models\marts\product\dim_product.yml'
[0m02:02:37.534670 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_sales' in the 'models' section of file 'models\marts\sales\sales.yml'
[0m02:02:37.634334 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.unique_dim_customers_customer_sk.22a014df62' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m02:02:37.635330 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_customers_customer_sk.8ae5836863' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m02:02:37.635330 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_customers_customerid.209fbdda85' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m02:02:37.636333 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_customers_AddressId.86b771f63e' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m02:02:37.636839 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.unique_dim_products_product_sk.8f20ac7c5b' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m02:02:37.637982 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_products_product_sk.2a2df3e1b9' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m02:02:37.637982 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_products_product_name.991aec73f3' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m02:02:37.638897 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_products_sellstartdate.f97a265a0f' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m02:02:37.639897 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.unique_dim_sales_saleOrderID.810c5f247c' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.640896 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_saleOrderID.48ce11e7f3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.641896 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.unique_dim_sales_saleOrderDetailID.343b942405' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.643896 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_saleOrderDetailID.a60664de3a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.644896 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_orderQty.66af966596' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.644896 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_productID.cbf6d34890' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.646403 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_unitPrice.3545b5473a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.646910 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_lineTotal.d55bca27f8' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.648926 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_name.4c7b961f77' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.648926 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_productNumber.3a23a94ddd' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.650926 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_standardCost.d3f58be9a3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.650926 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_listPrice.4ee58b9e3f' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.651927 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_sellStartDate.b44c8ea118' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.652927 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_orderDate.6f6f720ec3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.653927 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_customerID.60b0993af5' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.654927 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_subTotal.bfeb62a487' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.656432 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_taxAmt.94cff67d6a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.657948 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_freight.ca13e04131' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.658259 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_totalDue.920571e023' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m02:02:37.696014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd918290c-a93a-4109-9d7a-ee9b5c488324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B18F6E30A0>]}
[0m02:02:37.711608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd918290c-a93a-4109-9d7a-ee9b5c488324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B18EF19CA0>]}
[0m02:02:37.711608 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m02:02:37.712609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd918290c-a93a-4109-9d7a-ee9b5c488324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B18F6E3880>]}
[0m02:02:37.713610 [info ] [MainThread]: 
[0m02:02:37.714611 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (8456, 5572), compute: ``
[0m02:02:37.714611 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:02:37.714611 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (8456, 5572), compute: ``, acquire_release_count: 0, idle time: 0s
[0m02:02:37.716120 [debug] [MainThread]: Databricks adapter: Thread (8456, 5572) using default compute resource.
[0m02:02:37.717129 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore, thread: (8456, 17896), compute: ``
[0m02:02:37.718266 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m02:02:37.718591 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore, thread: (8456, 17896), compute: ``, acquire_release_count: 0, idle time: 0s
[0m02:02:37.718591 [debug] [ThreadPool]: Databricks adapter: Thread (8456, 17896) using default compute resource.
[0m02:02:37.718591 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m02:02:37.718591 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m02:02:37.719590 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:02:47.352043 [debug] [ThreadPool]: SQL status: OK in 9.630000114440918 seconds
[0m02:02:47.360667 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore, thread: (8456, 17896), compute: ``, acquire_release_count: 1, idle time: 0s
[0m02:02:47.362667 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_snapshots)
[0m02:02:47.363667 [debug] [ThreadPool]: Databricks adapter: Reusing DatabricksDBTConnection. name: create_hive_metastore_snapshots, thread: (8456, 17896), compute: ``, acquire_release_count: 0, idle time: 0.002001047134399414s
[0m02:02:47.364179 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: create_hive_metastore_snapshots, thread: (8456, 17896), compute: ``, acquire_release_count: 0, idle time: 0.0025119781494140625s
[0m02:02:47.364683 [debug] [ThreadPool]: Databricks adapter: Thread (8456, 17896) using default compute resource.
[0m02:02:47.364683 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: create_hive_metastore_snapshots, thread: (8456, 17896), compute: ``, acquire_release_count: 1, idle time: 0.003016948699951172s
[0m02:02:47.365688 [debug] [ThreadPool]: Databricks adapter: Thread (8456, 17896) using default compute resource.
[0m02:02:47.365688 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "snapshots"
"
[0m02:02:47.375775 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m02:02:47.377073 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_snapshots"
[0m02:02:47.377073 [debug] [ThreadPool]: On create_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "create_hive_metastore_snapshots"} */
create schema if not exists `hive_metastore`.`snapshots`
  
[0m02:02:47.789040 [debug] [ThreadPool]: SQL status: OK in 0.4099999964237213 seconds
[0m02:02:47.791041 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m02:02:47.791041 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: create_hive_metastore_snapshots, thread: (8456, 17896), compute: ``, acquire_release_count: 2, idle time: 0.42937493324279785s
[0m02:02:47.793043 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: create_hive_metastore_snapshots, thread: (8456, 17896), compute: ``, acquire_release_count: 1, idle time: 0.43137669563293457s
[0m02:02:47.794548 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (8456, 15300), compute: ``
[0m02:02:47.795973 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m02:02:47.795973 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (8456, 15300), compute: ``, acquire_release_count: 0, idle time: 0s
[0m02:02:47.795973 [debug] [ThreadPool]: Databricks adapter: Thread (8456, 15300) using default compute resource.
[0m02:02:47.800972 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m02:02:47.800972 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m02:02:47.800972 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:02:48.400743 [debug] [ThreadPool]: SQL status: OK in 0.6000000238418579 seconds
[0m02:02:48.411992 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m02:02:48.412995 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m02:02:48.412995 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m02:02:48.797195 [debug] [ThreadPool]: SQL status: OK in 0.3799999952316284 seconds
[0m02:02:48.803196 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m02:02:48.804701 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m02:02:49.104418 [debug] [ThreadPool]: SQL status: OK in 0.30000001192092896 seconds
[0m02:02:49.114534 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m02:02:49.115476 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show table extended in `hive_metastore`.`saleslt` like '*'
  
[0m02:02:49.729904 [debug] [ThreadPool]: SQL status: OK in 0.6100000143051147 seconds
[0m02:02:49.732414 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (8456, 15300), compute: ``, acquire_release_count: 1, idle time: 0s
[0m02:02:49.733919 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m02:02:49.733919 [debug] [ThreadPool]: Databricks adapter: Reusing DatabricksDBTConnection. name: list_hive_metastore_snapshots, thread: (8456, 15300), compute: ``, acquire_release_count: 0, idle time: 0.0015053749084472656s
[0m02:02:49.734928 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_snapshots, thread: (8456, 15300), compute: ``, acquire_release_count: 0, idle time: 0.002513408660888672s
[0m02:02:49.735434 [debug] [ThreadPool]: Databricks adapter: Thread (8456, 15300) using default compute resource.
[0m02:02:49.738450 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m02:02:49.739451 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m02:02:49.914589 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m02:02:49.916599 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_snapshots, thread: (8456, 15300), compute: ``, acquire_release_count: 1, idle time: 0.18418455123901367s
[0m02:02:49.917598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd918290c-a93a-4109-9d7a-ee9b5c488324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B18F603B80>]}
[0m02:02:49.918601 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:02:49.918601 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:02:49.918601 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (8456, 5572), compute: ``, acquire_release_count: 1, idle time: 0s
[0m02:02:49.919600 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:02:49.919600 [info ] [MainThread]: 
[0m02:02:49.930037 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.address_snapshot
[0m02:02:49.930037 [info ] [Thread-1  ]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m02:02:49.932555 [debug] [Thread-1  ]: Databricks adapter: Creating DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.address_snapshot, thread: (8456, 4852), compute: ``
[0m02:02:49.932555 [debug] [Thread-1  ]: Acquiring new databricks connection 'snapshot.dbt_spark_modeling.address_snapshot'
[0m02:02:49.932555 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.address_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0s
[0m02:02:49.932555 [debug] [Thread-1  ]: Databricks adapter: On thread (8456, 4852): `hive_metastore`.`snapshots`.`address_snapshot` using default compute resource.
[0m02:02:49.934059 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.address_snapshot
[0m02:02:49.942920 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.address_snapshot (compile): 02:02:49.934059 => 02:02:49.941923
[0m02:02:49.942920 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.address_snapshot
[0m02:02:50.035885 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.dbt_spark_modeling.address_snapshot"
[0m02:02:50.036884 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m02:02:50.037885 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.address_snapshot"
[0m02:02:50.037885 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.address_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`address_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/address/address_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) sbq



  
      
[0m02:02:50.037885 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m02:03:05.369590 [debug] [Thread-1  ]: SQL status: OK in 15.329999923706055 seconds
[0m02:03:05.500547 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m02:03:05.501550 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.address_snapshot (execute): 02:02:49.943920 => 02:03:05.501550
[0m02:03:05.501550 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.address_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 1, idle time: 0s
[0m02:03:05.502548 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.address_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m02:03:05.502548 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd918290c-a93a-4109-9d7a-ee9b5c488324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B18F806EB0>]}
[0m02:03:05.502548 [info ] [Thread-1  ]: 1 of 7 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 15.57s]
[0m02:03:05.504569 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.address_snapshot
[0m02:03:05.504569 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.customer_snapshot
[0m02:03:05.505575 [info ] [Thread-1  ]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m02:03:05.505980 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.address_snapshot, now snapshot.dbt_spark_modeling.customer_snapshot)
[0m02:03:05.506576 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.customer_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.0040283203125s
[0m02:03:05.507665 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.customer_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.0040283203125s
[0m02:03:05.507665 [debug] [Thread-1  ]: Databricks adapter: On thread (8456, 4852): `hive_metastore`.`snapshots`.`customer_snapshot` using default compute resource.
[0m02:03:05.508593 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.customer_snapshot
[0m02:03:05.515611 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.customer_snapshot (compile): 02:03:05.508593 => 02:03:05.515611
[0m02:03:05.516608 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.customer_snapshot
[0m02:03:05.526715 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.dbt_spark_modeling.customer_snapshot"
[0m02:03:05.527720 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customer_snapshot"
[0m02:03:05.528719 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customer_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`customer_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/customer_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) sbq



  
      
[0m02:03:10.551156 [debug] [Thread-1  ]: SQL status: OK in 5.019999980926514 seconds
[0m02:03:10.553660 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m02:03:10.554664 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.customer_snapshot (execute): 02:03:05.516608 => 02:03:10.554664
[0m02:03:10.554664 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.customer_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 1, idle time: 5.052116870880127s
[0m02:03:10.555686 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.customer_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.0010211467742919922s
[0m02:03:10.555686 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd918290c-a93a-4109-9d7a-ee9b5c488324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B18F874040>]}
[0m02:03:10.556686 [info ] [Thread-1  ]: 2 of 7 OK snapshotted snapshots.customer_snapshot .............................. [[32mOK[0m in 5.05s]
[0m02:03:10.558688 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.customer_snapshot
[0m02:03:10.558688 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.customeraddress_snapshot
[0m02:03:10.559844 [info ] [Thread-1  ]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m02:03:10.560687 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.customer_snapshot, now snapshot.dbt_spark_modeling.customeraddress_snapshot)
[0m02:03:10.561686 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.customeraddress_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.0060002803802490234s
[0m02:03:10.561686 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.customeraddress_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.0060002803802490234s
[0m02:03:10.562684 [debug] [Thread-1  ]: Databricks adapter: On thread (8456, 4852): `hive_metastore`.`snapshots`.`customeraddress_snapshot` using default compute resource.
[0m02:03:10.562684 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.customeraddress_snapshot
[0m02:03:10.566020 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.customeraddress_snapshot (compile): 02:03:10.562684 => 02:03:10.566020
[0m02:03:10.566700 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.customeraddress_snapshot
[0m02:03:10.572700 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.dbt_spark_modeling.customeraddress_snapshot"
[0m02:03:10.572700 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customeraddress_snapshot"
[0m02:03:10.573701 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customeraddress_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customeraddress/customeraddress_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) sbq



  
      
[0m02:03:14.352974 [debug] [Thread-1  ]: SQL status: OK in 3.7799999713897705 seconds
[0m02:03:14.354972 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m02:03:14.355977 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.customeraddress_snapshot (execute): 02:03:10.566700 => 02:03:14.355977
[0m02:03:14.355977 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.customeraddress_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 1, idle time: 3.8002915382385254s
[0m02:03:14.358096 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.customeraddress_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m02:03:14.359010 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd918290c-a93a-4109-9d7a-ee9b5c488324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B1908A3CD0>]}
[0m02:03:14.359979 [info ] [Thread-1  ]: 3 of 7 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 3.80s]
[0m02:03:14.360979 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.customeraddress_snapshot
[0m02:03:14.361976 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.product_snapshot
[0m02:03:14.361976 [info ] [Thread-1  ]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m02:03:14.362977 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.customeraddress_snapshot, now snapshot.dbt_spark_modeling.product_snapshot)
[0m02:03:14.362977 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.product_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.004880428314208984s
[0m02:03:14.363977 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.product_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.005880832672119141s
[0m02:03:14.364482 [debug] [Thread-1  ]: Databricks adapter: On thread (8456, 4852): `hive_metastore`.`snapshots`.`product_snapshot` using default compute resource.
[0m02:03:14.364482 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.product_snapshot
[0m02:03:14.367964 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.product_snapshot (compile): 02:03:14.364482 => 02:03:14.367964
[0m02:03:14.367964 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.product_snapshot
[0m02:03:14.373966 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.dbt_spark_modeling.product_snapshot"
[0m02:03:14.376682 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.product_snapshot"
[0m02:03:14.377696 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.product_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`product_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/product/product_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(ProductID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) sbq



  
      
[0m02:03:18.688783 [debug] [Thread-1  ]: SQL status: OK in 4.309999942779541 seconds
[0m02:03:18.690810 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m02:03:18.692785 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.product_snapshot (execute): 02:03:14.367964 => 02:03:18.692785
[0m02:03:18.693783 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.product_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 1, idle time: 4.334688901901245s
[0m02:03:18.694715 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.product_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.0009317398071289062s
[0m02:03:18.694715 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd918290c-a93a-4109-9d7a-ee9b5c488324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B1908D6520>]}
[0m02:03:18.695721 [info ] [Thread-1  ]: 4 of 7 OK snapshotted snapshots.product_snapshot ............................... [[32mOK[0m in 4.33s]
[0m02:03:18.696722 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.product_snapshot
[0m02:03:18.696722 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.productmodel_snapshot
[0m02:03:18.696722 [info ] [Thread-1  ]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m02:03:18.697720 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.product_snapshot, now snapshot.dbt_spark_modeling.productmodel_snapshot)
[0m02:03:18.698721 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.productmodel_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.0030050277709960938s
[0m02:03:18.698721 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.productmodel_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.004006147384643555s
[0m02:03:18.698721 [debug] [Thread-1  ]: Databricks adapter: On thread (8456, 4852): `hive_metastore`.`snapshots`.`productmodel_snapshot` using default compute resource.
[0m02:03:18.699720 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.productmodel_snapshot
[0m02:03:18.703720 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.productmodel_snapshot (compile): 02:03:18.699720 => 02:03:18.702721
[0m02:03:18.704226 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.productmodel_snapshot
[0m02:03:18.710736 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.dbt_spark_modeling.productmodel_snapshot"
[0m02:03:18.712738 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productmodel_snapshot"
[0m02:03:18.712738 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productmodel_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`productmodel_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/productmodel/productmodel_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) sbq



  
      
[0m02:03:22.337872 [debug] [Thread-1  ]: SQL status: OK in 3.630000114440918 seconds
[0m02:03:22.342396 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m02:03:22.343902 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.productmodel_snapshot (execute): 02:03:18.704226 => 02:03:22.343902
[0m02:03:22.344912 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.productmodel_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 1, idle time: 3.6491873264312744s
[0m02:03:22.344912 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.productmodel_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m02:03:22.345986 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd918290c-a93a-4109-9d7a-ee9b5c488324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B18F874040>]}
[0m02:03:22.346178 [info ] [Thread-1  ]: 5 of 7 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 3.65s]
[0m02:03:22.346984 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.productmodel_snapshot
[0m02:03:22.347983 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.salesorderdetail_snapshot
[0m02:03:22.347983 [info ] [Thread-1  ]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m02:03:22.348983 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.productmodel_snapshot, now snapshot.dbt_spark_modeling.salesorderdetail_snapshot)
[0m02:03:22.349987 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.salesorderdetail_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.004071474075317383s
[0m02:03:22.349987 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.salesorderdetail_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.0050754547119140625s
[0m02:03:22.349987 [debug] [Thread-1  ]: Databricks adapter: On thread (8456, 4852): `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` using default compute resource.
[0m02:03:22.349987 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.salesorderdetail_snapshot
[0m02:03:22.354485 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.salesorderdetail_snapshot (compile): 02:03:22.351190 => 02:03:22.353981
[0m02:03:22.354485 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.salesorderdetail_snapshot
[0m02:03:22.362492 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"
[0m02:03:22.367865 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"
[0m02:03:22.368879 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/salesorderdetail/salesorderdetail_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) sbq



  
      
[0m02:03:25.715667 [debug] [Thread-1  ]: SQL status: OK in 3.3499999046325684 seconds
[0m02:03:25.716671 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m02:03:25.717671 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.salesorderdetail_snapshot (execute): 02:03:22.354485 => 02:03:25.717671
[0m02:03:25.718671 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.salesorderdetail_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 1, idle time: 3.3737590312957764s
[0m02:03:25.718671 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.salesorderdetail_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m02:03:25.719671 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd918290c-a93a-4109-9d7a-ee9b5c488324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B18F823070>]}
[0m02:03:25.719671 [info ] [Thread-1  ]: 6 of 7 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 3.37s]
[0m02:03:25.720673 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.salesorderdetail_snapshot
[0m02:03:25.721674 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.salesorderheader_snapshot
[0m02:03:25.721674 [info ] [Thread-1  ]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m02:03:25.722673 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.salesorderdetail_snapshot, now snapshot.dbt_spark_modeling.salesorderheader_snapshot)
[0m02:03:25.723674 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.salesorderheader_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.005002737045288086s
[0m02:03:25.724179 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.salesorderheader_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.0055086612701416016s
[0m02:03:25.724684 [debug] [Thread-1  ]: Databricks adapter: On thread (8456, 4852): `hive_metastore`.`snapshots`.`salesorderheader_snapshot` using default compute resource.
[0m02:03:25.725691 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.salesorderheader_snapshot
[0m02:03:25.728697 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.salesorderheader_snapshot (compile): 02:03:25.725691 => 02:03:25.728697
[0m02:03:25.728697 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.salesorderheader_snapshot
[0m02:03:25.734710 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.dbt_spark_modeling.salesorderheader_snapshot"
[0m02:03:25.736146 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderheader_snapshot"
[0m02:03:25.736723 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderheader_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/salesorderheader/salesorderheader_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) sbq



  
      
[0m02:03:29.578639 [debug] [Thread-1  ]: SQL status: OK in 3.8399999141693115 seconds
[0m02:03:29.581639 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m02:03:29.582639 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.salesorderheader_snapshot (execute): 02:03:25.729698 => 02:03:29.582639
[0m02:03:29.583639 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.salesorderheader_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 1, idle time: 3.8649682998657227s
[0m02:03:29.584144 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.salesorderheader_snapshot, thread: (8456, 4852), compute: ``, acquire_release_count: 0, idle time: 0.0005047321319580078s
[0m02:03:29.584648 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd918290c-a93a-4109-9d7a-ee9b5c488324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B1908CB580>]}
[0m02:03:29.584648 [info ] [Thread-1  ]: 7 of 7 OK snapshotted snapshots.salesorderheader_snapshot ...................... [[32mOK[0m in 3.86s]
[0m02:03:29.585657 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.salesorderheader_snapshot
[0m02:03:29.586653 [debug] [MainThread]: Databricks adapter: Reusing DatabricksDBTConnection. name: master, thread: (8456, 5572), compute: ``, acquire_release_count: 0, idle time: 39.668051958084106s
[0m02:03:29.586653 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (8456, 5572), compute: ``, acquire_release_count: 0, idle time: 39.668051958084106s
[0m02:03:29.587653 [debug] [MainThread]: Databricks adapter: Thread (8456, 5572) using default compute resource.
[0m02:03:29.587653 [debug] [MainThread]: On master: ROLLBACK
[0m02:03:29.587653 [debug] [MainThread]: Opening a new connection, currently in state init
[0m02:03:29.822519 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m02:03:29.822519 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:03:29.822519 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:03:29.824025 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (8456, 5572), compute: ``, acquire_release_count: 1, idle time: 39.905423164367676s
[0m02:03:29.824542 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:03:29.825816 [debug] [MainThread]: On master: ROLLBACK
[0m02:03:29.825816 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m02:03:29.826823 [debug] [MainThread]: On master: Close
[0m02:03:29.904613 [debug] [MainThread]: Connection 'create_hive_metastore_snapshots' was properly closed.
[0m02:03:29.905618 [debug] [MainThread]: On create_hive_metastore_snapshots: ROLLBACK
[0m02:03:29.905618 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m02:03:29.905618 [debug] [MainThread]: On create_hive_metastore_snapshots: Close
[0m02:03:29.987621 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m02:03:29.988620 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m02:03:29.989619 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m02:03:29.989619 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m02:03:30.084616 [debug] [MainThread]: Connection 'snapshot.dbt_spark_modeling.salesorderheader_snapshot' was properly closed.
[0m02:03:30.085139 [debug] [MainThread]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: ROLLBACK
[0m02:03:30.086155 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m02:03:30.086155 [debug] [MainThread]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: Close
[0m02:03:30.189778 [info ] [MainThread]: 
[0m02:03:30.192778 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 52.47 seconds (52.47s).
[0m02:03:30.195967 [debug] [MainThread]: Command end result
[0m02:03:30.219859 [info ] [MainThread]: 
[0m02:03:30.220859 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:03:30.222858 [info ] [MainThread]: 
[0m02:03:30.223859 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m02:03:30.227888 [debug] [MainThread]: Command `dbt snapshot` succeeded at 02:03:30.226888 after 55.36 seconds
[0m02:03:30.227888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B1EAC35520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B18F806EB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B18F6F07F0>]}
[0m02:03:30.228888 [debug] [MainThread]: Flushing usage events
[0m02:04:40.617051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001615CF56820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001615FF34B50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001615FF349D0>]}


============================== 02:04:40.620112 | 61461eb3-6987-457a-811b-4410cac59ef9 ==============================
[0m02:04:40.620112 [info ] [MainThread]: Running with dbt=1.7.4
[0m02:04:40.621110 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt test', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m02:04:41.691050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '61461eb3-6987-457a-811b-4410cac59ef9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001615BE398B0>]}
[0m02:04:41.782999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '61461eb3-6987-457a-811b-4410cac59ef9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016101063EE0>]}
[0m02:04:41.782999 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m02:04:41.797029 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m02:04:41.910451 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:04:41.910451 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:04:41.916451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '61461eb3-6987-457a-811b-4410cac59ef9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000161014B00D0>]}
[0m02:04:41.930099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '61461eb3-6987-457a-811b-4410cac59ef9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016101485F70>]}
[0m02:04:41.930099 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m02:04:41.931099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '61461eb3-6987-457a-811b-4410cac59ef9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000161010B4490>]}
[0m02:04:41.932099 [info ] [MainThread]: 
[0m02:04:41.933099 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m02:04:41.933099 [debug] [MainThread]: Command end result
[0m02:04:41.942114 [debug] [MainThread]: Command `dbt test` succeeded at 02:04:41.942114 after 1.38 seconds
[0m02:04:41.942114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001615CF56820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016101129280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001610147BC70>]}
[0m02:04:41.943113 [debug] [MainThread]: Flushing usage events
[0m02:04:59.992741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4133A5730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4163810A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B416381B20>]}


============================== 02:04:59.995745 | be1d76c6-ca31-4e04-8ad9-594d545fd880 ==============================
[0m02:04:59.995745 [info ] [MainThread]: Running with dbt=1.7.4
[0m02:04:59.995745 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m02:05:00.995759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'be1d76c6-ca31-4e04-8ad9-594d545fd880', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B412279940>]}
[0m02:05:01.084291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'be1d76c6-ca31-4e04-8ad9-594d545fd880', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B437583AF0>]}
[0m02:05:01.085298 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m02:05:01.100781 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m02:05:01.183293 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:05:01.183293 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:05:01.189292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'be1d76c6-ca31-4e04-8ad9-594d545fd880', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4378F00D0>]}
[0m02:05:01.202298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'be1d76c6-ca31-4e04-8ad9-594d545fd880', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4378C1D60>]}
[0m02:05:01.202298 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m02:05:01.203301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'be1d76c6-ca31-4e04-8ad9-594d545fd880', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B43764B850>]}
[0m02:05:01.204302 [info ] [MainThread]: 
[0m02:05:01.205298 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (15716, 18156), compute: ``
[0m02:05:01.205298 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:05:01.205298 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (15716, 18156), compute: ``, acquire_release_count: 0, idle time: 0s
[0m02:05:01.206297 [debug] [MainThread]: Databricks adapter: Thread (15716, 18156) using default compute resource.
[0m02:05:01.207643 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore, thread: (15716, 17452), compute: ``
[0m02:05:01.207643 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m02:05:01.208650 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore, thread: (15716, 17452), compute: ``, acquire_release_count: 0, idle time: 0s
[0m02:05:01.208650 [debug] [ThreadPool]: Databricks adapter: Thread (15716, 17452) using default compute resource.
[0m02:05:01.208650 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m02:05:01.209649 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m02:05:01.209649 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:05:01.913186 [debug] [ThreadPool]: SQL status: OK in 0.699999988079071 seconds
[0m02:05:01.917182 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore, thread: (15716, 17452), compute: ``, acquire_release_count: 1, idle time: 0s
[0m02:05:01.920181 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (15716, 21532), compute: ``
[0m02:05:01.920181 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m02:05:01.921179 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (15716, 21532), compute: ``, acquire_release_count: 0, idle time: 0s
[0m02:05:01.922179 [debug] [ThreadPool]: Databricks adapter: Thread (15716, 21532) using default compute resource.
[0m02:05:01.926178 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m02:05:01.926178 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m02:05:01.926178 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:05:02.309897 [debug] [ThreadPool]: SQL status: OK in 0.3799999952316284 seconds
[0m02:05:02.327631 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m02:05:02.327631 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m02:05:02.328629 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m02:05:02.520698 [debug] [ThreadPool]: SQL status: OK in 0.1899999976158142 seconds
[0m02:05:02.529697 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m02:05:02.530698 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m02:05:02.756552 [debug] [ThreadPool]: SQL status: OK in 0.23000000417232513 seconds
[0m02:05:02.766551 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m02:05:02.767551 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show table extended in `hive_metastore`.`saleslt` like '*'
  
[0m02:05:03.347655 [debug] [ThreadPool]: SQL status: OK in 0.5799999833106995 seconds
[0m02:05:03.351651 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (15716, 21532), compute: ``, acquire_release_count: 1, idle time: 0s
[0m02:05:03.352651 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m02:05:03.356653 [debug] [ThreadPool]: Databricks adapter: Reusing DatabricksDBTConnection. name: list_hive_metastore_snapshots, thread: (15716, 21532), compute: ``, acquire_release_count: 0, idle time: 0.004006862640380859s
[0m02:05:03.356653 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_snapshots, thread: (15716, 21532), compute: ``, acquire_release_count: 0, idle time: 0.005002021789550781s
[0m02:05:03.356653 [debug] [ThreadPool]: Databricks adapter: Thread (15716, 21532) using default compute resource.
[0m02:05:03.359650 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m02:05:03.359650 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m02:05:03.494676 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m02:05:03.501687 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m02:05:03.501687 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m02:05:03.636308 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m02:05:03.643306 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m02:05:03.644311 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m02:05:03.863571 [debug] [ThreadPool]: SQL status: OK in 0.2199999988079071 seconds
[0m02:05:03.869567 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m02:05:03.870569 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show table extended in `hive_metastore`.`snapshots` like '*'
  
[0m02:05:04.706326 [debug] [ThreadPool]: SQL status: OK in 0.8299999833106995 seconds
[0m02:05:04.710319 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_snapshots, thread: (15716, 21532), compute: ``, acquire_release_count: 1, idle time: 1.3576698303222656s
[0m02:05:04.713331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'be1d76c6-ca31-4e04-8ad9-594d545fd880', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4378C1D00>]}
[0m02:05:04.713331 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:05:04.713331 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:05:04.714844 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (15716, 18156), compute: ``, acquire_release_count: 1, idle time: 0s
[0m02:05:04.714844 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:05:04.715857 [info ] [MainThread]: 
[0m02:05:04.718853 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.dim_customer
[0m02:05:04.719853 [info ] [Thread-1  ]: 1 of 3 START sql table model saleslt.dim_customer .............................. [RUN]
[0m02:05:04.720852 [debug] [Thread-1  ]: Databricks adapter: Creating DatabricksDBTConnection. name: model.dbt_spark_modeling.dim_customer, thread: (15716, 20268), compute: ``
[0m02:05:04.720852 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbt_spark_modeling.dim_customer'
[0m02:05:04.720852 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.dim_customer, thread: (15716, 20268), compute: ``, acquire_release_count: 0, idle time: 0s
[0m02:05:04.721852 [debug] [Thread-1  ]: Databricks adapter: On thread (15716, 20268): `hive_metastore`.`saleslt`.`dim_customer` using default compute resource.
[0m02:05:04.721852 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.dim_customer
[0m02:05:04.729851 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.dim_customer"
[0m02:05:04.731853 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_customer (compile): 02:05:04.721852 => 02:05:04.730851
[0m02:05:04.731853 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.dim_customer
[0m02:05:04.777854 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.dim_customer"
[0m02:05:04.778856 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m02:05:04.779853 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_customer"
[0m02:05:04.779853 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_customer: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
)

, transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select *
from transformed
  
[0m02:05:04.780855 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m02:05:10.980487 [debug] [Thread-1  ]: SQL status: OK in 6.199999809265137 seconds
[0m02:05:11.097450 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_customer (execute): 02:05:04.731853 => 02:05:11.096448
[0m02:05:11.097450 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_customer, thread: (15716, 20268), compute: ``, acquire_release_count: 1, idle time: 0s
[0m02:05:11.098450 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_customer, thread: (15716, 20268), compute: ``, acquire_release_count: 0, idle time: 0.0009996891021728516s
[0m02:05:11.098450 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'be1d76c6-ca31-4e04-8ad9-594d545fd880', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4379D4AF0>]}
[0m02:05:11.099450 [info ] [Thread-1  ]: 1 of 3 OK created sql table model saleslt.dim_customer ......................... [[32mOK[0m in 6.38s]
[0m02:05:11.099450 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.dim_customer
[0m02:05:11.100450 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.dim_product
[0m02:05:11.100450 [info ] [Thread-1  ]: 2 of 3 START sql table model saleslt.dim_product ............................... [RUN]
[0m02:05:11.101449 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.dbt_spark_modeling.dim_customer, now model.dbt_spark_modeling.dim_product)
[0m02:05:11.101449 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: model.dbt_spark_modeling.dim_product, thread: (15716, 20268), compute: ``, acquire_release_count: 0, idle time: 0.0029997825622558594s
[0m02:05:11.102453 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.dim_product, thread: (15716, 20268), compute: ``, acquire_release_count: 0, idle time: 0.0029997825622558594s
[0m02:05:11.102453 [debug] [Thread-1  ]: Databricks adapter: On thread (15716, 20268): `hive_metastore`.`saleslt`.`dim_product` using default compute resource.
[0m02:05:11.102453 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.dim_product
[0m02:05:11.106452 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.dim_product"
[0m02:05:11.107680 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (compile): 02:05:11.102453 => 02:05:11.107680
[0m02:05:11.108688 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.dim_product
[0m02:05:11.187201 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.dim_product"
[0m02:05:11.188199 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_product"
[0m02:05:11.189201 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_product: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  
[0m02:05:14.755454 [debug] [Thread-1  ]: SQL status: OK in 3.569999933242798 seconds
[0m02:05:14.759454 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (execute): 02:05:11.108688 => 02:05:14.759454
[0m02:05:14.760457 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (15716, 20268), compute: ``, acquire_release_count: 1, idle time: 3.6620075702667236s
[0m02:05:14.762456 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (15716, 20268), compute: ``, acquire_release_count: 0, idle time: 0.0010013580322265625s
[0m02:05:14.763454 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'be1d76c6-ca31-4e04-8ad9-594d545fd880', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B437A9D070>]}
[0m02:05:14.764455 [info ] [Thread-1  ]: 2 of 3 OK created sql table model saleslt.dim_product .......................... [[32mOK[0m in 3.66s]
[0m02:05:14.766453 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.dim_product
[0m02:05:14.766453 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.sales
[0m02:05:14.767450 [info ] [Thread-1  ]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m02:05:14.768454 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.dbt_spark_modeling.dim_product, now model.dbt_spark_modeling.sales)
[0m02:05:14.768454 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: model.dbt_spark_modeling.sales, thread: (15716, 20268), compute: ``, acquire_release_count: 0, idle time: 0.0059986114501953125s
[0m02:05:14.768454 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.sales, thread: (15716, 20268), compute: ``, acquire_release_count: 0, idle time: 0.0059986114501953125s
[0m02:05:14.769454 [debug] [Thread-1  ]: Databricks adapter: On thread (15716, 20268): `hive_metastore`.`saleslt`.`sales` using default compute resource.
[0m02:05:14.769454 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.sales
[0m02:05:14.772451 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.sales"
[0m02:05:14.774450 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.sales (compile): 02:05:14.769454 => 02:05:14.773453
[0m02:05:14.774450 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.sales
[0m02:05:14.779451 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.sales"
[0m02:05:14.780451 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.sales"
[0m02:05:14.780451 [debug] [Thread-1  ]: On model.dbt_spark_modeling.sales: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
[0m02:05:18.346381 [debug] [Thread-1  ]: SQL status: OK in 3.559999942779541 seconds
[0m02:05:18.351379 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.sales (execute): 02:05:14.774450 => 02:05:18.350380
[0m02:05:18.352380 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.sales, thread: (15716, 20268), compute: ``, acquire_release_count: 1, idle time: 3.5889229774475098s
[0m02:05:18.353380 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.sales, thread: (15716, 20268), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m02:05:18.353380 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'be1d76c6-ca31-4e04-8ad9-594d545fd880', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B437983E50>]}
[0m02:05:18.354380 [info ] [Thread-1  ]: 3 of 3 OK created sql table model saleslt.sales ................................ [[32mOK[0m in 3.59s]
[0m02:05:18.356381 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.sales
[0m02:05:18.358380 [debug] [MainThread]: Databricks adapter: Reusing DatabricksDBTConnection. name: master, thread: (15716, 18156), compute: ``, acquire_release_count: 0, idle time: 13.642534255981445s
[0m02:05:18.358380 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (15716, 18156), compute: ``, acquire_release_count: 0, idle time: 13.643536806106567s
[0m02:05:18.359380 [debug] [MainThread]: Databricks adapter: Thread (15716, 18156) using default compute resource.
[0m02:05:18.359380 [debug] [MainThread]: On master: ROLLBACK
[0m02:05:18.360380 [debug] [MainThread]: Opening a new connection, currently in state init
[0m02:05:18.595108 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m02:05:18.595108 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:05:18.596119 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:05:18.596119 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (15716, 18156), compute: ``, acquire_release_count: 1, idle time: 13.881275653839111s
[0m02:05:18.597116 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:05:18.598116 [debug] [MainThread]: On master: ROLLBACK
[0m02:05:18.598116 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m02:05:18.599116 [debug] [MainThread]: On master: Close
[0m02:05:18.689238 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m02:05:18.690234 [debug] [MainThread]: On list_hive_metastore: Close
[0m02:05:18.812150 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m02:05:18.813168 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m02:05:18.813168 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m02:05:18.814168 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m02:05:18.906301 [debug] [MainThread]: Connection 'model.dbt_spark_modeling.sales' was properly closed.
[0m02:05:18.906301 [debug] [MainThread]: On model.dbt_spark_modeling.sales: ROLLBACK
[0m02:05:18.907302 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m02:05:18.908298 [debug] [MainThread]: On model.dbt_spark_modeling.sales: Close
[0m02:05:19.008627 [info ] [MainThread]: 
[0m02:05:19.009628 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 17.80 seconds (17.80s).
[0m02:05:19.010633 [debug] [MainThread]: Command end result
[0m02:05:19.023637 [info ] [MainThread]: 
[0m02:05:19.024640 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:05:19.024640 [info ] [MainThread]: 
[0m02:05:19.025637 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m02:05:19.026637 [debug] [MainThread]: Command `dbt run` succeeded at 02:05:19.026637 after 19.07 seconds
[0m02:05:19.026637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4133A5730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4378C1D60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4378C1FA0>]}
[0m02:05:19.026637 [debug] [MainThread]: Flushing usage events
[0m02:05:26.096657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A184A76820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A187A5F460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A187A5F070>]}


============================== 02:05:26.099657 | df2046c9-3c7f-434e-99df-ab97fb3afb1f ==============================
[0m02:05:26.099657 [info ] [MainThread]: Running with dbt=1.7.4
[0m02:05:26.100657 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt docs generate', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m02:05:27.117727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'df2046c9-3c7f-434e-99df-ab97fb3afb1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A1839598B0>]}
[0m02:05:27.196239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'df2046c9-3c7f-434e-99df-ab97fb3afb1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A1A8B4B460>]}
[0m02:05:27.197239 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m02:05:27.208239 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m02:05:27.297753 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:05:27.298753 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:05:27.304753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'df2046c9-3c7f-434e-99df-ab97fb3afb1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A1A8FD00D0>]}
[0m02:05:27.308753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'df2046c9-3c7f-434e-99df-ab97fb3afb1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A1A8CCEE20>]}
[0m02:05:27.308753 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m02:05:27.309752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'df2046c9-3c7f-434e-99df-ab97fb3afb1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A1A8CCE700>]}
[0m02:05:27.311753 [info ] [MainThread]: 
[0m02:05:27.312753 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (13456, 18020), compute: ``
[0m02:05:27.312753 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:05:27.313753 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (13456, 18020), compute: ``, acquire_release_count: 0, idle time: 0s
[0m02:05:27.313753 [debug] [MainThread]: Databricks adapter: Thread (13456, 18020) using default compute resource.
[0m02:05:27.315753 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (13456, 23172), compute: ``
[0m02:05:27.315753 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m02:05:27.315753 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (13456, 23172), compute: ``, acquire_release_count: 0, idle time: 0s
[0m02:05:27.316753 [debug] [ThreadPool]: Databricks adapter: Thread (13456, 23172) using default compute resource.
[0m02:05:27.320756 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m02:05:27.320756 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m02:05:27.320756 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:05:27.731070 [debug] [ThreadPool]: SQL status: OK in 0.4099999964237213 seconds
[0m02:05:27.744586 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m02:05:27.745588 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m02:05:27.745588 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m02:05:27.929346 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m02:05:27.939865 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m02:05:27.940860 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m02:05:28.150018 [debug] [ThreadPool]: SQL status: OK in 0.20999999344348907 seconds
[0m02:05:28.160017 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m02:05:28.161016 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show table extended in `hive_metastore`.`saleslt` like '*'
  
[0m02:05:29.772642 [debug] [ThreadPool]: SQL status: OK in 1.6100000143051147 seconds
[0m02:05:29.777647 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (13456, 23172), compute: ``, acquire_release_count: 1, idle time: 0s
[0m02:05:29.778644 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m02:05:29.783647 [debug] [ThreadPool]: Databricks adapter: Reusing DatabricksDBTConnection. name: list_hive_metastore_snapshots, thread: (13456, 23172), compute: ``, acquire_release_count: 0, idle time: 0.0060002803802490234s
[0m02:05:29.783647 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_snapshots, thread: (13456, 23172), compute: ``, acquire_release_count: 0, idle time: 0.0060002803802490234s
[0m02:05:29.784643 [debug] [ThreadPool]: Databricks adapter: Thread (13456, 23172) using default compute resource.
[0m02:05:29.786639 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m02:05:29.787640 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m02:05:29.925832 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m02:05:29.932832 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m02:05:29.933833 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m02:05:30.071708 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m02:05:30.077705 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m02:05:30.077705 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m02:05:30.306245 [debug] [ThreadPool]: SQL status: OK in 0.23000000417232513 seconds
[0m02:05:30.314245 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m02:05:30.314245 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show table extended in `hive_metastore`.`snapshots` like '*'
  
[0m02:05:31.027082 [debug] [ThreadPool]: SQL status: OK in 0.7099999785423279 seconds
[0m02:05:31.032084 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_snapshots, thread: (13456, 23172), compute: ``, acquire_release_count: 1, idle time: 1.2544372081756592s
[0m02:05:31.035598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'df2046c9-3c7f-434e-99df-ab97fb3afb1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A1A8FD00A0>]}
[0m02:05:31.035598 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (13456, 18020), compute: ``, acquire_release_count: 1, idle time: 0s
[0m02:05:31.036610 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:05:31.037657 [info ] [MainThread]: 
[0m02:05:31.040607 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.address_snapshot
[0m02:05:31.041608 [debug] [Thread-1  ]: Databricks adapter: Creating DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.address_snapshot, thread: (13456, 4580), compute: ``
[0m02:05:31.041608 [debug] [Thread-1  ]: Acquiring new databricks connection 'snapshot.dbt_spark_modeling.address_snapshot'
[0m02:05:31.041608 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.address_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0s
[0m02:05:31.042609 [debug] [Thread-1  ]: Databricks adapter: On thread (13456, 4580): `hive_metastore`.`snapshots`.`address_snapshot` using default compute resource.
[0m02:05:31.042609 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.address_snapshot
[0m02:05:31.052610 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.address_snapshot (compile): 02:05:31.042609 => 02:05:31.051611
[0m02:05:31.052610 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.address_snapshot
[0m02:05:31.053610 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.address_snapshot (execute): 02:05:31.052610 => 02:05:31.052610
[0m02:05:31.053610 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.address_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 1, idle time: 0s
[0m02:05:31.053610 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.address_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m02:05:31.054610 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.address_snapshot
[0m02:05:31.054610 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.customer_snapshot
[0m02:05:31.055607 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.address_snapshot, now snapshot.dbt_spark_modeling.customer_snapshot)
[0m02:05:31.056606 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.customer_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0009963512420654297s
[0m02:05:31.056606 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.customer_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0019958019256591797s
[0m02:05:31.057608 [debug] [Thread-1  ]: Databricks adapter: On thread (13456, 4580): `hive_metastore`.`snapshots`.`customer_snapshot` using default compute resource.
[0m02:05:31.057608 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.customer_snapshot
[0m02:05:31.061609 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.customer_snapshot (compile): 02:05:31.057608 => 02:05:31.061609
[0m02:05:31.062609 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.customer_snapshot
[0m02:05:31.062609 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.customer_snapshot (execute): 02:05:31.062609 => 02:05:31.062609
[0m02:05:31.063606 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.customer_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 1, idle time: 0.007998466491699219s
[0m02:05:31.063606 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.customer_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m02:05:31.064727 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.customer_snapshot
[0m02:05:31.064727 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.customeraddress_snapshot
[0m02:05:31.065607 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.customer_snapshot, now snapshot.dbt_spark_modeling.customeraddress_snapshot)
[0m02:05:31.065607 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.customeraddress_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0008807182312011719s
[0m02:05:31.066607 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.customeraddress_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0018808841705322266s
[0m02:05:31.066607 [debug] [Thread-1  ]: Databricks adapter: On thread (13456, 4580): `hive_metastore`.`snapshots`.`customeraddress_snapshot` using default compute resource.
[0m02:05:31.066607 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.customeraddress_snapshot
[0m02:05:31.071760 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.customeraddress_snapshot (compile): 02:05:31.066607 => 02:05:31.070605
[0m02:05:31.071760 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.customeraddress_snapshot
[0m02:05:31.071760 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.customeraddress_snapshot (execute): 02:05:31.071760 => 02:05:31.071760
[0m02:05:31.072610 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.customeraddress_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 1, idle time: 0.007883787155151367s
[0m02:05:31.072610 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.customeraddress_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m02:05:31.073611 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.customeraddress_snapshot
[0m02:05:31.073611 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.product_snapshot
[0m02:05:31.074608 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.customeraddress_snapshot, now snapshot.dbt_spark_modeling.product_snapshot)
[0m02:05:31.074608 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.product_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0009970664978027344s
[0m02:05:31.075610 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.product_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0009970664978027344s
[0m02:05:31.075610 [debug] [Thread-1  ]: Databricks adapter: On thread (13456, 4580): `hive_metastore`.`snapshots`.`product_snapshot` using default compute resource.
[0m02:05:31.076610 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.product_snapshot
[0m02:05:31.081610 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.product_snapshot (compile): 02:05:31.077607 => 02:05:31.080609
[0m02:05:31.081610 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.product_snapshot
[0m02:05:31.081610 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.product_snapshot (execute): 02:05:31.081610 => 02:05:31.081610
[0m02:05:31.082610 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.product_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 1, idle time: 0.008999824523925781s
[0m02:05:31.082610 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.product_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m02:05:31.083610 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.product_snapshot
[0m02:05:31.083610 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.productmodel_snapshot
[0m02:05:31.084606 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.product_snapshot, now snapshot.dbt_spark_modeling.productmodel_snapshot)
[0m02:05:31.084606 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.productmodel_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0009953975677490234s
[0m02:05:31.085607 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.productmodel_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.001996278762817383s
[0m02:05:31.085607 [debug] [Thread-1  ]: Databricks adapter: On thread (13456, 4580): `hive_metastore`.`snapshots`.`productmodel_snapshot` using default compute resource.
[0m02:05:31.085607 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.productmodel_snapshot
[0m02:05:31.088606 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.productmodel_snapshot (compile): 02:05:31.086607 => 02:05:31.088606
[0m02:05:31.089607 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.productmodel_snapshot
[0m02:05:31.089607 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.productmodel_snapshot (execute): 02:05:31.089607 => 02:05:31.089607
[0m02:05:31.089607 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.productmodel_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 1, idle time: 0.0059969425201416016s
[0m02:05:31.090607 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.productmodel_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m02:05:31.091607 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.productmodel_snapshot
[0m02:05:31.091607 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.salesorderdetail_snapshot
[0m02:05:31.092607 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.productmodel_snapshot, now snapshot.dbt_spark_modeling.salesorderdetail_snapshot)
[0m02:05:31.093132 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.salesorderdetail_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0025243759155273438s
[0m02:05:31.093654 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.salesorderdetail_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0025243759155273438s
[0m02:05:31.093654 [debug] [Thread-1  ]: Databricks adapter: On thread (13456, 4580): `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` using default compute resource.
[0m02:05:31.093654 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.salesorderdetail_snapshot
[0m02:05:31.096661 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.salesorderdetail_snapshot (compile): 02:05:31.094662 => 02:05:31.096661
[0m02:05:31.097662 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.salesorderdetail_snapshot
[0m02:05:31.097662 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.salesorderdetail_snapshot (execute): 02:05:31.097662 => 02:05:31.097662
[0m02:05:31.098662 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.salesorderdetail_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 1, idle time: 0.007055044174194336s
[0m02:05:31.098662 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.salesorderdetail_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m02:05:31.099662 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.salesorderdetail_snapshot
[0m02:05:31.099662 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.salesorderheader_snapshot
[0m02:05:31.100663 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.salesorderdetail_snapshot, now snapshot.dbt_spark_modeling.salesorderheader_snapshot)
[0m02:05:31.100663 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.salesorderheader_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.002001523971557617s
[0m02:05:31.100663 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.salesorderheader_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.002001523971557617s
[0m02:05:31.101665 [debug] [Thread-1  ]: Databricks adapter: On thread (13456, 4580): `hive_metastore`.`snapshots`.`salesorderheader_snapshot` using default compute resource.
[0m02:05:31.101665 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.salesorderheader_snapshot
[0m02:05:31.104664 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.salesorderheader_snapshot (compile): 02:05:31.101665 => 02:05:31.104664
[0m02:05:31.104664 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.salesorderheader_snapshot
[0m02:05:31.105664 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.salesorderheader_snapshot (execute): 02:05:31.105664 => 02:05:31.105664
[0m02:05:31.105664 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.salesorderheader_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 1, idle time: 0.007002115249633789s
[0m02:05:31.106661 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.salesorderheader_snapshot, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0009965896606445312s
[0m02:05:31.106661 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.salesorderheader_snapshot
[0m02:05:31.107665 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.dim_customer
[0m02:05:31.108663 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.salesorderheader_snapshot, now model.dbt_spark_modeling.dim_customer)
[0m02:05:31.108663 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: model.dbt_spark_modeling.dim_customer, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.002002716064453125s
[0m02:05:31.108663 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.dim_customer, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.002002716064453125s
[0m02:05:31.109665 [debug] [Thread-1  ]: Databricks adapter: On thread (13456, 4580): `hive_metastore`.`saleslt`.`dim_customer` using default compute resource.
[0m02:05:31.109665 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.dim_customer
[0m02:05:31.112664 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.dim_customer"
[0m02:05:31.113660 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_customer (compile): 02:05:31.109665 => 02:05:31.113660
[0m02:05:31.114664 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.dim_customer
[0m02:05:31.114664 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_customer (execute): 02:05:31.114664 => 02:05:31.114664
[0m02:05:31.115664 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_customer, thread: (13456, 4580), compute: ``, acquire_release_count: 1, idle time: 0.009002685546875s
[0m02:05:31.115664 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_customer, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m02:05:31.116662 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.dim_customer
[0m02:05:31.116662 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.dim_product
[0m02:05:31.117661 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.dbt_spark_modeling.dim_customer, now model.dbt_spark_modeling.dim_product)
[0m02:05:31.117661 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: model.dbt_spark_modeling.dim_product, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0009987354278564453s
[0m02:05:31.117661 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.dim_product, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0009987354278564453s
[0m02:05:31.118662 [debug] [Thread-1  ]: Databricks adapter: On thread (13456, 4580): `hive_metastore`.`saleslt`.`dim_product` using default compute resource.
[0m02:05:31.118662 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.dim_product
[0m02:05:31.122675 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.dim_product"
[0m02:05:31.123672 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (compile): 02:05:31.118662 => 02:05:31.123672
[0m02:05:31.123672 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.dim_product
[0m02:05:31.124676 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (execute): 02:05:31.123672 => 02:05:31.123672
[0m02:05:31.124676 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (13456, 4580), compute: ``, acquire_release_count: 1, idle time: 0.008013248443603516s
[0m02:05:31.125676 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m02:05:31.125676 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.dim_product
[0m02:05:31.126674 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.sales
[0m02:05:31.126674 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.dbt_spark_modeling.dim_product, now model.dbt_spark_modeling.sales)
[0m02:05:31.127674 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: model.dbt_spark_modeling.sales, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.000997781753540039s
[0m02:05:31.127674 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.sales, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0019979476928710938s
[0m02:05:31.127674 [debug] [Thread-1  ]: Databricks adapter: On thread (13456, 4580): `hive_metastore`.`saleslt`.`sales` using default compute resource.
[0m02:05:31.128673 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.sales
[0m02:05:31.131675 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.sales"
[0m02:05:31.132675 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.sales (compile): 02:05:31.128673 => 02:05:31.132675
[0m02:05:31.132675 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.sales
[0m02:05:31.133673 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.sales (execute): 02:05:31.132675 => 02:05:31.132675
[0m02:05:31.133673 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.sales, thread: (13456, 4580), compute: ``, acquire_release_count: 1, idle time: 0.007996559143066406s
[0m02:05:31.134672 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.sales, thread: (13456, 4580), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m02:05:31.134672 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.sales
[0m02:05:31.135671 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:05:31.135671 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m02:05:31.135671 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m02:05:31.135671 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m02:05:31.137056 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m02:05:31.208482 [debug] [MainThread]: Connection 'model.dbt_spark_modeling.sales' was properly closed.
[0m02:05:31.211506 [debug] [MainThread]: Command end result
[0m02:05:31.228511 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: generate_catalog, thread: (13456, 18020), compute: ``
[0m02:05:31.229510 [debug] [MainThread]: Acquiring new databricks connection 'generate_catalog'
[0m02:05:31.230509 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: generate_catalog, thread: (13456, 18020), compute: ``, acquire_release_count: 0, idle time: 0s
[0m02:05:31.230509 [debug] [MainThread]: Databricks adapter: Thread (13456, 18020) using default compute resource.
[0m02:05:31.230509 [info ] [MainThread]: Building catalog
[0m02:05:31.234678 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: hive_metastore, thread: (13456, 6184), compute: ``
[0m02:05:31.235508 [debug] [ThreadPool]: Acquiring new databricks connection 'hive_metastore'
[0m02:05:31.236019 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: hive_metastore, thread: (13456, 6184), compute: ``, acquire_release_count: 0, idle time: 0s
[0m02:05:31.236019 [debug] [ThreadPool]: Databricks adapter: Thread (13456, 6184) using default compute resource.
[0m02:05:31.238028 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m02:05:31.238028 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:31.239029 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */

      select current_catalog()
  
[0m02:05:31.239029 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:05:31.740930 [debug] [ThreadPool]: SQL status: OK in 0.5 seconds
[0m02:05:31.747932 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:31.747932 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
show table extended in `hive_metastore`.`snapshots` like 'salesorderheader_snapshot|address_snapshot|productmodel_snapshot|salesorderdetail_snapshot|customer_snapshot|customeraddress_snapshot|product_snapshot'
  
[0m02:05:32.452932 [debug] [ThreadPool]: SQL status: OK in 0.699999988079071 seconds
[0m02:05:32.457926 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`snapshots`.`address_snapshot`
[0m02:05:32.465927 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:32.465927 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m02:05:32.849370 [debug] [ThreadPool]: SQL status: OK in 0.3799999952316284 seconds
[0m02:05:32.853375 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`snapshots`.`customer_snapshot`
[0m02:05:32.856370 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:32.857374 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m02:05:33.094747 [debug] [ThreadPool]: SQL status: OK in 0.23999999463558197 seconds
[0m02:05:33.098748 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`snapshots`.`customeraddress_snapshot`
[0m02:05:33.101749 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:33.102750 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m02:05:33.320192 [debug] [ThreadPool]: SQL status: OK in 0.2199999988079071 seconds
[0m02:05:33.325203 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`snapshots`.`product_snapshot`
[0m02:05:33.329199 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:33.330205 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m02:05:33.552042 [debug] [ThreadPool]: SQL status: OK in 0.2199999988079071 seconds
[0m02:05:33.556044 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`snapshots`.`productmodel_snapshot`
[0m02:05:33.560047 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:33.560047 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m02:05:33.785298 [debug] [ThreadPool]: SQL status: OK in 0.2199999988079071 seconds
[0m02:05:33.789301 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
[0m02:05:33.793298 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:33.794299 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m02:05:34.024303 [debug] [ThreadPool]: SQL status: OK in 0.23000000417232513 seconds
[0m02:05:34.028302 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
[0m02:05:34.103812 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:34.104811 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m02:05:34.337561 [debug] [ThreadPool]: SQL status: OK in 0.23000000417232513 seconds
[0m02:05:34.345563 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: hive_metastore, thread: (13456, 6184), compute: ``, acquire_release_count: 1, idle time: 0s
[0m02:05:34.347563 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: hive_metastore, thread: (13456, 6184), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m02:05:34.347563 [debug] [ThreadPool]: Databricks adapter: Thread (13456, 6184) using default compute resource.
[0m02:05:34.350559 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:34.350559 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */

      select current_catalog()
  
[0m02:05:34.479209 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m02:05:34.485206 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:34.486213 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
show table extended in `hive_metastore`.`saleslt` like 'productdescription|productmodel|dim_customer|customeraddress|productcategory|customer|sales|product|address|salesorderheader|dim_product|salesorderdetail'
  
[0m02:05:35.201975 [debug] [ThreadPool]: SQL status: OK in 0.7200000286102295 seconds
[0m02:05:35.205975 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`address`
[0m02:05:35.208977 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:35.209975 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`address`
  
[0m02:05:35.689460 [debug] [ThreadPool]: SQL status: OK in 0.47999998927116394 seconds
[0m02:05:35.692462 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`customer`
[0m02:05:35.696458 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:35.697466 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`customer`
  
[0m02:05:35.934408 [debug] [ThreadPool]: SQL status: OK in 0.23999999463558197 seconds
[0m02:05:35.937930 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`customeraddress`
[0m02:05:35.940927 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:35.941929 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`customeraddress`
  
[0m02:05:36.430878 [debug] [ThreadPool]: SQL status: OK in 0.49000000953674316 seconds
[0m02:05:36.433873 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`dim_customer`
[0m02:05:36.439397 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:36.439397 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`dim_customer`
  
[0m02:05:36.664996 [debug] [ThreadPool]: SQL status: OK in 0.2199999988079071 seconds
[0m02:05:36.668999 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`dim_product`
[0m02:05:36.671997 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:36.671997 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`dim_product`
  
[0m02:05:36.871385 [debug] [ThreadPool]: SQL status: OK in 0.20000000298023224 seconds
[0m02:05:36.875385 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`product`
[0m02:05:36.878384 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:36.879385 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`product`
  
[0m02:05:37.093504 [debug] [ThreadPool]: SQL status: OK in 0.20999999344348907 seconds
[0m02:05:37.097505 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`productcategory`
[0m02:05:37.100504 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:37.101509 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`productcategory`
  
[0m02:05:37.303908 [debug] [ThreadPool]: SQL status: OK in 0.20000000298023224 seconds
[0m02:05:37.306917 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`productdescription`
[0m02:05:37.310908 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:37.310908 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`productdescription`
  
[0m02:05:37.529971 [debug] [ThreadPool]: SQL status: OK in 0.2199999988079071 seconds
[0m02:05:37.533973 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`productmodel`
[0m02:05:37.537491 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:37.537491 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`productmodel`
  
[0m02:05:37.746157 [debug] [ThreadPool]: SQL status: OK in 0.20999999344348907 seconds
[0m02:05:37.750153 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`sales`
[0m02:05:37.754158 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:37.754158 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`sales`
  
[0m02:05:37.949788 [debug] [ThreadPool]: SQL status: OK in 0.20000000298023224 seconds
[0m02:05:37.955782 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`salesorderdetail`
[0m02:05:37.958787 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:37.959791 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`salesorderdetail`
  
[0m02:05:38.155833 [debug] [ThreadPool]: SQL status: OK in 0.20000000298023224 seconds
[0m02:05:38.159828 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`salesorderheader`
[0m02:05:38.162833 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m02:05:38.163836 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`salesorderheader`
  
[0m02:05:38.356813 [debug] [ThreadPool]: SQL status: OK in 0.1899999976158142 seconds
[0m02:05:38.365815 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: hive_metastore, thread: (13456, 6184), compute: ``, acquire_release_count: 1, idle time: 4.0182435512542725s
[0m02:05:38.375809 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: generate_catalog, thread: (13456, 18020), compute: ``, acquire_release_count: 1, idle time: 0s
[0m02:05:38.388811 [info ] [MainThread]: Catalog written to D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\dbt_spark_modeling\target\catalog.json
[0m02:05:38.389806 [debug] [MainThread]: Command `dbt docs generate` succeeded at 02:05:38.389806 after 12.33 seconds
[0m02:05:38.390808 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m02:05:38.390808 [debug] [MainThread]: Connection 'hive_metastore' was properly closed.
[0m02:05:38.390808 [debug] [MainThread]: On hive_metastore: ROLLBACK
[0m02:05:38.391807 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m02:05:38.391807 [debug] [MainThread]: On hive_metastore: Close
[0m02:05:38.469009 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A184A76820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A1839598B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A1A908A190>]}
[0m02:05:38.469009 [debug] [MainThread]: Flushing usage events
[0m02:05:45.135686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189A1A26A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189A4A0EB20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189A4A43AF0>]}


============================== 02:05:45.138685 | 3d7357a1-548a-46a9-a1e4-3350267d492a ==============================
[0m02:05:45.138685 [info ] [MainThread]: Running with dbt=1.7.4
[0m02:05:45.138685 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt docs serve', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m02:05:46.216327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3d7357a1-548a-46a9-a1e4-3350267d492a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189A4A43310>]}
[0m02:05:46.304856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3d7357a1-548a-46a9-a1e4-3350267d492a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189C5C1EDC0>]}
[0m02:07:35.454664 [error] [MainThread]: Encountered an error:

[0m02:07:35.482973 [error] [MainThread]: Traceback (most recent call last):
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\cli\requires.py", line 90, in wrapper
    result, success = func(*args, **kwargs)
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\cli\requires.py", line 75, in wrapper
    return func(*args, **kwargs)
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\cli\requires.py", line 168, in wrapper
    return func(*args, **kwargs)
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\cli\requires.py", line 197, in wrapper
    return func(*args, **kwargs)
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\cli\requires.py", line 244, in wrapper
    return func(*args, **kwargs)
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\cli\main.py", line 324, in docs_serve
    results = task.run()
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\task\serve.py", line 28, in run
    httpd.serve_forever()
  File "C:\Program Files\Python39\lib\socketserver.py", line 232, in serve_forever
    ready = selector.select(poll_interval)
  File "C:\Program Files\Python39\lib\selectors.py", line 324, in select
    r, w, _ = self._select(self._readers, self._writers, [], timeout)
  File "C:\Program Files\Python39\lib\selectors.py", line 315, in _select
    r, w, x = select.select(r, w, w, timeout)
KeyboardInterrupt

[0m02:07:35.488974 [debug] [MainThread]: Command `dbt docs serve` failed at 02:07:35.486974 after 110.39 seconds
[0m02:07:35.489974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189A1A26A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189C5C96610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189C5BE9E20>]}
[0m02:07:35.490975 [debug] [MainThread]: Flushing usage events
[0m12:43:33.346196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022373806670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223767F71C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223767F7190>]}


============================== 12:43:33.351197 | e28e6d51-86af-4eda-a2ac-5d8640644e5b ==============================
[0m12:43:33.351197 [info ] [MainThread]: Running with dbt=1.7.4
[0m12:43:33.352196 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt debug', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:43:33.353195 [info ] [MainThread]: dbt version: 1.7.4
[0m12:43:33.354197 [info ] [MainThread]: python version: 3.9.0
[0m12:43:33.355198 [info ] [MainThread]: python path: D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\Scripts\python.exe
[0m12:43:33.356196 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m12:43:34.925378 [info ] [MainThread]: Using profiles dir at C:\Users\EliteSniper\.dbt
[0m12:43:34.926378 [info ] [MainThread]: Using profiles.yml file at C:\Users\EliteSniper\.dbt\profiles.yml
[0m12:43:34.927388 [info ] [MainThread]: Using dbt_project.yml file at D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\dbt_spark_modeling\dbt_project.yml
[0m12:43:34.928383 [info ] [MainThread]: adapter type: databricks
[0m12:43:34.929379 [info ] [MainThread]: adapter version: 1.7.3
[0m12:43:35.043048 [info ] [MainThread]: Configuration:
[0m12:43:35.044035 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:43:35.044035 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:43:35.045032 [info ] [MainThread]: Required dependencies:
[0m12:43:35.046145 [debug] [MainThread]: Executing "git --help"
[0m12:43:35.091183 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:43:35.092167 [debug] [MainThread]: STDERR: "b''"
[0m12:43:35.092167 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:43:35.093177 [info ] [MainThread]: Connection:
[0m12:43:35.094208 [info ] [MainThread]:   host: adb-6516581332475033.13.azuredatabricks.net
[0m12:43:35.095203 [info ] [MainThread]:   http_path: sql/protocolv1/o/6516581332475033/1219-172427-cg5hgyuy
[0m12:43:35.095203 [info ] [MainThread]:   catalog: hive_metastore
[0m12:43:35.096470 [info ] [MainThread]:   schema: saleslt
[0m12:43:35.097494 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m12:43:35.098494 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: debug, thread: (23032, 24504), compute: ``
[0m12:43:35.099494 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m12:43:35.100494 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: debug, thread: (23032, 24504), compute: ``, acquire_release_count: 0, idle time: 0s
[0m12:43:35.101497 [debug] [MainThread]: Databricks adapter: Thread (23032, 24504) using default compute resource.
[0m12:43:35.101497 [debug] [MainThread]: Using databricks connection "debug"
[0m12:43:35.102498 [debug] [MainThread]: On debug: select 1 as id
[0m12:43:35.102498 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:43:36.041999 [debug] [MainThread]: SQL status: OK in 0.9399999976158142 seconds
[0m12:43:36.043999 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: debug, thread: (23032, 24504), compute: ``, acquire_release_count: 1, idle time: 0s
[0m12:43:36.043999 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m12:43:36.044998 [info ] [MainThread]: [32mAll checks passed![0m
[0m12:43:36.046995 [debug] [MainThread]: Command `dbt debug` succeeded at 12:43:36.046995 after 2.77 seconds
[0m12:43:36.047995 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:43:36.047995 [debug] [MainThread]: On debug: Close
[0m12:43:36.140917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022373806670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022317988790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022317975340>]}
[0m12:43:36.141916 [debug] [MainThread]: Flushing usage events
[0m12:44:20.933575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022E93376670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022E963555E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022E96355EE0>]}


============================== 12:44:20.938556 | 5984f20f-3dbd-4299-bb3f-30bb72fe3ed0 ==============================
[0m12:44:20.938556 [info ] [MainThread]: Running with dbt=1.7.4
[0m12:44:20.939558 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m12:44:22.736506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5984f20f-3dbd-4299-bb3f-30bb72fe3ed0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022E96394B80>]}
[0m12:44:22.861081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5984f20f-3dbd-4299-bb3f-30bb72fe3ed0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EB7416700>]}
[0m12:44:22.862081 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m12:44:22.879063 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m12:44:22.903658 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m12:44:22.904980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '5984f20f-3dbd-4299-bb3f-30bb72fe3ed0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EB753A220>]}
[0m12:44:25.238168 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_customers' in the 'models' section of file 'models\marts\customer\dim_customer.yml'
[0m12:44:25.377718 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_products' in the 'models' section of file 'models\marts\product\dim_product.yml'
[0m12:44:25.410227 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_sales' in the 'models' section of file 'models\marts\sales\sales.yml'
[0m12:44:25.570774 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.unique_dim_customers_customer_sk.22a014df62' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m12:44:25.572774 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_customers_customer_sk.8ae5836863' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m12:44:25.573772 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_customers_customerid.209fbdda85' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m12:44:25.576774 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_customers_AddressId.86b771f63e' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m12:44:25.581773 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.unique_dim_products_product_sk.8f20ac7c5b' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m12:44:25.583772 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_products_product_sk.2a2df3e1b9' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m12:44:25.587771 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_products_product_name.991aec73f3' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m12:44:25.590789 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_products_sellstartdate.f97a265a0f' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m12:44:25.591781 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.unique_dim_sales_saleOrderID.810c5f247c' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.593782 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_saleOrderID.48ce11e7f3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.594785 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.unique_dim_sales_saleOrderDetailID.343b942405' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.595782 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_saleOrderDetailID.a60664de3a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.597785 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_orderQty.66af966596' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.604291 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_productID.cbf6d34890' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.606303 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_unitPrice.3545b5473a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.607303 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_lineTotal.d55bca27f8' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.608303 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_name.4c7b961f77' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.609302 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_productNumber.3a23a94ddd' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.610305 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_standardCost.d3f58be9a3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.612303 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_listPrice.4ee58b9e3f' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.613304 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_sellStartDate.b44c8ea118' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.616305 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_orderDate.6f6f720ec3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.620302 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_customerID.60b0993af5' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.629303 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_subTotal.bfeb62a487' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.631306 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_taxAmt.94cff67d6a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.637301 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_freight.ca13e04131' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.638302 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_spark_modeling.not_null_dim_sales_totalDue.920571e023' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m12:44:25.702315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5984f20f-3dbd-4299-bb3f-30bb72fe3ed0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EB7DF50D0>]}
[0m12:44:25.730059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5984f20f-3dbd-4299-bb3f-30bb72fe3ed0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EB7C3FC70>]}
[0m12:44:25.731025 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m12:44:25.732006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5984f20f-3dbd-4299-bb3f-30bb72fe3ed0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EB7DF5370>]}
[0m12:44:25.734025 [info ] [MainThread]: 
[0m12:44:25.736021 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (11480, 25152), compute: ``
[0m12:44:25.737025 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:44:25.738025 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (11480, 25152), compute: ``, acquire_release_count: 0, idle time: 0s
[0m12:44:25.739009 [debug] [MainThread]: Databricks adapter: Thread (11480, 25152) using default compute resource.
[0m12:44:25.742005 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore, thread: (11480, 16744), compute: ``
[0m12:44:25.742005 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m12:44:25.743005 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore, thread: (11480, 16744), compute: ``, acquire_release_count: 0, idle time: 0s
[0m12:44:25.743005 [debug] [ThreadPool]: Databricks adapter: Thread (11480, 16744) using default compute resource.
[0m12:44:25.744005 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m12:44:25.744005 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m12:44:25.745005 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:44:26.637449 [debug] [ThreadPool]: SQL status: OK in 0.8899999856948853 seconds
[0m12:44:26.652357 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore, thread: (11480, 16744), compute: ``, acquire_release_count: 1, idle time: 0s
[0m12:44:26.660372 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_snapshots, thread: (11480, 15496), compute: ``
[0m12:44:26.661327 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m12:44:26.661327 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_snapshots, thread: (11480, 15496), compute: ``, acquire_release_count: 0, idle time: 0s
[0m12:44:26.662354 [debug] [ThreadPool]: Databricks adapter: Thread (11480, 15496) using default compute resource.
[0m12:44:26.669351 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m12:44:26.669351 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m12:44:26.670352 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:44:27.356345 [debug] [ThreadPool]: SQL status: OK in 0.6899999976158142 seconds
[0m12:44:27.377120 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:44:27.377120 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m12:44:27.378119 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m12:44:27.652585 [debug] [ThreadPool]: SQL status: OK in 0.27000001072883606 seconds
[0m12:44:27.664601 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m12:44:27.665602 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m12:44:27.959787 [debug] [ThreadPool]: SQL status: OK in 0.28999999165534973 seconds
[0m12:44:27.973791 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m12:44:27.973791 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show table extended in `hive_metastore`.`snapshots` like '*'
  
[0m12:44:29.296733 [debug] [ThreadPool]: SQL status: OK in 1.3200000524520874 seconds
[0m12:44:29.301736 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_snapshots, thread: (11480, 15496), compute: ``, acquire_release_count: 1, idle time: 0s
[0m12:44:29.303736 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m12:44:29.307716 [debug] [ThreadPool]: Databricks adapter: Reusing DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (11480, 15496), compute: ``, acquire_release_count: 0, idle time: 0.0049822330474853516s
[0m12:44:29.307716 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (11480, 15496), compute: ``, acquire_release_count: 0, idle time: 0.005979061126708984s
[0m12:44:29.308717 [debug] [ThreadPool]: Databricks adapter: Thread (11480, 15496) using default compute resource.
[0m12:44:29.312235 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m12:44:29.312235 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m12:44:29.500808 [debug] [ThreadPool]: SQL status: OK in 0.1899999976158142 seconds
[0m12:44:29.510192 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m12:44:29.510726 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m12:44:29.700220 [debug] [ThreadPool]: SQL status: OK in 0.1899999976158142 seconds
[0m12:44:29.709219 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m12:44:29.710223 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m12:44:30.012606 [debug] [ThreadPool]: SQL status: OK in 0.30000001192092896 seconds
[0m12:44:30.020600 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m12:44:30.020600 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show table extended in `hive_metastore`.`saleslt` like '*'
  
[0m12:44:30.628987 [debug] [ThreadPool]: SQL status: OK in 0.6100000143051147 seconds
[0m12:44:30.635009 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (11480, 15496), compute: ``, acquire_release_count: 1, idle time: 1.332249641418457s
[0m12:44:30.640000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5984f20f-3dbd-4299-bb3f-30bb72fe3ed0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EB7669160>]}
[0m12:44:30.645048 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:44:30.646061 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:44:30.646061 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (11480, 25152), compute: ``, acquire_release_count: 1, idle time: 0s
[0m12:44:30.647059 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:44:30.648059 [info ] [MainThread]: 
[0m12:44:30.654075 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.dim_customer
[0m12:44:30.655075 [info ] [Thread-1  ]: 1 of 3 START sql table model saleslt.dim_customer .............................. [RUN]
[0m12:44:30.657060 [debug] [Thread-1  ]: Databricks adapter: Creating DatabricksDBTConnection. name: model.dbt_spark_modeling.dim_customer, thread: (11480, 8704), compute: ``
[0m12:44:30.658081 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbt_spark_modeling.dim_customer'
[0m12:44:30.659057 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.dim_customer, thread: (11480, 8704), compute: ``, acquire_release_count: 0, idle time: 0s
[0m12:44:30.660110 [debug] [Thread-1  ]: Databricks adapter: On thread (11480, 8704): `hive_metastore`.`saleslt`.`dim_customer` using default compute resource.
[0m12:44:30.660110 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.dim_customer
[0m12:44:30.677063 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.dim_customer"
[0m12:44:30.678065 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_customer (compile): 12:44:30.661075 => 12:44:30.678065
[0m12:44:30.679064 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.dim_customer
[0m12:44:30.704093 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m12:44:30.705097 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_customer"
[0m12:44:30.706078 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_customer: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_customer"} */

      describe extended `hive_metastore`.`saleslt`.`dim_customer`
  
[0m12:44:30.707093 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m12:44:33.253538 [debug] [Thread-1  ]: SQL status: OK in 2.549999952316284 seconds
[0m12:44:33.300541 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.dim_customer"
[0m12:44:33.301541 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_customer"
[0m12:44:33.302543 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_customer: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
)

, transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select *
from transformed
  
[0m12:44:46.609935 [debug] [Thread-1  ]: SQL status: OK in 13.3100004196167 seconds
[0m12:44:46.837938 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_customer (execute): 12:44:30.679064 => 12:44:46.837938
[0m12:44:46.838941 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_customer, thread: (11480, 8704), compute: ``, acquire_release_count: 1, idle time: 0s
[0m12:44:46.839940 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_customer, thread: (11480, 8704), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m12:44:46.840943 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5984f20f-3dbd-4299-bb3f-30bb72fe3ed0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EB7F523D0>]}
[0m12:44:46.841937 [info ] [Thread-1  ]: 1 of 3 OK created sql table model saleslt.dim_customer ......................... [[32mOK[0m in 16.18s]
[0m12:44:46.842941 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.dim_customer
[0m12:44:46.844940 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.dim_product
[0m12:44:46.845938 [info ] [Thread-1  ]: 2 of 3 START sql table model saleslt.dim_product ............................... [RUN]
[0m12:44:46.846940 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.dbt_spark_modeling.dim_customer, now model.dbt_spark_modeling.dim_product)
[0m12:44:46.847938 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: model.dbt_spark_modeling.dim_product, thread: (11480, 8704), compute: ``, acquire_release_count: 0, idle time: 0.006995677947998047s
[0m12:44:46.847938 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.dim_product, thread: (11480, 8704), compute: ``, acquire_release_count: 0, idle time: 0.006995677947998047s
[0m12:44:46.848941 [debug] [Thread-1  ]: Databricks adapter: On thread (11480, 8704): `hive_metastore`.`saleslt`.`dim_product` using default compute resource.
[0m12:44:46.848941 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.dim_product
[0m12:44:46.854964 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.dim_product"
[0m12:44:46.856943 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (compile): 12:44:46.849938 => 12:44:46.855950
[0m12:44:46.857967 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.dim_product
[0m12:44:46.867958 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_product"
[0m12:44:46.869960 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_product: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
[0m12:44:47.838710 [debug] [Thread-1  ]: SQL status: OK in 0.9700000286102295 seconds
[0m12:44:47.877713 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.dim_product"
[0m12:44:47.879711 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_product"
[0m12:44:47.880711 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_product: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  
[0m12:44:48.243620 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  
[0m12:44:48.245618 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'p'.(line 52, pos 8)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID
        p.name as product_name,
--------^^^
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  

[0m12:44:48.246620 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'p'.(line 52, pos 8)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID
        p.name as product_name,
--------^^^
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'p'.(line 52, pos 8)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID
        p.name as product_name,
--------^^^
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:259)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:111)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:541)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more

[0m12:44:48.247615 [debug] [Thread-1  ]: Databricks adapter: operation-id: 80b77e91-14ea-44ec-a8fa-5889c4a0b974
[0m12:44:48.248616 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (execute): 12:44:46.857967 => 12:44:48.248616
[0m12:44:48.249620 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (11480, 8704), compute: ``, acquire_release_count: 1, idle time: 1.408677339553833s
[0m12:44:48.287168 [debug] [Thread-1  ]: Runtime Error in model dim_product (models\marts\product\dim_product.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'p'.(line 52, pos 8)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_product`
        
        
      using delta
        
        
        
        
        
      location '/mnt/gold/products/dim_product'
        
        
        as
        
  
  with product_snapshot as (
      select
          productId,
          name,
          standardCost,
          listPrice,
          size,
          weight,
          productcategoryid,
          productmodelid,
          sellstartdate,
          sellenddate,
          discontinueddate
      from `hive_metastore`.`snapshots`.`product_snapshot`
      where dbt_valid_to is null
  ),
  
  product_model_snapshot as (
      select
          productmodelid,
          name,
          CatalogDescription,
          row_number() over (order by name) as model_id
      from `hive_metastore`.`snapshots`.`productmodel_snapshot`
      where dbt_valid_to is null
  ),
  
  
  transformed as (
      select
          row_number() over (order by p.productId) as product_sk,
          p.productId as productID
          p.name as product_name,
  --------^^^
          p.standardCost,
          p.listPrice,
          p.size,
          p.weight,
          pm.name as model,
          pm.CatalogDescription as description,
          p.sellstartdate,
          p.sellenddate,
          p.discontinueddate
      from product_snapshot p
      left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
  )
  
  select * from transformed
    
  
[0m12:44:48.288170 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (11480, 8704), compute: ``, acquire_release_count: 0, idle time: 0.038550376892089844s
[0m12:44:48.289169 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5984f20f-3dbd-4299-bb3f-30bb72fe3ed0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EB8FD2A30>]}
[0m12:44:48.290163 [error] [Thread-1  ]: 2 of 3 ERROR creating sql table model saleslt.dim_product ...................... [[31mERROR[0m in 1.44s]
[0m12:44:48.292162 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.dim_product
[0m12:44:48.293164 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.sales
[0m12:44:48.294162 [info ] [Thread-1  ]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m12:44:48.296180 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.dbt_spark_modeling.dim_product, now model.dbt_spark_modeling.sales)
[0m12:44:48.297180 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: model.dbt_spark_modeling.sales, thread: (11480, 8704), compute: ``, acquire_release_count: 0, idle time: 0.008010625839233398s
[0m12:44:48.298180 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.sales, thread: (11480, 8704), compute: ``, acquire_release_count: 0, idle time: 0.009010553359985352s
[0m12:44:48.299184 [debug] [Thread-1  ]: Databricks adapter: On thread (11480, 8704): `hive_metastore`.`saleslt`.`sales` using default compute resource.
[0m12:44:48.299184 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.sales
[0m12:44:48.310165 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.sales"
[0m12:44:48.313164 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.sales (compile): 12:44:48.300184 => 12:44:48.312163
[0m12:44:48.314163 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.sales
[0m12:44:48.325180 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.sales"
[0m12:44:48.326163 [debug] [Thread-1  ]: On model.dbt_spark_modeling.sales: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.sales"} */

      describe extended `hive_metastore`.`saleslt`.`sales`
  
[0m12:44:49.904512 [debug] [Thread-1  ]: SQL status: OK in 1.5800000429153442 seconds
[0m12:44:49.914531 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.sales"
[0m12:44:49.922244 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.sales"
[0m12:44:49.924260 [debug] [Thread-1  ]: On model.dbt_spark_modeling.sales: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
[0m12:44:56.749256 [debug] [Thread-1  ]: SQL status: OK in 6.820000171661377 seconds
[0m12:44:56.840986 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.sales (execute): 12:44:48.314163 => 12:44:56.840986
[0m12:44:56.841998 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.sales, thread: (11480, 8704), compute: ``, acquire_release_count: 1, idle time: 8.552829027175903s
[0m12:44:56.842998 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.sales, thread: (11480, 8704), compute: ``, acquire_release_count: 0, idle time: 0.0010001659393310547s
[0m12:44:56.844002 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5984f20f-3dbd-4299-bb3f-30bb72fe3ed0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EB8FD2A30>]}
[0m12:44:56.845005 [info ] [Thread-1  ]: 3 of 3 OK created sql table model saleslt.sales ................................ [[32mOK[0m in 8.55s]
[0m12:44:56.845986 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.sales
[0m12:44:56.847982 [debug] [MainThread]: Databricks adapter: Reusing DatabricksDBTConnection. name: master, thread: (11480, 25152), compute: ``, acquire_release_count: 0, idle time: 26.200923204421997s
[0m12:44:56.848998 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (11480, 25152), compute: ``, acquire_release_count: 0, idle time: 26.201939582824707s
[0m12:44:56.848998 [debug] [MainThread]: Databricks adapter: Thread (11480, 25152) using default compute resource.
[0m12:44:56.849999 [debug] [MainThread]: On master: ROLLBACK
[0m12:44:56.849999 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:44:57.147286 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:44:57.148287 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:44:57.149288 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:44:57.149288 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (11480, 25152), compute: ``, acquire_release_count: 1, idle time: 26.5022292137146s
[0m12:44:57.150291 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:44:57.151289 [debug] [MainThread]: On master: ROLLBACK
[0m12:44:57.151289 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:44:57.152288 [debug] [MainThread]: On master: Close
[0m12:44:57.245229 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m12:44:57.246245 [debug] [MainThread]: On list_hive_metastore: Close
[0m12:44:57.347372 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m12:44:57.347372 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m12:44:57.348371 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:44:57.348371 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m12:44:57.552127 [debug] [MainThread]: Connection 'model.dbt_spark_modeling.sales' was properly closed.
[0m12:44:57.553126 [debug] [MainThread]: On model.dbt_spark_modeling.sales: ROLLBACK
[0m12:44:57.554126 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:44:57.554126 [debug] [MainThread]: On model.dbt_spark_modeling.sales: Close
[0m12:44:57.664098 [info ] [MainThread]: 
[0m12:44:57.666103 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 31.92 seconds (31.92s).
[0m12:44:57.667109 [debug] [MainThread]: Command end result
[0m12:44:57.688093 [info ] [MainThread]: 
[0m12:44:57.690096 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m12:44:57.691096 [info ] [MainThread]: 
[0m12:44:57.692095 [error] [MainThread]:   Runtime Error in model dim_product (models\marts\product\dim_product.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'p'.(line 52, pos 8)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_product`
        
        
      using delta
        
        
        
        
        
      location '/mnt/gold/products/dim_product'
        
        
        as
        
  
  with product_snapshot as (
      select
          productId,
          name,
          standardCost,
          listPrice,
          size,
          weight,
          productcategoryid,
          productmodelid,
          sellstartdate,
          sellenddate,
          discontinueddate
      from `hive_metastore`.`snapshots`.`product_snapshot`
      where dbt_valid_to is null
  ),
  
  product_model_snapshot as (
      select
          productmodelid,
          name,
          CatalogDescription,
          row_number() over (order by name) as model_id
      from `hive_metastore`.`snapshots`.`productmodel_snapshot`
      where dbt_valid_to is null
  ),
  
  
  transformed as (
      select
          row_number() over (order by p.productId) as product_sk,
          p.productId as productID
          p.name as product_name,
  --------^^^
          p.standardCost,
          p.listPrice,
          p.size,
          p.weight,
          pm.name as model,
          pm.CatalogDescription as description,
          p.sellstartdate,
          p.sellenddate,
          p.discontinueddate
      from product_snapshot p
      left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
  )
  
  select * from transformed
    
  
[0m12:44:57.695095 [info ] [MainThread]: 
[0m12:44:57.696095 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
[0m12:44:57.698098 [debug] [MainThread]: Command `dbt run` failed at 12:44:57.698098 after 36.83 seconds
[0m12:44:57.699096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022E93376670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EB7F523D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EB7645F40>]}
[0m12:44:57.700095 [debug] [MainThread]: Flushing usage events
[0m12:47:35.826178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001868C9362E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001868F92F2B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001868F92F730>]}


============================== 12:47:35.831408 | 14199e87-6d89-47f5-83cd-a9a0d4bec74f ==============================
[0m12:47:35.831408 [info ] [MainThread]: Running with dbt=1.7.4
[0m12:47:35.832411 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt test', 'send_anonymous_usage_stats': 'True'}
[0m12:47:37.609348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '14199e87-6d89-47f5-83cd-a9a0d4bec74f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001868B81A910>]}
[0m12:47:37.738115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '14199e87-6d89-47f5-83cd-a9a0d4bec74f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186B0AB6E80>]}
[0m12:47:37.739115 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m12:47:37.759115 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m12:47:37.913111 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m12:47:37.914111 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_modeling://models\marts\customer\dim_customer.sql
[0m12:47:37.915111 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_modeling://models\marts\product\dim_product.sql
[0m12:47:37.915111 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_modeling://models\marts\sales\sales.sql
[0m12:47:38.126135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '14199e87-6d89-47f5-83cd-a9a0d4bec74f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186B10440D0>]}
[0m12:47:38.145114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '14199e87-6d89-47f5-83cd-a9a0d4bec74f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186B0C556A0>]}
[0m12:47:38.146130 [info ] [MainThread]: Found 7 snapshots, 3 models, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m12:47:38.146130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '14199e87-6d89-47f5-83cd-a9a0d4bec74f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186B0C55520>]}
[0m12:47:38.148116 [info ] [MainThread]: 
[0m12:47:38.149341 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m12:47:38.150365 [debug] [MainThread]: Command end result
[0m12:47:38.166366 [debug] [MainThread]: Command `dbt test` succeeded at 12:47:38.165364 after 2.40 seconds
[0m12:47:38.166366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001868C9362E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186B0AB6E80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186B0C555E0>]}
[0m12:47:38.167365 [debug] [MainThread]: Flushing usage events
[0m12:47:47.021101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B259996820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B25C974B50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B25C974C10>]}


============================== 12:47:47.027110 | 48060669-7172-4736-96a9-3c37f840946a ==============================
[0m12:47:47.027110 [info ] [MainThread]: Running with dbt=1.7.4
[0m12:47:47.028092 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m12:47:48.794785 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '48060669-7172-4736-96a9-3c37f840946a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B2588798B0>]}
[0m12:47:48.922193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '48060669-7172-4736-96a9-3c37f840946a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B27DAA20D0>]}
[0m12:47:48.922193 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m12:47:48.938191 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m12:47:49.070193 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:47:49.071192 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:47:49.080193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '48060669-7172-4736-96a9-3c37f840946a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B27DEEB0D0>]}
[0m12:47:49.099193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '48060669-7172-4736-96a9-3c37f840946a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B27DEC5F40>]}
[0m12:47:49.100192 [info ] [MainThread]: Found 7 snapshots, 3 models, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m12:47:49.101193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '48060669-7172-4736-96a9-3c37f840946a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B27DC51250>]}
[0m12:47:49.103192 [info ] [MainThread]: 
[0m12:47:49.105193 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (16576, 23992), compute: ``
[0m12:47:49.105193 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:47:49.106193 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (16576, 23992), compute: ``, acquire_release_count: 0, idle time: 0s
[0m12:47:49.106193 [debug] [MainThread]: Databricks adapter: Thread (16576, 23992) using default compute resource.
[0m12:47:49.109193 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore, thread: (16576, 15488), compute: ``
[0m12:47:49.109193 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m12:47:49.110193 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore, thread: (16576, 15488), compute: ``, acquire_release_count: 0, idle time: 0s
[0m12:47:49.111194 [debug] [ThreadPool]: Databricks adapter: Thread (16576, 15488) using default compute resource.
[0m12:47:49.111194 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m12:47:49.112193 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m12:47:49.112193 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:47:49.902324 [debug] [ThreadPool]: SQL status: OK in 0.7900000214576721 seconds
[0m12:47:49.907329 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore, thread: (16576, 15488), compute: ``, acquire_release_count: 1, idle time: 0s
[0m12:47:49.910328 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (16576, 22360), compute: ``
[0m12:47:49.911329 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m12:47:49.911329 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (16576, 22360), compute: ``, acquire_release_count: 0, idle time: 0s
[0m12:47:49.912332 [debug] [ThreadPool]: Databricks adapter: Thread (16576, 22360) using default compute resource.
[0m12:47:49.919332 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m12:47:49.919332 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m12:47:49.920331 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:47:50.822687 [debug] [ThreadPool]: SQL status: OK in 0.8999999761581421 seconds
[0m12:47:50.843690 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:47:50.844688 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m12:47:50.845690 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m12:47:51.126420 [debug] [ThreadPool]: SQL status: OK in 0.2800000011920929 seconds
[0m12:47:51.143421 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m12:47:51.144419 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m12:47:51.435236 [debug] [ThreadPool]: SQL status: OK in 0.28999999165534973 seconds
[0m12:47:51.447256 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m12:47:51.447256 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show table extended in `hive_metastore`.`saleslt` like '*'
  
[0m12:47:52.058629 [debug] [ThreadPool]: SQL status: OK in 0.6100000143051147 seconds
[0m12:47:52.065632 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (16576, 22360), compute: ``, acquire_release_count: 1, idle time: 0s
[0m12:47:52.068631 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m12:47:52.073632 [debug] [ThreadPool]: Databricks adapter: Reusing DatabricksDBTConnection. name: list_hive_metastore_snapshots, thread: (16576, 22360), compute: ``, acquire_release_count: 0, idle time: 0.006999015808105469s
[0m12:47:52.074645 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_snapshots, thread: (16576, 22360), compute: ``, acquire_release_count: 0, idle time: 0.008012771606445312s
[0m12:47:52.074645 [debug] [ThreadPool]: Databricks adapter: Thread (16576, 22360) using default compute resource.
[0m12:47:52.079628 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m12:47:52.079628 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m12:47:52.258188 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m12:47:52.269187 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m12:47:52.270188 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m12:47:52.456966 [debug] [ThreadPool]: SQL status: OK in 0.1899999976158142 seconds
[0m12:47:52.465967 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m12:47:52.468408 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m12:47:52.652315 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m12:47:52.660315 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m12:47:52.661300 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show table extended in `hive_metastore`.`snapshots` like '*'
  
[0m12:47:53.390258 [debug] [ThreadPool]: SQL status: OK in 0.7300000190734863 seconds
[0m12:47:53.395242 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_snapshots, thread: (16576, 22360), compute: ``, acquire_release_count: 1, idle time: 1.3276126384735107s
[0m12:47:53.398262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '48060669-7172-4736-96a9-3c37f840946a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B27DC6CF10>]}
[0m12:47:53.399258 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:47:53.399258 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:47:53.400258 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (16576, 23992), compute: ``, acquire_release_count: 1, idle time: 0s
[0m12:47:53.401262 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:47:53.402249 [info ] [MainThread]: 
[0m12:47:53.407024 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.dim_customer
[0m12:47:53.408047 [info ] [Thread-1  ]: 1 of 3 START sql table model saleslt.dim_customer .............................. [RUN]
[0m12:47:53.409982 [debug] [Thread-1  ]: Databricks adapter: Creating DatabricksDBTConnection. name: model.dbt_spark_modeling.dim_customer, thread: (16576, 17960), compute: ``
[0m12:47:53.409982 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbt_spark_modeling.dim_customer'
[0m12:47:53.410974 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.dim_customer, thread: (16576, 17960), compute: ``, acquire_release_count: 0, idle time: 0s
[0m12:47:53.411976 [debug] [Thread-1  ]: Databricks adapter: On thread (16576, 17960): `hive_metastore`.`saleslt`.`dim_customer` using default compute resource.
[0m12:47:53.412980 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.dim_customer
[0m12:47:53.424977 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.dim_customer"
[0m12:47:53.426978 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_customer (compile): 12:47:53.412980 => 12:47:53.425979
[0m12:47:53.426978 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.dim_customer
[0m12:47:53.448978 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m12:47:53.449979 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_customer"
[0m12:47:53.449979 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_customer: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_customer"} */

      describe extended `hive_metastore`.`saleslt`.`dim_customer`
  
[0m12:47:53.450979 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m12:47:54.814848 [debug] [Thread-1  ]: SQL status: OK in 1.3600000143051147 seconds
[0m12:47:54.882841 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.dim_customer"
[0m12:47:54.884843 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_customer"
[0m12:47:54.885842 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_customer: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerID,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
)

, transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId as CustomerID,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select *
from transformed
  
[0m12:47:59.184053 [debug] [Thread-1  ]: SQL status: OK in 4.300000190734863 seconds
[0m12:47:59.233933 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_customer (execute): 12:47:53.427976 => 12:47:59.232929
[0m12:47:59.234915 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_customer, thread: (16576, 17960), compute: ``, acquire_release_count: 1, idle time: 0s
[0m12:47:59.235929 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_customer, thread: (16576, 17960), compute: ``, acquire_release_count: 0, idle time: 0.0010139942169189453s
[0m12:47:59.236913 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '48060669-7172-4736-96a9-3c37f840946a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B27DFBF760>]}
[0m12:47:59.238916 [info ] [Thread-1  ]: 1 of 3 OK created sql table model saleslt.dim_customer ......................... [[32mOK[0m in 5.83s]
[0m12:47:59.240915 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.dim_customer
[0m12:47:59.241913 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.dim_product
[0m12:47:59.242912 [info ] [Thread-1  ]: 2 of 3 START sql table model saleslt.dim_product ............................... [RUN]
[0m12:47:59.244915 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.dbt_spark_modeling.dim_customer, now model.dbt_spark_modeling.dim_product)
[0m12:47:59.245917 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: model.dbt_spark_modeling.dim_product, thread: (16576, 17960), compute: ``, acquire_release_count: 0, idle time: 0.008001089096069336s
[0m12:47:59.245917 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.dim_product, thread: (16576, 17960), compute: ``, acquire_release_count: 0, idle time: 0.00900411605834961s
[0m12:47:59.246918 [debug] [Thread-1  ]: Databricks adapter: On thread (16576, 17960): `hive_metastore`.`saleslt`.`dim_product` using default compute resource.
[0m12:47:59.246918 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.dim_product
[0m12:47:59.470910 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.dim_product"
[0m12:47:59.472929 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (compile): 12:47:59.247918 => 12:47:59.472929
[0m12:47:59.473915 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.dim_product
[0m12:47:59.480919 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_product"
[0m12:47:59.481918 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_product: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
[0m12:48:00.114601 [debug] [Thread-1  ]: SQL status: OK in 0.6299999952316284 seconds
[0m12:48:00.123600 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.dim_product"
[0m12:48:00.124600 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_product"
[0m12:48:00.125600 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_product: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  
[0m12:48:04.848996 [debug] [Thread-1  ]: SQL status: OK in 4.71999979019165 seconds
[0m12:48:04.853996 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (execute): 12:47:59.473915 => 12:48:04.853996
[0m12:48:04.854996 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (16576, 17960), compute: ``, acquire_release_count: 1, idle time: 5.617082595825195s
[0m12:48:04.855996 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (16576, 17960), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m12:48:04.855996 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '48060669-7172-4736-96a9-3c37f840946a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B27E08D460>]}
[0m12:48:04.856996 [info ] [Thread-1  ]: 2 of 3 OK created sql table model saleslt.dim_product .......................... [[32mOK[0m in 5.61s]
[0m12:48:04.858975 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.dim_product
[0m12:48:04.859977 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.sales
[0m12:48:04.859977 [info ] [Thread-1  ]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m12:48:04.861979 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.dbt_spark_modeling.dim_product, now model.dbt_spark_modeling.sales)
[0m12:48:04.862992 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: model.dbt_spark_modeling.sales, thread: (16576, 17960), compute: ``, acquire_release_count: 0, idle time: 0.005982398986816406s
[0m12:48:04.862992 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.sales, thread: (16576, 17960), compute: ``, acquire_release_count: 0, idle time: 0.0069959163665771484s
[0m12:48:04.863996 [debug] [Thread-1  ]: Databricks adapter: On thread (16576, 17960): `hive_metastore`.`saleslt`.`sales` using default compute resource.
[0m12:48:04.863996 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.sales
[0m12:48:04.870996 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.sales"
[0m12:48:04.871976 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.sales (compile): 12:48:04.864996 => 12:48:04.871976
[0m12:48:04.872978 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.sales
[0m12:48:04.879994 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.sales"
[0m12:48:04.880992 [debug] [Thread-1  ]: On model.dbt_spark_modeling.sales: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.sales"} */

      describe extended `hive_metastore`.`saleslt`.`sales`
  
[0m12:48:05.771470 [debug] [Thread-1  ]: SQL status: OK in 0.8899999856948853 seconds
[0m12:48:05.784472 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.sales"
[0m12:48:05.786473 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.sales"
[0m12:48:05.786473 [debug] [Thread-1  ]: On model.dbt_spark_modeling.sales: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.ProductID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        soh.CustomerID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
[0m12:48:10.484283 [debug] [Thread-1  ]: SQL status: OK in 4.699999809265137 seconds
[0m12:48:10.490285 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.sales (execute): 12:48:04.872978 => 12:48:10.490285
[0m12:48:10.491282 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.sales, thread: (16576, 17960), compute: ``, acquire_release_count: 1, idle time: 5.6352856159210205s
[0m12:48:10.492281 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.sales, thread: (16576, 17960), compute: ``, acquire_release_count: 0, idle time: 0.0009996891021728516s
[0m12:48:10.493281 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '48060669-7172-4736-96a9-3c37f840946a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B27E08D3D0>]}
[0m12:48:10.494281 [info ] [Thread-1  ]: 3 of 3 OK created sql table model saleslt.sales ................................ [[32mOK[0m in 5.63s]
[0m12:48:10.495281 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.sales
[0m12:48:10.499281 [debug] [MainThread]: Databricks adapter: Reusing DatabricksDBTConnection. name: master, thread: (16576, 23992), compute: ``, acquire_release_count: 0, idle time: 17.09902262687683s
[0m12:48:10.500287 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (16576, 23992), compute: ``, acquire_release_count: 0, idle time: 17.10002875328064s
[0m12:48:10.501298 [debug] [MainThread]: Databricks adapter: Thread (16576, 23992) using default compute resource.
[0m12:48:10.502297 [debug] [MainThread]: On master: ROLLBACK
[0m12:48:10.502297 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:48:10.893797 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:48:10.894797 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:48:10.895798 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:48:10.895798 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (16576, 23992), compute: ``, acquire_release_count: 1, idle time: 17.495540142059326s
[0m12:48:10.897798 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:48:10.897798 [debug] [MainThread]: On master: ROLLBACK
[0m12:48:10.898799 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:48:10.899797 [debug] [MainThread]: On master: Close
[0m12:48:11.022091 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m12:48:11.024090 [debug] [MainThread]: On list_hive_metastore: Close
[0m12:48:11.151107 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m12:48:11.152107 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m12:48:11.153111 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:48:11.153111 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m12:48:11.297091 [debug] [MainThread]: Connection 'model.dbt_spark_modeling.sales' was properly closed.
[0m12:48:11.298099 [debug] [MainThread]: On model.dbt_spark_modeling.sales: ROLLBACK
[0m12:48:11.299089 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:48:11.299089 [debug] [MainThread]: On model.dbt_spark_modeling.sales: Close
[0m12:48:11.387092 [info ] [MainThread]: 
[0m12:48:11.389090 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 22.28 seconds (22.28s).
[0m12:48:11.391093 [debug] [MainThread]: Command end result
[0m12:48:11.418089 [info ] [MainThread]: 
[0m12:48:11.419090 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:48:11.420090 [info ] [MainThread]: 
[0m12:48:11.421089 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m12:48:11.423090 [debug] [MainThread]: Command `dbt run` succeeded at 12:48:11.423090 after 24.47 seconds
[0m12:48:11.424094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B259996820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B2577B6F10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B27E091490>]}
[0m12:48:11.426093 [debug] [MainThread]: Flushing usage events
[0m13:37:46.305781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C90C25730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C93C02370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C93C02B20>]}


============================== 13:37:46.308782 | 71a87d28-b223-42e3-9d2a-bf96127dc478 ==============================
[0m13:37:46.308782 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:37:46.309781 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt snapshot', 'send_anonymous_usage_stats': 'True'}
[0m13:37:47.526798 [error] [MainThread]: Encountered an error:

[0m13:37:47.626103 [error] [MainThread]: Traceback (most recent call last):
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\cli\requires.py", line 90, in wrapper
    result, success = func(*args, **kwargs)
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\cli\requires.py", line 75, in wrapper
    return func(*args, **kwargs)
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\cli\requires.py", line 165, in wrapper
    profile = load_profile(flags.PROJECT_DIR, flags.VARS, flags.PROFILE, flags.TARGET, threads)
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\config\runtime.py", line 70, in load_profile
    profile = Profile.render(
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\config\profile.py", line 436, in render
    return cls.from_raw_profiles(
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\config\profile.py", line 401, in from_raw_profiles
    return cls.from_raw_profile_info(
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\config\profile.py", line 355, in from_raw_profile_info
    credentials: Credentials = cls._credentials_from_profile(
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\config\profile.py", line 165, in _credentials_from_profile
    cls = load_plugin(typename)
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\adapters\factory.py", line 212, in load_plugin
    return FACTORY.load_plugin(name)
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\adapters\factory.py", line 58, in load_plugin
    mod: Any = import_module("." + name, "dbt.adapters")
  File "C:\Program Files\Python39\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 790, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\adapters\databricks\__init__.py", line 1, in <module>
    from dbt.adapters.databricks.connections import DatabricksConnectionManager  # noqa
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\adapters\databricks\connections.py", line 63, in <module>
    from databricks.sql.client import (
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\databricks\sql\client.py", line 3, in <module>
    import pandas
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\pandas\__init__.py", line 73, in <module>
    from pandas.core.api import (
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\pandas\core\api.py", line 47, in <module>
    from pandas.core.groupby import (
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\pandas\core\groupby\__init__.py", line 1, in <module>
    from pandas.core.groupby.generic import (
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\pandas\core\groupby\generic.py", line 67, in <module>
    from pandas.core.frame import DataFrame
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\pandas\core\frame.py", line 180, in <module>
    from pandas.core.reshape.melt import melt
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 786, in exec_module
  File "<frozen importlib._bootstrap_external>", line 881, in get_code
  File "<frozen importlib._bootstrap_external>", line 979, in get_data
KeyboardInterrupt

[0m13:37:47.628104 [debug] [MainThread]: Command `dbt snapshot` failed at 13:37:47.628104 after 1.37 seconds
[0m13:37:47.629102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C90C25730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C93C020A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C93C4B5E0>]}
[0m13:37:47.629102 [debug] [MainThread]: Flushing usage events
[0m13:38:29.836306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DFF3E7130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D82516490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D82516EB0>]}


============================== 13:38:29.839311 | a793bc50-848b-41ca-b416-eb0a7cc49140 ==============================
[0m13:38:29.839311 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:38:29.839311 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt snapshot', 'send_anonymous_usage_stats': 'True'}
[0m13:38:30.933841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a793bc50-848b-41ca-b416-eb0a7cc49140', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DFE2C9880>]}
[0m13:38:31.008840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a793bc50-848b-41ca-b416-eb0a7cc49140', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DA359C070>]}
[0m13:38:31.009842 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m13:38:31.036839 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:38:31.739052 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m13:38:31.740052 [debug] [MainThread]: Partial parsing: added file: dbt_spark_modeling://snapshots\productcategory.sql
[0m13:38:31.740052 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_modeling://models\marts\customer\dim_customer.sql
[0m13:38:32.037050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a793bc50-848b-41ca-b416-eb0a7cc49140', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DA3A07DC0>]}
[0m13:38:32.063086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a793bc50-848b-41ca-b416-eb0a7cc49140', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DA35D6490>]}
[0m13:38:32.063086 [info ] [MainThread]: Found 8 snapshots, 3 models, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m13:38:32.064087 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a793bc50-848b-41ca-b416-eb0a7cc49140', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DA35DCAC0>]}
[0m13:38:32.065085 [info ] [MainThread]: 
[0m13:38:32.066084 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (15524, 24048), compute: ``
[0m13:38:32.067091 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:38:32.067091 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (15524, 24048), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:38:32.067091 [debug] [MainThread]: Databricks adapter: Thread (15524, 24048) using default compute resource.
[0m13:38:32.069085 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore, thread: (15524, 3324), compute: ``
[0m13:38:32.069085 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m13:38:32.070087 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore, thread: (15524, 3324), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:38:32.070087 [debug] [ThreadPool]: Databricks adapter: Thread (15524, 3324) using default compute resource.
[0m13:38:32.070087 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m13:38:32.071085 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m13:38:32.071085 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:38:32.967154 [debug] [ThreadPool]: SQL status: OK in 0.8999999761581421 seconds
[0m13:38:32.971154 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore, thread: (15524, 3324), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:38:32.974159 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (15524, 23048), compute: ``
[0m13:38:32.975153 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m13:38:32.975153 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (15524, 23048), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:38:32.976154 [debug] [ThreadPool]: Databricks adapter: Thread (15524, 23048) using default compute resource.
[0m13:38:32.980670 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:38:32.980670 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m13:38:32.980670 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:38:33.347383 [debug] [ThreadPool]: SQL status: OK in 0.3700000047683716 seconds
[0m13:38:33.362381 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:38:33.362381 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:38:33.362381 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m13:38:33.583646 [debug] [ThreadPool]: SQL status: OK in 0.2199999988079071 seconds
[0m13:38:33.593647 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:38:33.594646 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m13:38:33.806821 [debug] [ThreadPool]: SQL status: OK in 0.20999999344348907 seconds
[0m13:38:33.816814 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:38:33.817817 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show table extended in `hive_metastore`.`saleslt` like '*'
  
[0m13:38:34.396440 [debug] [ThreadPool]: SQL status: OK in 0.5799999833106995 seconds
[0m13:38:34.401435 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (15524, 23048), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:38:34.403436 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m13:38:34.407440 [debug] [ThreadPool]: Databricks adapter: Reusing DatabricksDBTConnection. name: list_hive_metastore_snapshots, thread: (15524, 23048), compute: ``, acquire_release_count: 0, idle time: 0.005004167556762695s
[0m13:38:34.407440 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_snapshots, thread: (15524, 23048), compute: ``, acquire_release_count: 0, idle time: 0.005004167556762695s
[0m13:38:34.407440 [debug] [ThreadPool]: Databricks adapter: Thread (15524, 23048) using default compute resource.
[0m13:38:34.411435 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:38:34.411435 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m13:38:34.531390 [debug] [ThreadPool]: SQL status: OK in 0.11999999731779099 seconds
[0m13:38:34.538391 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:38:34.539392 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m13:38:34.667771 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m13:38:34.676774 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:38:34.676774 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m13:38:34.858532 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m13:38:34.865534 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:38:34.865534 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show table extended in `hive_metastore`.`snapshots` like '*'
  
[0m13:38:35.553948 [debug] [ThreadPool]: SQL status: OK in 0.6899999976158142 seconds
[0m13:38:35.557944 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_snapshots, thread: (15524, 23048), compute: ``, acquire_release_count: 1, idle time: 1.1555087566375732s
[0m13:38:35.561947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a793bc50-848b-41ca-b416-eb0a7cc49140', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DA367BE20>]}
[0m13:38:35.562946 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:38:35.563946 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:38:35.563946 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (15524, 24048), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:38:35.564945 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:38:35.565944 [info ] [MainThread]: 
[0m13:38:35.568945 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.address_snapshot
[0m13:38:35.568945 [info ] [Thread-1  ]: 1 of 8 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m13:38:35.569943 [debug] [Thread-1  ]: Databricks adapter: Creating DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.address_snapshot, thread: (15524, 19148), compute: ``
[0m13:38:35.569943 [debug] [Thread-1  ]: Acquiring new databricks connection 'snapshot.dbt_spark_modeling.address_snapshot'
[0m13:38:35.569943 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.address_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:38:35.570943 [debug] [Thread-1  ]: Databricks adapter: On thread (15524, 19148): `hive_metastore`.`snapshots`.`address_snapshot` using default compute resource.
[0m13:38:35.570943 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.address_snapshot
[0m13:38:35.579942 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.address_snapshot (compile): 13:38:35.570943 => 13:38:35.579942
[0m13:38:35.579942 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.address_snapshot
[0m13:38:35.603945 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:38:35.603945 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.address_snapshot"
[0m13:38:35.604942 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m13:38:35.604942 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m13:38:37.274138 [debug] [Thread-1  ]: SQL status: OK in 1.6699999570846558 seconds
[0m13:38:37.310133 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.address_snapshot"
[0m13:38:37.310133 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.address_snapshot"} */
select * from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m13:38:37.637070 [debug] [Thread-1  ]: SQL status: OK in 0.33000001311302185 seconds
[0m13:38:37.650311 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.address_snapshot"
[0m13:38:37.651312 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m13:38:38.179831 [debug] [Thread-1  ]: SQL status: OK in 0.5299999713897705 seconds
[0m13:38:38.190827 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.address_snapshot"
[0m13:38:38.193946 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m13:38:38.990769 [debug] [Thread-1  ]: SQL status: OK in 0.800000011920929 seconds
[0m13:38:39.017762 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.address_snapshot"
[0m13:38:39.018763 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.address_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            AddressID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`address_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            AddressID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m13:38:40.078993 [debug] [Thread-1  ]: SQL status: OK in 1.059999942779541 seconds
[0m13:38:40.083993 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.address_snapshot"
[0m13:38:40.085000 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m13:38:40.280314 [debug] [Thread-1  ]: SQL status: OK in 0.20000000298023224 seconds
[0m13:38:40.286303 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.address_snapshot"
[0m13:38:40.287304 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m13:38:40.754920 [debug] [Thread-1  ]: SQL status: OK in 0.4699999988079071 seconds
[0m13:38:40.761920 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.address_snapshot"
[0m13:38:40.762923 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m13:38:40.930595 [debug] [Thread-1  ]: SQL status: OK in 0.17000000178813934 seconds
[0m13:38:40.938594 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.address_snapshot"
[0m13:38:40.938594 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m13:38:41.420045 [debug] [Thread-1  ]: SQL status: OK in 0.47999998927116394 seconds
[0m13:38:41.447013 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.address_snapshot"
[0m13:38:41.448014 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m13:38:41.737158 [debug] [Thread-1  ]: SQL status: OK in 0.28999999165534973 seconds
[0m13:38:41.748157 [debug] [Thread-1  ]: Writing runtime SQL for node "snapshot.dbt_spark_modeling.address_snapshot"
[0m13:38:41.749157 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.address_snapshot"
[0m13:38:41.750157 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.address_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`address_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m13:38:49.716063 [debug] [Thread-1  ]: SQL status: OK in 7.96999979019165 seconds
[0m13:38:49.790792 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.address_snapshot"
[0m13:38:49.790792 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.address_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m13:38:50.474960 [debug] [Thread-1  ]: SQL status: OK in 0.6800000071525574 seconds
[0m13:38:50.495960 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m13:38:50.495960 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.address_snapshot (execute): 13:38:35.580943 => 13:38:50.495960
[0m13:38:50.496960 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.address_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:38:50.496960 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.address_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m13:38:50.497960 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a793bc50-848b-41ca-b416-eb0a7cc49140', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DA3DC75B0>]}
[0m13:38:50.498960 [info ] [Thread-1  ]: 1 of 8 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 14.93s]
[0m13:38:50.498960 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.address_snapshot
[0m13:38:50.499956 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.customer_snapshot
[0m13:38:50.499956 [info ] [Thread-1  ]: 2 of 8 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m13:38:50.501961 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.address_snapshot, now snapshot.dbt_spark_modeling.customer_snapshot)
[0m13:38:50.501961 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.customer_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.0040013790130615234s
[0m13:38:50.502960 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.customer_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.005000591278076172s
[0m13:38:50.502960 [debug] [Thread-1  ]: Databricks adapter: On thread (15524, 19148): `hive_metastore`.`snapshots`.`customer_snapshot` using default compute resource.
[0m13:38:50.503958 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.customer_snapshot
[0m13:38:50.506956 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.customer_snapshot (compile): 13:38:50.503958 => 13:38:50.506956
[0m13:38:50.507955 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.customer_snapshot
[0m13:38:50.512957 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customer_snapshot"
[0m13:38:50.512957 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m13:38:51.217387 [debug] [Thread-1  ]: SQL status: OK in 0.699999988079071 seconds
[0m13:38:51.224388 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customer_snapshot"
[0m13:38:51.225398 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customer_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m13:38:51.422095 [debug] [Thread-1  ]: SQL status: OK in 0.20000000298023224 seconds
[0m13:38:51.428097 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customer_snapshot"
[0m13:38:51.429096 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m13:38:51.893839 [debug] [Thread-1  ]: SQL status: OK in 0.46000000834465027 seconds
[0m13:38:51.899838 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customer_snapshot"
[0m13:38:51.899838 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m13:38:52.367170 [debug] [Thread-1  ]: SQL status: OK in 0.4699999988079071 seconds
[0m13:38:52.373169 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customer_snapshot"
[0m13:38:52.374169 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customer_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            CustomerId as dbt_unique_key

        from `hive_metastore`.`snapshots`.`customer_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            CustomerId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            CustomerId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            CustomerId as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`NameStyle` != source_data.`NameStyle`
        or
        (
            ((snapshotted_data.`NameStyle` is null) and not (source_data.`NameStyle` is null))
            or
            ((not snapshotted_data.`NameStyle` is null) and (source_data.`NameStyle` is null))
        ) or snapshotted_data.`Title` != source_data.`Title`
        or
        (
            ((snapshotted_data.`Title` is null) and not (source_data.`Title` is null))
            or
            ((not snapshotted_data.`Title` is null) and (source_data.`Title` is null))
        ) or snapshotted_data.`FirstName` != source_data.`FirstName`
        or
        (
            ((snapshotted_data.`FirstName` is null) and not (source_data.`FirstName` is null))
            or
            ((not snapshotted_data.`FirstName` is null) and (source_data.`FirstName` is null))
        ) or snapshotted_data.`MiddleName` != source_data.`MiddleName`
        or
        (
            ((snapshotted_data.`MiddleName` is null) and not (source_data.`MiddleName` is null))
            or
            ((not snapshotted_data.`MiddleName` is null) and (source_data.`MiddleName` is null))
        ) or snapshotted_data.`LastName` != source_data.`LastName`
        or
        (
            ((snapshotted_data.`LastName` is null) and not (source_data.`LastName` is null))
            or
            ((not snapshotted_data.`LastName` is null) and (source_data.`LastName` is null))
        ) or snapshotted_data.`Suffix` != source_data.`Suffix`
        or
        (
            ((snapshotted_data.`Suffix` is null) and not (source_data.`Suffix` is null))
            or
            ((not snapshotted_data.`Suffix` is null) and (source_data.`Suffix` is null))
        ) or snapshotted_data.`CompanyName` != source_data.`CompanyName`
        or
        (
            ((snapshotted_data.`CompanyName` is null) and not (source_data.`CompanyName` is null))
            or
            ((not snapshotted_data.`CompanyName` is null) and (source_data.`CompanyName` is null))
        ) or snapshotted_data.`SalesPerson` != source_data.`SalesPerson`
        or
        (
            ((snapshotted_data.`SalesPerson` is null) and not (source_data.`SalesPerson` is null))
            or
            ((not snapshotted_data.`SalesPerson` is null) and (source_data.`SalesPerson` is null))
        ) or snapshotted_data.`EmailAddress` != source_data.`EmailAddress`
        or
        (
            ((snapshotted_data.`EmailAddress` is null) and not (source_data.`EmailAddress` is null))
            or
            ((not snapshotted_data.`EmailAddress` is null) and (source_data.`EmailAddress` is null))
        ) or snapshotted_data.`Phone` != source_data.`Phone`
        or
        (
            ((snapshotted_data.`Phone` is null) and not (source_data.`Phone` is null))
            or
            ((not snapshotted_data.`Phone` is null) and (source_data.`Phone` is null))
        ) or snapshotted_data.`PasswordHash` != source_data.`PasswordHash`
        or
        (
            ((snapshotted_data.`PasswordHash` is null) and not (source_data.`PasswordHash` is null))
            or
            ((not snapshotted_data.`PasswordHash` is null) and (source_data.`PasswordHash` is null))
        ) or snapshotted_data.`PasswordSalt` != source_data.`PasswordSalt`
        or
        (
            ((snapshotted_data.`PasswordSalt` is null) and not (source_data.`PasswordSalt` is null))
            or
            ((not snapshotted_data.`PasswordSalt` is null) and (source_data.`PasswordSalt` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`NameStyle` != source_data.`NameStyle`
        or
        (
            ((snapshotted_data.`NameStyle` is null) and not (source_data.`NameStyle` is null))
            or
            ((not snapshotted_data.`NameStyle` is null) and (source_data.`NameStyle` is null))
        ) or snapshotted_data.`Title` != source_data.`Title`
        or
        (
            ((snapshotted_data.`Title` is null) and not (source_data.`Title` is null))
            or
            ((not snapshotted_data.`Title` is null) and (source_data.`Title` is null))
        ) or snapshotted_data.`FirstName` != source_data.`FirstName`
        or
        (
            ((snapshotted_data.`FirstName` is null) and not (source_data.`FirstName` is null))
            or
            ((not snapshotted_data.`FirstName` is null) and (source_data.`FirstName` is null))
        ) or snapshotted_data.`MiddleName` != source_data.`MiddleName`
        or
        (
            ((snapshotted_data.`MiddleName` is null) and not (source_data.`MiddleName` is null))
            or
            ((not snapshotted_data.`MiddleName` is null) and (source_data.`MiddleName` is null))
        ) or snapshotted_data.`LastName` != source_data.`LastName`
        or
        (
            ((snapshotted_data.`LastName` is null) and not (source_data.`LastName` is null))
            or
            ((not snapshotted_data.`LastName` is null) and (source_data.`LastName` is null))
        ) or snapshotted_data.`Suffix` != source_data.`Suffix`
        or
        (
            ((snapshotted_data.`Suffix` is null) and not (source_data.`Suffix` is null))
            or
            ((not snapshotted_data.`Suffix` is null) and (source_data.`Suffix` is null))
        ) or snapshotted_data.`CompanyName` != source_data.`CompanyName`
        or
        (
            ((snapshotted_data.`CompanyName` is null) and not (source_data.`CompanyName` is null))
            or
            ((not snapshotted_data.`CompanyName` is null) and (source_data.`CompanyName` is null))
        ) or snapshotted_data.`SalesPerson` != source_data.`SalesPerson`
        or
        (
            ((snapshotted_data.`SalesPerson` is null) and not (source_data.`SalesPerson` is null))
            or
            ((not snapshotted_data.`SalesPerson` is null) and (source_data.`SalesPerson` is null))
        ) or snapshotted_data.`EmailAddress` != source_data.`EmailAddress`
        or
        (
            ((snapshotted_data.`EmailAddress` is null) and not (source_data.`EmailAddress` is null))
            or
            ((not snapshotted_data.`EmailAddress` is null) and (source_data.`EmailAddress` is null))
        ) or snapshotted_data.`Phone` != source_data.`Phone`
        or
        (
            ((snapshotted_data.`Phone` is null) and not (source_data.`Phone` is null))
            or
            ((not snapshotted_data.`Phone` is null) and (source_data.`Phone` is null))
        ) or snapshotted_data.`PasswordHash` != source_data.`PasswordHash`
        or
        (
            ((snapshotted_data.`PasswordHash` is null) and not (source_data.`PasswordHash` is null))
            or
            ((not snapshotted_data.`PasswordHash` is null) and (source_data.`PasswordHash` is null))
        ) or snapshotted_data.`PasswordSalt` != source_data.`PasswordSalt`
        or
        (
            ((snapshotted_data.`PasswordSalt` is null) and not (source_data.`PasswordSalt` is null))
            or
            ((not snapshotted_data.`PasswordSalt` is null) and (source_data.`PasswordSalt` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m13:38:52.908496 [debug] [Thread-1  ]: SQL status: OK in 0.5299999713897705 seconds
[0m13:38:52.915603 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customer_snapshot"
[0m13:38:52.916500 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp`
  
[0m13:38:53.099172 [debug] [Thread-1  ]: SQL status: OK in 0.18000000715255737 seconds
[0m13:38:53.106171 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customer_snapshot"
[0m13:38:53.106171 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m13:38:53.867850 [debug] [Thread-1  ]: SQL status: OK in 0.7599999904632568 seconds
[0m13:38:53.872850 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customer_snapshot"
[0m13:38:53.873851 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp`
  
[0m13:38:54.041881 [debug] [Thread-1  ]: SQL status: OK in 0.17000000178813934 seconds
[0m13:38:54.048881 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customer_snapshot"
[0m13:38:54.048881 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m13:38:54.585776 [debug] [Thread-1  ]: SQL status: OK in 0.5400000214576721 seconds
[0m13:38:54.590778 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customer_snapshot"
[0m13:38:54.590778 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp`
  
[0m13:38:54.769364 [debug] [Thread-1  ]: SQL status: OK in 0.18000000715255737 seconds
[0m13:38:54.772361 [debug] [Thread-1  ]: Writing runtime SQL for node "snapshot.dbt_spark_modeling.customer_snapshot"
[0m13:38:54.773363 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customer_snapshot"
[0m13:38:54.774362 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customer_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`customer_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m13:39:00.291509 [debug] [Thread-1  ]: SQL status: OK in 5.519999980926514 seconds
[0m13:39:00.362402 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customer_snapshot"
[0m13:39:00.363404 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customer_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp`
[0m13:39:00.954853 [debug] [Thread-1  ]: SQL status: OK in 0.5899999737739563 seconds
[0m13:39:00.958853 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m13:39:00.959854 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.customer_snapshot (execute): 13:38:50.507955 => 13:39:00.959854
[0m13:39:00.960854 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.customer_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 1, idle time: 10.461893796920776s
[0m13:39:00.961852 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.customer_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.0009984970092773438s
[0m13:39:00.961852 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a793bc50-848b-41ca-b416-eb0a7cc49140', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DA3D7D910>]}
[0m13:39:00.962853 [info ] [Thread-1  ]: 2 of 8 OK snapshotted snapshots.customer_snapshot .............................. [[32mOK[0m in 10.46s]
[0m13:39:00.964862 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.customer_snapshot
[0m13:39:00.964862 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.customeraddress_snapshot
[0m13:39:00.965859 [info ] [Thread-1  ]: 3 of 8 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m13:39:00.967854 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.customer_snapshot, now snapshot.dbt_spark_modeling.customeraddress_snapshot)
[0m13:39:00.967854 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.customeraddress_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.006001710891723633s
[0m13:39:00.968851 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.customeraddress_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.006001710891723633s
[0m13:39:00.968851 [debug] [Thread-1  ]: Databricks adapter: On thread (15524, 19148): `hive_metastore`.`snapshots`.`customeraddress_snapshot` using default compute resource.
[0m13:39:00.968851 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.customeraddress_snapshot
[0m13:39:00.972852 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.customeraddress_snapshot (compile): 13:39:00.968851 => 13:39:00.971853
[0m13:39:00.972852 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.customeraddress_snapshot
[0m13:39:00.976855 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customeraddress_snapshot"
[0m13:39:00.976855 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m13:39:02.314793 [debug] [Thread-1  ]: SQL status: OK in 1.340000033378601 seconds
[0m13:39:02.319793 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customeraddress_snapshot"
[0m13:39:02.320793 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customeraddress_snapshot"} */
select * from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) as __dbt_sbq
    where false
    limit 0

    
[0m13:39:02.852104 [debug] [Thread-1  ]: SQL status: OK in 0.5299999713897705 seconds
[0m13:39:02.858102 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customeraddress_snapshot"
[0m13:39:02.859103 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m13:39:03.348093 [debug] [Thread-1  ]: SQL status: OK in 0.49000000953674316 seconds
[0m13:39:03.355093 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customeraddress_snapshot"
[0m13:39:03.356095 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m13:39:03.810582 [debug] [Thread-1  ]: SQL status: OK in 0.44999998807907104 seconds
[0m13:39:03.819581 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customeraddress_snapshot"
[0m13:39:03.820582 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customeraddress_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data


    ),

    snapshotted_data as (

        select *,
            CustomerId||'-'||AddressId as dbt_unique_key

        from `hive_metastore`.`snapshots`.`customeraddress_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            CustomerId||'-'||AddressId as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`AddressId` != source_data.`AddressId`
        or
        (
            ((snapshotted_data.`AddressId` is null) and not (source_data.`AddressId` is null))
            or
            ((not snapshotted_data.`AddressId` is null) and (source_data.`AddressId` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`AddressId` != source_data.`AddressId`
        or
        (
            ((snapshotted_data.`AddressId` is null) and not (source_data.`AddressId` is null))
            or
            ((not snapshotted_data.`AddressId` is null) and (source_data.`AddressId` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m13:39:04.273445 [debug] [Thread-1  ]: SQL status: OK in 0.44999998807907104 seconds
[0m13:39:04.279446 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customeraddress_snapshot"
[0m13:39:04.280449 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m13:39:04.459578 [debug] [Thread-1  ]: SQL status: OK in 0.18000000715255737 seconds
[0m13:39:04.466578 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customeraddress_snapshot"
[0m13:39:04.467577 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m13:39:04.913999 [debug] [Thread-1  ]: SQL status: OK in 0.44999998807907104 seconds
[0m13:39:04.921001 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customeraddress_snapshot"
[0m13:39:04.922001 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m13:39:05.139968 [debug] [Thread-1  ]: SQL status: OK in 0.2199999988079071 seconds
[0m13:39:05.147964 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customeraddress_snapshot"
[0m13:39:05.148966 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m13:39:05.614654 [debug] [Thread-1  ]: SQL status: OK in 0.46000000834465027 seconds
[0m13:39:05.621652 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customeraddress_snapshot"
[0m13:39:05.622651 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m13:39:05.785315 [debug] [Thread-1  ]: SQL status: OK in 0.1599999964237213 seconds
[0m13:39:05.790312 [debug] [Thread-1  ]: Writing runtime SQL for node "snapshot.dbt_spark_modeling.customeraddress_snapshot"
[0m13:39:05.791313 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customeraddress_snapshot"
[0m13:39:05.793311 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customeraddress_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`customeraddress_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m13:39:10.604203 [debug] [Thread-1  ]: SQL status: OK in 4.809999942779541 seconds
[0m13:39:10.610201 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customeraddress_snapshot"
[0m13:39:10.611203 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customeraddress_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m13:39:11.209912 [debug] [Thread-1  ]: SQL status: OK in 0.6000000238418579 seconds
[0m13:39:11.212908 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m13:39:11.214912 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.customeraddress_snapshot (execute): 13:39:00.972852 => 13:39:11.213910
[0m13:39:11.214912 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.customeraddress_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 1, idle time: 10.253059387207031s
[0m13:39:11.215912 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.customeraddress_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m13:39:11.216912 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a793bc50-848b-41ca-b416-eb0a7cc49140', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DA3DC75B0>]}
[0m13:39:11.217911 [info ] [Thread-1  ]: 3 of 8 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 10.25s]
[0m13:39:11.218908 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.customeraddress_snapshot
[0m13:39:11.219904 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.product_snapshot
[0m13:39:11.219904 [info ] [Thread-1  ]: 4 of 8 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m13:39:11.220906 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.customeraddress_snapshot, now snapshot.dbt_spark_modeling.product_snapshot)
[0m13:39:11.220906 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.product_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.003993988037109375s
[0m13:39:11.221908 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.product_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.004995584487915039s
[0m13:39:11.221908 [debug] [Thread-1  ]: Databricks adapter: On thread (15524, 19148): `hive_metastore`.`snapshots`.`product_snapshot` using default compute resource.
[0m13:39:11.221908 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.product_snapshot
[0m13:39:11.225906 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.product_snapshot (compile): 13:39:11.222906 => 13:39:11.225906
[0m13:39:11.225906 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.product_snapshot
[0m13:39:11.232904 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.product_snapshot"
[0m13:39:11.233904 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m13:39:11.858497 [debug] [Thread-1  ]: SQL status: OK in 0.6200000047683716 seconds
[0m13:39:11.866496 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.product_snapshot"
[0m13:39:11.867500 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.product_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m13:39:12.066367 [debug] [Thread-1  ]: SQL status: OK in 0.20000000298023224 seconds
[0m13:39:12.078384 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.product_snapshot"
[0m13:39:12.078384 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m13:39:12.791572 [debug] [Thread-1  ]: SQL status: OK in 0.7099999785423279 seconds
[0m13:39:12.799572 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.product_snapshot"
[0m13:39:12.800572 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m13:39:13.384153 [debug] [Thread-1  ]: SQL status: OK in 0.5799999833106995 seconds
[0m13:39:13.393157 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.product_snapshot"
[0m13:39:13.395157 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.product_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot


    ),

    snapshotted_data as (

        select *,
            ProductID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`product_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            ProductID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(ProductID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            ProductID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            ProductID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`ProductNumber` != source_data.`ProductNumber`
        or
        (
            ((snapshotted_data.`ProductNumber` is null) and not (source_data.`ProductNumber` is null))
            or
            ((not snapshotted_data.`ProductNumber` is null) and (source_data.`ProductNumber` is null))
        ) or snapshotted_data.`Color` != source_data.`Color`
        or
        (
            ((snapshotted_data.`Color` is null) and not (source_data.`Color` is null))
            or
            ((not snapshotted_data.`Color` is null) and (source_data.`Color` is null))
        ) or snapshotted_data.`StandardCost` != source_data.`StandardCost`
        or
        (
            ((snapshotted_data.`StandardCost` is null) and not (source_data.`StandardCost` is null))
            or
            ((not snapshotted_data.`StandardCost` is null) and (source_data.`StandardCost` is null))
        ) or snapshotted_data.`ListPrice` != source_data.`ListPrice`
        or
        (
            ((snapshotted_data.`ListPrice` is null) and not (source_data.`ListPrice` is null))
            or
            ((not snapshotted_data.`ListPrice` is null) and (source_data.`ListPrice` is null))
        ) or snapshotted_data.`Size` != source_data.`Size`
        or
        (
            ((snapshotted_data.`Size` is null) and not (source_data.`Size` is null))
            or
            ((not snapshotted_data.`Size` is null) and (source_data.`Size` is null))
        ) or snapshotted_data.`Weight` != source_data.`Weight`
        or
        (
            ((snapshotted_data.`Weight` is null) and not (source_data.`Weight` is null))
            or
            ((not snapshotted_data.`Weight` is null) and (source_data.`Weight` is null))
        ) or snapshotted_data.`ProductCategoryID` != source_data.`ProductCategoryID`
        or
        (
            ((snapshotted_data.`ProductCategoryID` is null) and not (source_data.`ProductCategoryID` is null))
            or
            ((not snapshotted_data.`ProductCategoryID` is null) and (source_data.`ProductCategoryID` is null))
        ) or snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`SellStartDate` != source_data.`SellStartDate`
        or
        (
            ((snapshotted_data.`SellStartDate` is null) and not (source_data.`SellStartDate` is null))
            or
            ((not snapshotted_data.`SellStartDate` is null) and (source_data.`SellStartDate` is null))
        ) or snapshotted_data.`SellEndDate` != source_data.`SellEndDate`
        or
        (
            ((snapshotted_data.`SellEndDate` is null) and not (source_data.`SellEndDate` is null))
            or
            ((not snapshotted_data.`SellEndDate` is null) and (source_data.`SellEndDate` is null))
        ) or snapshotted_data.`DiscontinuedDate` != source_data.`DiscontinuedDate`
        or
        (
            ((snapshotted_data.`DiscontinuedDate` is null) and not (source_data.`DiscontinuedDate` is null))
            or
            ((not snapshotted_data.`DiscontinuedDate` is null) and (source_data.`DiscontinuedDate` is null))
        ) or snapshotted_data.`ThumbNailPhoto` != source_data.`ThumbNailPhoto`
        or
        (
            ((snapshotted_data.`ThumbNailPhoto` is null) and not (source_data.`ThumbNailPhoto` is null))
            or
            ((not snapshotted_data.`ThumbNailPhoto` is null) and (source_data.`ThumbNailPhoto` is null))
        ) or snapshotted_data.`ThumbnailPhotoFileName` != source_data.`ThumbnailPhotoFileName`
        or
        (
            ((snapshotted_data.`ThumbnailPhotoFileName` is null) and not (source_data.`ThumbnailPhotoFileName` is null))
            or
            ((not snapshotted_data.`ThumbnailPhotoFileName` is null) and (source_data.`ThumbnailPhotoFileName` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`ProductNumber` != source_data.`ProductNumber`
        or
        (
            ((snapshotted_data.`ProductNumber` is null) and not (source_data.`ProductNumber` is null))
            or
            ((not snapshotted_data.`ProductNumber` is null) and (source_data.`ProductNumber` is null))
        ) or snapshotted_data.`Color` != source_data.`Color`
        or
        (
            ((snapshotted_data.`Color` is null) and not (source_data.`Color` is null))
            or
            ((not snapshotted_data.`Color` is null) and (source_data.`Color` is null))
        ) or snapshotted_data.`StandardCost` != source_data.`StandardCost`
        or
        (
            ((snapshotted_data.`StandardCost` is null) and not (source_data.`StandardCost` is null))
            or
            ((not snapshotted_data.`StandardCost` is null) and (source_data.`StandardCost` is null))
        ) or snapshotted_data.`ListPrice` != source_data.`ListPrice`
        or
        (
            ((snapshotted_data.`ListPrice` is null) and not (source_data.`ListPrice` is null))
            or
            ((not snapshotted_data.`ListPrice` is null) and (source_data.`ListPrice` is null))
        ) or snapshotted_data.`Size` != source_data.`Size`
        or
        (
            ((snapshotted_data.`Size` is null) and not (source_data.`Size` is null))
            or
            ((not snapshotted_data.`Size` is null) and (source_data.`Size` is null))
        ) or snapshotted_data.`Weight` != source_data.`Weight`
        or
        (
            ((snapshotted_data.`Weight` is null) and not (source_data.`Weight` is null))
            or
            ((not snapshotted_data.`Weight` is null) and (source_data.`Weight` is null))
        ) or snapshotted_data.`ProductCategoryID` != source_data.`ProductCategoryID`
        or
        (
            ((snapshotted_data.`ProductCategoryID` is null) and not (source_data.`ProductCategoryID` is null))
            or
            ((not snapshotted_data.`ProductCategoryID` is null) and (source_data.`ProductCategoryID` is null))
        ) or snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`SellStartDate` != source_data.`SellStartDate`
        or
        (
            ((snapshotted_data.`SellStartDate` is null) and not (source_data.`SellStartDate` is null))
            or
            ((not snapshotted_data.`SellStartDate` is null) and (source_data.`SellStartDate` is null))
        ) or snapshotted_data.`SellEndDate` != source_data.`SellEndDate`
        or
        (
            ((snapshotted_data.`SellEndDate` is null) and not (source_data.`SellEndDate` is null))
            or
            ((not snapshotted_data.`SellEndDate` is null) and (source_data.`SellEndDate` is null))
        ) or snapshotted_data.`DiscontinuedDate` != source_data.`DiscontinuedDate`
        or
        (
            ((snapshotted_data.`DiscontinuedDate` is null) and not (source_data.`DiscontinuedDate` is null))
            or
            ((not snapshotted_data.`DiscontinuedDate` is null) and (source_data.`DiscontinuedDate` is null))
        ) or snapshotted_data.`ThumbNailPhoto` != source_data.`ThumbNailPhoto`
        or
        (
            ((snapshotted_data.`ThumbNailPhoto` is null) and not (source_data.`ThumbNailPhoto` is null))
            or
            ((not snapshotted_data.`ThumbNailPhoto` is null) and (source_data.`ThumbNailPhoto` is null))
        ) or snapshotted_data.`ThumbnailPhotoFileName` != source_data.`ThumbnailPhotoFileName`
        or
        (
            ((snapshotted_data.`ThumbnailPhotoFileName` is null) and not (source_data.`ThumbnailPhotoFileName` is null))
            or
            ((not snapshotted_data.`ThumbnailPhotoFileName` is null) and (source_data.`ThumbnailPhotoFileName` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m13:39:13.924546 [debug] [Thread-1  ]: SQL status: OK in 0.5299999713897705 seconds
[0m13:39:13.931548 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.product_snapshot"
[0m13:39:13.931548 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp`
  
[0m13:39:14.100183 [debug] [Thread-1  ]: SQL status: OK in 0.17000000178813934 seconds
[0m13:39:14.107188 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.product_snapshot"
[0m13:39:14.108192 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m13:39:14.587666 [debug] [Thread-1  ]: SQL status: OK in 0.47999998927116394 seconds
[0m13:39:14.595671 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.product_snapshot"
[0m13:39:14.595671 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp`
  
[0m13:39:14.760797 [debug] [Thread-1  ]: SQL status: OK in 0.1599999964237213 seconds
[0m13:39:14.766791 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.product_snapshot"
[0m13:39:14.767793 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m13:39:15.225637 [debug] [Thread-1  ]: SQL status: OK in 0.46000000834465027 seconds
[0m13:39:15.234629 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.product_snapshot"
[0m13:39:15.234629 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp`
  
[0m13:39:15.401932 [debug] [Thread-1  ]: SQL status: OK in 0.17000000178813934 seconds
[0m13:39:15.407930 [debug] [Thread-1  ]: Writing runtime SQL for node "snapshot.dbt_spark_modeling.product_snapshot"
[0m13:39:15.408932 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.product_snapshot"
[0m13:39:15.409939 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.product_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`product_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m13:39:20.882926 [debug] [Thread-1  ]: SQL status: OK in 5.46999979019165 seconds
[0m13:39:20.959838 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.product_snapshot"
[0m13:39:20.960843 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.product_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp`
[0m13:39:21.509207 [debug] [Thread-1  ]: SQL status: OK in 0.550000011920929 seconds
[0m13:39:21.513212 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m13:39:21.515216 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.product_snapshot (execute): 13:39:11.226903 => 13:39:21.514215
[0m13:39:21.515216 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.product_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 1, idle time: 10.298303604125977s
[0m13:39:21.516215 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.product_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m13:39:21.517216 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a793bc50-848b-41ca-b416-eb0a7cc49140', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DA3CC6700>]}
[0m13:39:21.518216 [info ] [Thread-1  ]: 4 of 8 OK snapshotted snapshots.product_snapshot ............................... [[32mOK[0m in 10.30s]
[0m13:39:21.519212 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.product_snapshot
[0m13:39:21.520209 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.productcategory_snapshot
[0m13:39:21.521210 [info ] [Thread-1  ]: 5 of 8 START snapshot snapshots.productcategory_snapshot ....................... [RUN]
[0m13:39:21.522214 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.product_snapshot, now snapshot.dbt_spark_modeling.productcategory_snapshot)
[0m13:39:21.522214 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.productcategory_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.004998207092285156s
[0m13:39:21.522214 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.productcategory_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.004998207092285156s
[0m13:39:21.523212 [debug] [Thread-1  ]: Databricks adapter: On thread (15524, 19148): `hive_metastore`.`snapshots`.`productcategory_snapshot` using default compute resource.
[0m13:39:21.523212 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.productcategory_snapshot
[0m13:39:21.528210 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.productcategory_snapshot (compile): 13:39:21.523212 => 13:39:21.528210
[0m13:39:21.529209 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.productcategory_snapshot
[0m13:39:21.564207 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.dbt_spark_modeling.productcategory_snapshot"
[0m13:39:21.565207 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productcategory_snapshot"
[0m13:39:21.566209 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productcategory_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productcategory_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`productcategory_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/productcategory/productcategory_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(ProductCategoryID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with product_snapshot as (
    SELECT
        ProductCategoryID,
        Name
    FROM `hive_metastore`.`saleslt`.`productcategory`
    WHERE ParentProductCategoryID is not null
)

select * from product_snapshot

    ) sbq



  
      
[0m13:39:24.720794 [debug] [Thread-1  ]: SQL status: OK in 3.1500000953674316 seconds
[0m13:39:24.724792 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m13:39:24.725794 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.productcategory_snapshot (execute): 13:39:21.529209 => 13:39:24.725794
[0m13:39:24.726794 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.productcategory_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 1, idle time: 3.208578109741211s
[0m13:39:24.728787 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.productcategory_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m13:39:24.728787 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a793bc50-848b-41ca-b416-eb0a7cc49140', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DA3DD9C70>]}
[0m13:39:24.729786 [info ] [Thread-1  ]: 5 of 8 OK snapshotted snapshots.productcategory_snapshot ....................... [[32mOK[0m in 3.21s]
[0m13:39:24.730792 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.productcategory_snapshot
[0m13:39:24.731786 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.productmodel_snapshot
[0m13:39:24.732785 [info ] [Thread-1  ]: 6 of 8 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m13:39:24.732785 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.productcategory_snapshot, now snapshot.dbt_spark_modeling.productmodel_snapshot)
[0m13:39:24.733789 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.productmodel_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.005001068115234375s
[0m13:39:24.733789 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.productmodel_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.005001068115234375s
[0m13:39:24.734788 [debug] [Thread-1  ]: Databricks adapter: On thread (15524, 19148): `hive_metastore`.`snapshots`.`productmodel_snapshot` using default compute resource.
[0m13:39:24.734788 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.productmodel_snapshot
[0m13:39:24.739786 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.productmodel_snapshot (compile): 13:39:24.734788 => 13:39:24.738787
[0m13:39:24.739786 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.productmodel_snapshot
[0m13:39:24.746786 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productmodel_snapshot"
[0m13:39:24.746786 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m13:39:25.554442 [debug] [Thread-1  ]: SQL status: OK in 0.8100000023841858 seconds
[0m13:39:25.561445 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productmodel_snapshot"
[0m13:39:25.562445 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productmodel_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m13:39:25.753304 [debug] [Thread-1  ]: SQL status: OK in 0.1899999976158142 seconds
[0m13:39:25.759305 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productmodel_snapshot"
[0m13:39:25.761311 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m13:39:26.231356 [debug] [Thread-1  ]: SQL status: OK in 0.4699999988079071 seconds
[0m13:39:26.239355 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productmodel_snapshot"
[0m13:39:26.240358 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m13:39:26.670387 [debug] [Thread-1  ]: SQL status: OK in 0.4300000071525574 seconds
[0m13:39:26.679384 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productmodel_snapshot"
[0m13:39:26.680387 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productmodel_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot


    ),

    snapshotted_data as (

        select *,
            ProductModelID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`productmodel_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m13:39:27.143003 [debug] [Thread-1  ]: SQL status: OK in 0.46000000834465027 seconds
[0m13:39:27.147006 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productmodel_snapshot"
[0m13:39:27.148007 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m13:39:27.311531 [debug] [Thread-1  ]: SQL status: OK in 0.1599999964237213 seconds
[0m13:39:27.317532 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productmodel_snapshot"
[0m13:39:27.318533 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m13:39:27.747654 [debug] [Thread-1  ]: SQL status: OK in 0.4300000071525574 seconds
[0m13:39:27.756654 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productmodel_snapshot"
[0m13:39:27.756654 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m13:39:27.915157 [debug] [Thread-1  ]: SQL status: OK in 0.1599999964237213 seconds
[0m13:39:27.921155 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productmodel_snapshot"
[0m13:39:27.922154 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m13:39:28.339296 [debug] [Thread-1  ]: SQL status: OK in 0.41999998688697815 seconds
[0m13:39:28.346290 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productmodel_snapshot"
[0m13:39:28.347293 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m13:39:28.515251 [debug] [Thread-1  ]: SQL status: OK in 0.17000000178813934 seconds
[0m13:39:28.520251 [debug] [Thread-1  ]: Writing runtime SQL for node "snapshot.dbt_spark_modeling.productmodel_snapshot"
[0m13:39:28.521259 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productmodel_snapshot"
[0m13:39:28.522255 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productmodel_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`productmodel_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m13:39:32.897475 [debug] [Thread-1  ]: SQL status: OK in 4.380000114440918 seconds
[0m13:39:32.901477 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productmodel_snapshot"
[0m13:39:32.902477 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productmodel_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m13:39:33.424046 [debug] [Thread-1  ]: SQL status: OK in 0.5199999809265137 seconds
[0m13:39:33.428040 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m13:39:33.430040 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.productmodel_snapshot (execute): 13:39:24.740932 => 13:39:33.429044
[0m13:39:33.430040 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.productmodel_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 1, idle time: 8.701252698898315s
[0m13:39:33.431040 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.productmodel_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m13:39:33.432038 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a793bc50-848b-41ca-b416-eb0a7cc49140', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DA4F89EB0>]}
[0m13:39:33.433038 [info ] [Thread-1  ]: 6 of 8 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 8.70s]
[0m13:39:33.434041 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.productmodel_snapshot
[0m13:39:33.434041 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.salesorderdetail_snapshot
[0m13:39:33.435039 [info ] [Thread-1  ]: 7 of 8 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m13:39:33.436037 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.productmodel_snapshot, now snapshot.dbt_spark_modeling.salesorderdetail_snapshot)
[0m13:39:33.436037 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.salesorderdetail_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.0039997100830078125s
[0m13:39:33.437036 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.salesorderdetail_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.0039997100830078125s
[0m13:39:33.437036 [debug] [Thread-1  ]: Databricks adapter: On thread (15524, 19148): `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` using default compute resource.
[0m13:39:33.437036 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.salesorderdetail_snapshot
[0m13:39:33.442038 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.salesorderdetail_snapshot (compile): 13:39:33.438038 => 13:39:33.442038
[0m13:39:33.443038 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.salesorderdetail_snapshot
[0m13:39:33.449037 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"
[0m13:39:33.450036 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m13:39:34.436502 [debug] [Thread-1  ]: SQL status: OK in 0.9900000095367432 seconds
[0m13:39:34.442503 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"
[0m13:39:34.443504 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"} */
select * from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m13:39:34.775292 [debug] [Thread-1  ]: SQL status: OK in 0.33000001311302185 seconds
[0m13:39:34.781291 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"
[0m13:39:34.781291 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m13:39:35.196283 [debug] [Thread-1  ]: SQL status: OK in 0.4099999964237213 seconds
[0m13:39:35.203284 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"
[0m13:39:35.204282 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m13:39:35.832284 [debug] [Thread-1  ]: SQL status: OK in 0.6299999952316284 seconds
[0m13:39:35.841282 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"
[0m13:39:35.842286 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot


    ),

    snapshotted_data as (

        select *,
            SalesOrderDetailID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m13:39:36.353285 [debug] [Thread-1  ]: SQL status: OK in 0.5099999904632568 seconds
[0m13:39:36.357283 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"
[0m13:39:36.358282 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m13:39:36.520375 [debug] [Thread-1  ]: SQL status: OK in 0.1599999964237213 seconds
[0m13:39:36.527378 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"
[0m13:39:36.527378 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m13:39:37.124930 [debug] [Thread-1  ]: SQL status: OK in 0.6000000238418579 seconds
[0m13:39:37.131928 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"
[0m13:39:37.132929 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m13:39:37.312477 [debug] [Thread-1  ]: SQL status: OK in 0.18000000715255737 seconds
[0m13:39:37.319476 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"
[0m13:39:37.320476 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m13:39:37.741156 [debug] [Thread-1  ]: SQL status: OK in 0.41999998688697815 seconds
[0m13:39:37.751710 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"
[0m13:39:37.751710 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m13:39:37.926923 [debug] [Thread-1  ]: SQL status: OK in 0.17000000178813934 seconds
[0m13:39:37.931929 [debug] [Thread-1  ]: Writing runtime SQL for node "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"
[0m13:39:37.932932 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"
[0m13:39:37.933931 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m13:39:42.693513 [debug] [Thread-1  ]: SQL status: OK in 4.760000228881836 seconds
[0m13:39:42.697512 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"
[0m13:39:42.697512 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m13:39:43.221473 [debug] [Thread-1  ]: SQL status: OK in 0.5199999809265137 seconds
[0m13:39:43.224474 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m13:39:43.226474 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.salesorderdetail_snapshot (execute): 13:39:33.443038 => 13:39:43.225474
[0m13:39:43.226474 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.salesorderdetail_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 1, idle time: 9.79443621635437s
[0m13:39:43.228477 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.salesorderdetail_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m13:39:43.228477 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a793bc50-848b-41ca-b416-eb0a7cc49140', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DA39CBCD0>]}
[0m13:39:43.229473 [info ] [Thread-1  ]: 7 of 8 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 9.79s]
[0m13:39:43.231484 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.salesorderdetail_snapshot
[0m13:39:43.231484 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.salesorderheader_snapshot
[0m13:39:43.232477 [info ] [Thread-1  ]: 8 of 8 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m13:39:43.233474 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.salesorderdetail_snapshot, now snapshot.dbt_spark_modeling.salesorderheader_snapshot)
[0m13:39:43.234474 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.salesorderheader_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.0059967041015625s
[0m13:39:43.234474 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.salesorderheader_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.0059967041015625s
[0m13:39:43.235472 [debug] [Thread-1  ]: Databricks adapter: On thread (15524, 19148): `hive_metastore`.`snapshots`.`salesorderheader_snapshot` using default compute resource.
[0m13:39:43.235472 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.salesorderheader_snapshot
[0m13:39:43.239472 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.salesorderheader_snapshot (compile): 13:39:43.235472 => 13:39:43.238472
[0m13:39:43.239472 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.salesorderheader_snapshot
[0m13:39:43.243472 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderheader_snapshot"
[0m13:39:43.243472 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m13:39:44.215158 [debug] [Thread-1  ]: SQL status: OK in 0.9700000286102295 seconds
[0m13:39:44.222152 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderheader_snapshot"
[0m13:39:44.223152 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderheader_snapshot"} */
select * from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m13:39:44.388683 [debug] [Thread-1  ]: SQL status: OK in 0.1599999964237213 seconds
[0m13:39:44.396685 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderheader_snapshot"
[0m13:39:44.397687 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m13:39:44.872937 [debug] [Thread-1  ]: SQL status: OK in 0.4699999988079071 seconds
[0m13:39:44.880937 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderheader_snapshot"
[0m13:39:44.881937 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m13:39:45.323139 [debug] [Thread-1  ]: SQL status: OK in 0.4399999976158142 seconds
[0m13:39:45.332136 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderheader_snapshot"
[0m13:39:45.334135 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderheader_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot


    ),

    snapshotted_data as (

        select *,
            SalesOrderID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`RevisionNumber` != source_data.`RevisionNumber`
        or
        (
            ((snapshotted_data.`RevisionNumber` is null) and not (source_data.`RevisionNumber` is null))
            or
            ((not snapshotted_data.`RevisionNumber` is null) and (source_data.`RevisionNumber` is null))
        ) or snapshotted_data.`OrderDate` != source_data.`OrderDate`
        or
        (
            ((snapshotted_data.`OrderDate` is null) and not (source_data.`OrderDate` is null))
            or
            ((not snapshotted_data.`OrderDate` is null) and (source_data.`OrderDate` is null))
        ) or snapshotted_data.`DueDate` != source_data.`DueDate`
        or
        (
            ((snapshotted_data.`DueDate` is null) and not (source_data.`DueDate` is null))
            or
            ((not snapshotted_data.`DueDate` is null) and (source_data.`DueDate` is null))
        ) or snapshotted_data.`ShipDate` != source_data.`ShipDate`
        or
        (
            ((snapshotted_data.`ShipDate` is null) and not (source_data.`ShipDate` is null))
            or
            ((not snapshotted_data.`ShipDate` is null) and (source_data.`ShipDate` is null))
        ) or snapshotted_data.`Status` != source_data.`Status`
        or
        (
            ((snapshotted_data.`Status` is null) and not (source_data.`Status` is null))
            or
            ((not snapshotted_data.`Status` is null) and (source_data.`Status` is null))
        ) or snapshotted_data.`OnlineOrderFlag` != source_data.`OnlineOrderFlag`
        or
        (
            ((snapshotted_data.`OnlineOrderFlag` is null) and not (source_data.`OnlineOrderFlag` is null))
            or
            ((not snapshotted_data.`OnlineOrderFlag` is null) and (source_data.`OnlineOrderFlag` is null))
        ) or snapshotted_data.`SalesOrderNumber` != source_data.`SalesOrderNumber`
        or
        (
            ((snapshotted_data.`SalesOrderNumber` is null) and not (source_data.`SalesOrderNumber` is null))
            or
            ((not snapshotted_data.`SalesOrderNumber` is null) and (source_data.`SalesOrderNumber` is null))
        ) or snapshotted_data.`PurchaseOrderNumber` != source_data.`PurchaseOrderNumber`
        or
        (
            ((snapshotted_data.`PurchaseOrderNumber` is null) and not (source_data.`PurchaseOrderNumber` is null))
            or
            ((not snapshotted_data.`PurchaseOrderNumber` is null) and (source_data.`PurchaseOrderNumber` is null))
        ) or snapshotted_data.`AccountNumber` != source_data.`AccountNumber`
        or
        (
            ((snapshotted_data.`AccountNumber` is null) and not (source_data.`AccountNumber` is null))
            or
            ((not snapshotted_data.`AccountNumber` is null) and (source_data.`AccountNumber` is null))
        ) or snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`ShipToAddressID` != source_data.`ShipToAddressID`
        or
        (
            ((snapshotted_data.`ShipToAddressID` is null) and not (source_data.`ShipToAddressID` is null))
            or
            ((not snapshotted_data.`ShipToAddressID` is null) and (source_data.`ShipToAddressID` is null))
        ) or snapshotted_data.`BillToAddressID` != source_data.`BillToAddressID`
        or
        (
            ((snapshotted_data.`BillToAddressID` is null) and not (source_data.`BillToAddressID` is null))
            or
            ((not snapshotted_data.`BillToAddressID` is null) and (source_data.`BillToAddressID` is null))
        ) or snapshotted_data.`ShipMethod` != source_data.`ShipMethod`
        or
        (
            ((snapshotted_data.`ShipMethod` is null) and not (source_data.`ShipMethod` is null))
            or
            ((not snapshotted_data.`ShipMethod` is null) and (source_data.`ShipMethod` is null))
        ) or snapshotted_data.`CreditCardApprovalCode` != source_data.`CreditCardApprovalCode`
        or
        (
            ((snapshotted_data.`CreditCardApprovalCode` is null) and not (source_data.`CreditCardApprovalCode` is null))
            or
            ((not snapshotted_data.`CreditCardApprovalCode` is null) and (source_data.`CreditCardApprovalCode` is null))
        ) or snapshotted_data.`SubTotal` != source_data.`SubTotal`
        or
        (
            ((snapshotted_data.`SubTotal` is null) and not (source_data.`SubTotal` is null))
            or
            ((not snapshotted_data.`SubTotal` is null) and (source_data.`SubTotal` is null))
        ) or snapshotted_data.`TaxAmt` != source_data.`TaxAmt`
        or
        (
            ((snapshotted_data.`TaxAmt` is null) and not (source_data.`TaxAmt` is null))
            or
            ((not snapshotted_data.`TaxAmt` is null) and (source_data.`TaxAmt` is null))
        ) or snapshotted_data.`Freight` != source_data.`Freight`
        or
        (
            ((snapshotted_data.`Freight` is null) and not (source_data.`Freight` is null))
            or
            ((not snapshotted_data.`Freight` is null) and (source_data.`Freight` is null))
        ) or snapshotted_data.`TotalDue` != source_data.`TotalDue`
        or
        (
            ((snapshotted_data.`TotalDue` is null) and not (source_data.`TotalDue` is null))
            or
            ((not snapshotted_data.`TotalDue` is null) and (source_data.`TotalDue` is null))
        ) or snapshotted_data.`Comment` != source_data.`Comment`
        or
        (
            ((snapshotted_data.`Comment` is null) and not (source_data.`Comment` is null))
            or
            ((not snapshotted_data.`Comment` is null) and (source_data.`Comment` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`RevisionNumber` != source_data.`RevisionNumber`
        or
        (
            ((snapshotted_data.`RevisionNumber` is null) and not (source_data.`RevisionNumber` is null))
            or
            ((not snapshotted_data.`RevisionNumber` is null) and (source_data.`RevisionNumber` is null))
        ) or snapshotted_data.`OrderDate` != source_data.`OrderDate`
        or
        (
            ((snapshotted_data.`OrderDate` is null) and not (source_data.`OrderDate` is null))
            or
            ((not snapshotted_data.`OrderDate` is null) and (source_data.`OrderDate` is null))
        ) or snapshotted_data.`DueDate` != source_data.`DueDate`
        or
        (
            ((snapshotted_data.`DueDate` is null) and not (source_data.`DueDate` is null))
            or
            ((not snapshotted_data.`DueDate` is null) and (source_data.`DueDate` is null))
        ) or snapshotted_data.`ShipDate` != source_data.`ShipDate`
        or
        (
            ((snapshotted_data.`ShipDate` is null) and not (source_data.`ShipDate` is null))
            or
            ((not snapshotted_data.`ShipDate` is null) and (source_data.`ShipDate` is null))
        ) or snapshotted_data.`Status` != source_data.`Status`
        or
        (
            ((snapshotted_data.`Status` is null) and not (source_data.`Status` is null))
            or
            ((not snapshotted_data.`Status` is null) and (source_data.`Status` is null))
        ) or snapshotted_data.`OnlineOrderFlag` != source_data.`OnlineOrderFlag`
        or
        (
            ((snapshotted_data.`OnlineOrderFlag` is null) and not (source_data.`OnlineOrderFlag` is null))
            or
            ((not snapshotted_data.`OnlineOrderFlag` is null) and (source_data.`OnlineOrderFlag` is null))
        ) or snapshotted_data.`SalesOrderNumber` != source_data.`SalesOrderNumber`
        or
        (
            ((snapshotted_data.`SalesOrderNumber` is null) and not (source_data.`SalesOrderNumber` is null))
            or
            ((not snapshotted_data.`SalesOrderNumber` is null) and (source_data.`SalesOrderNumber` is null))
        ) or snapshotted_data.`PurchaseOrderNumber` != source_data.`PurchaseOrderNumber`
        or
        (
            ((snapshotted_data.`PurchaseOrderNumber` is null) and not (source_data.`PurchaseOrderNumber` is null))
            or
            ((not snapshotted_data.`PurchaseOrderNumber` is null) and (source_data.`PurchaseOrderNumber` is null))
        ) or snapshotted_data.`AccountNumber` != source_data.`AccountNumber`
        or
        (
            ((snapshotted_data.`AccountNumber` is null) and not (source_data.`AccountNumber` is null))
            or
            ((not snapshotted_data.`AccountNumber` is null) and (source_data.`AccountNumber` is null))
        ) or snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`ShipToAddressID` != source_data.`ShipToAddressID`
        or
        (
            ((snapshotted_data.`ShipToAddressID` is null) and not (source_data.`ShipToAddressID` is null))
            or
            ((not snapshotted_data.`ShipToAddressID` is null) and (source_data.`ShipToAddressID` is null))
        ) or snapshotted_data.`BillToAddressID` != source_data.`BillToAddressID`
        or
        (
            ((snapshotted_data.`BillToAddressID` is null) and not (source_data.`BillToAddressID` is null))
            or
            ((not snapshotted_data.`BillToAddressID` is null) and (source_data.`BillToAddressID` is null))
        ) or snapshotted_data.`ShipMethod` != source_data.`ShipMethod`
        or
        (
            ((snapshotted_data.`ShipMethod` is null) and not (source_data.`ShipMethod` is null))
            or
            ((not snapshotted_data.`ShipMethod` is null) and (source_data.`ShipMethod` is null))
        ) or snapshotted_data.`CreditCardApprovalCode` != source_data.`CreditCardApprovalCode`
        or
        (
            ((snapshotted_data.`CreditCardApprovalCode` is null) and not (source_data.`CreditCardApprovalCode` is null))
            or
            ((not snapshotted_data.`CreditCardApprovalCode` is null) and (source_data.`CreditCardApprovalCode` is null))
        ) or snapshotted_data.`SubTotal` != source_data.`SubTotal`
        or
        (
            ((snapshotted_data.`SubTotal` is null) and not (source_data.`SubTotal` is null))
            or
            ((not snapshotted_data.`SubTotal` is null) and (source_data.`SubTotal` is null))
        ) or snapshotted_data.`TaxAmt` != source_data.`TaxAmt`
        or
        (
            ((snapshotted_data.`TaxAmt` is null) and not (source_data.`TaxAmt` is null))
            or
            ((not snapshotted_data.`TaxAmt` is null) and (source_data.`TaxAmt` is null))
        ) or snapshotted_data.`Freight` != source_data.`Freight`
        or
        (
            ((snapshotted_data.`Freight` is null) and not (source_data.`Freight` is null))
            or
            ((not snapshotted_data.`Freight` is null) and (source_data.`Freight` is null))
        ) or snapshotted_data.`TotalDue` != source_data.`TotalDue`
        or
        (
            ((snapshotted_data.`TotalDue` is null) and not (source_data.`TotalDue` is null))
            or
            ((not snapshotted_data.`TotalDue` is null) and (source_data.`TotalDue` is null))
        ) or snapshotted_data.`Comment` != source_data.`Comment`
        or
        (
            ((snapshotted_data.`Comment` is null) and not (source_data.`Comment` is null))
            or
            ((not snapshotted_data.`Comment` is null) and (source_data.`Comment` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m13:39:45.953384 [debug] [Thread-1  ]: SQL status: OK in 0.6200000047683716 seconds
[0m13:39:45.958385 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderheader_snapshot"
[0m13:39:45.959389 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m13:39:46.189765 [debug] [Thread-1  ]: SQL status: OK in 0.23000000417232513 seconds
[0m13:39:46.198766 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderheader_snapshot"
[0m13:39:46.199763 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m13:39:46.933934 [debug] [Thread-1  ]: SQL status: OK in 0.7300000190734863 seconds
[0m13:39:46.941936 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderheader_snapshot"
[0m13:39:46.942935 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m13:39:47.115888 [debug] [Thread-1  ]: SQL status: OK in 0.17000000178813934 seconds
[0m13:39:47.122885 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderheader_snapshot"
[0m13:39:47.123888 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m13:39:47.573823 [debug] [Thread-1  ]: SQL status: OK in 0.44999998807907104 seconds
[0m13:39:47.583827 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderheader_snapshot"
[0m13:39:47.583827 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
  
[0m13:39:47.770851 [debug] [Thread-1  ]: SQL status: OK in 0.1899999976158142 seconds
[0m13:39:47.776848 [debug] [Thread-1  ]: Writing runtime SQL for node "snapshot.dbt_spark_modeling.salesorderheader_snapshot"
[0m13:39:47.777847 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderheader_snapshot"
[0m13:39:47.778849 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderheader_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderheader_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m13:39:52.852717 [debug] [Thread-1  ]: SQL status: OK in 5.070000171661377 seconds
[0m13:39:52.857718 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderheader_snapshot"
[0m13:39:52.858718 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderheader_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderheader_snapshot__dbt_tmp`
[0m13:39:53.635450 [debug] [Thread-1  ]: SQL status: OK in 0.7799999713897705 seconds
[0m13:39:53.638450 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m13:39:53.640451 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.salesorderheader_snapshot (execute): 13:39:43.239472 => 13:39:53.639451
[0m13:39:53.640451 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.salesorderheader_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 1, idle time: 10.411973714828491s
[0m13:39:53.641450 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.salesorderheader_snapshot, thread: (15524, 19148), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m13:39:53.642450 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a793bc50-848b-41ca-b416-eb0a7cc49140', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DA4E43850>]}
[0m13:39:53.643451 [info ] [Thread-1  ]: 8 of 8 OK snapshotted snapshots.salesorderheader_snapshot ...................... [[32mOK[0m in 10.41s]
[0m13:39:53.657451 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.salesorderheader_snapshot
[0m13:39:53.661450 [debug] [MainThread]: Databricks adapter: Reusing DatabricksDBTConnection. name: master, thread: (15524, 24048), compute: ``, acquire_release_count: 0, idle time: 78.09650444984436s
[0m13:39:53.662449 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (15524, 24048), compute: ``, acquire_release_count: 0, idle time: 78.09750366210938s
[0m13:39:53.662449 [debug] [MainThread]: Databricks adapter: Thread (15524, 24048) using default compute resource.
[0m13:39:53.745854 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=GET_SCHEMAS, getHandleIdentifier()=1d14ad01-0360-4179-9802-7a86a753166e]
[0m13:39:53.745854 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=GET_SCHEMAS, getHandleIdentifier()=1d14ad01-0360-4179-9802-7a86a753166e]
[0m13:39:53.838958 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=GET_TABLES, getHandleIdentifier()=069d6c8e-ec78-4969-ab61-3050d0c519cc]
[0m13:39:53.839960 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=GET_TABLES, getHandleIdentifier()=069d6c8e-ec78-4969-ab61-3050d0c519cc]
[0m13:39:53.916620 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=c041fa69-5b5b-4b7a-8b36-ff66c1864f0b]
[0m13:39:53.917622 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=c041fa69-5b5b-4b7a-8b36-ff66c1864f0b]
[0m13:39:54.008587 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=c6761d0e-bce8-4781-9f69-06d5a7e059a3]
[0m13:39:54.009589 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=c6761d0e-bce8-4781-9f69-06d5a7e059a3]
[0m13:39:54.079056 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=4beffaef-980d-4572-b10e-363f0ef676c3]
[0m13:39:54.080053 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=4beffaef-980d-4572-b10e-363f0ef676c3]
[0m13:39:54.164650 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=GET_TABLES, getHandleIdentifier()=f3118919-c50d-4524-840d-0a6b2741bd7f]
[0m13:39:54.165650 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=GET_TABLES, getHandleIdentifier()=f3118919-c50d-4524-840d-0a6b2741bd7f]
[0m13:39:54.252568 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=ee0904a0-156a-4de8-a887-7b8c301779ba]
[0m13:39:54.252568 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=ee0904a0-156a-4de8-a887-7b8c301779ba]
[0m13:39:54.320661 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=bbba548f-64ea-4246-8723-0778f33522ed]
[0m13:39:54.320661 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=bbba548f-64ea-4246-8723-0778f33522ed]
[0m13:39:54.391981 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=503a9372-f899-42c4-8f4e-4a9925509549]
[0m13:39:54.392984 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=503a9372-f899-42c4-8f4e-4a9925509549]
[0m13:39:54.460980 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=eead42d2-b0fa-463a-9f3a-146ed42d2126]
[0m13:39:54.461982 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=eead42d2-b0fa-463a-9f3a-146ed42d2126]
[0m13:39:54.541560 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=cbb0529d-ae47-47e4-aae6-b7e25477cafe]
[0m13:39:54.541560 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=cbb0529d-ae47-47e4-aae6-b7e25477cafe]
[0m13:39:54.621263 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=5c905c73-e016-498a-aa9f-ba70e7edb232]
[0m13:39:54.621263 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=5c905c73-e016-498a-aa9f-ba70e7edb232]
[0m13:39:54.698158 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=6bd6237d-988c-4108-8424-2939a4b1bcd9]
[0m13:39:54.699164 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=6bd6237d-988c-4108-8424-2939a4b1bcd9]
[0m13:39:54.778309 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=c55856df-c390-48b8-b47c-53fdc1dc4ced]
[0m13:39:54.778309 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=c55856df-c390-48b8-b47c-53fdc1dc4ced]
[0m13:39:54.871963 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=5ebf4c82-7274-4aa9-9650-c6109d7795ee]
[0m13:39:54.872963 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=5ebf4c82-7274-4aa9-9650-c6109d7795ee]
[0m13:39:54.950519 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a608c9e2-3287-43a6-8579-0ef87972b595]
[0m13:39:54.951515 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a608c9e2-3287-43a6-8579-0ef87972b595]
[0m13:39:55.027975 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=7af4a67b-eecb-4cd1-9725-b465e9c444f5]
[0m13:39:55.027975 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=7af4a67b-eecb-4cd1-9725-b465e9c444f5]
[0m13:39:55.105449 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=055551b2-a46f-46cb-a207-f0e6a0ea5313]
[0m13:39:55.106450 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=055551b2-a46f-46cb-a207-f0e6a0ea5313]
[0m13:39:55.183947 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=85c560e4-4446-43f4-95ec-59f9365a447e]
[0m13:39:55.184871 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=85c560e4-4446-43f4-95ec-59f9365a447e]
[0m13:39:55.262575 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=e6d91596-1217-4d1b-be2a-fafd016eaea4]
[0m13:39:55.263573 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=e6d91596-1217-4d1b-be2a-fafd016eaea4]
[0m13:39:55.332812 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=1c2d9de1-c5e9-4d54-a56f-9fc8b12432cf]
[0m13:39:55.332812 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=1c2d9de1-c5e9-4d54-a56f-9fc8b12432cf]
[0m13:39:55.419759 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=9877ed45-2d6c-4e6a-bbd3-016dbca7b745]
[0m13:39:55.420760 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=9877ed45-2d6c-4e6a-bbd3-016dbca7b745]
[0m13:39:55.492204 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=3c536380-5e8c-49c0-91e0-c5b92dcec4fd]
[0m13:39:55.493204 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=3c536380-5e8c-49c0-91e0-c5b92dcec4fd]
[0m13:39:55.576620 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=f53c7eb4-cda2-4f54-8851-b8f9d775a52a]
[0m13:39:55.577621 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=f53c7eb4-cda2-4f54-8851-b8f9d775a52a]
[0m13:39:55.655457 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=b3647ab7-fbe4-450f-ba40-eb2ae6f48f00]
[0m13:39:55.656457 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=b3647ab7-fbe4-450f-ba40-eb2ae6f48f00]
[0m13:39:55.765438 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=603ebddf-aea3-475b-bf5f-208b3d9c86c4]
[0m13:39:55.766439 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=603ebddf-aea3-475b-bf5f-208b3d9c86c4]
[0m13:39:55.832077 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=dbedef47-b13c-40f8-b171-abdce0e77af7]
[0m13:39:55.833080 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=dbedef47-b13c-40f8-b171-abdce0e77af7]
[0m13:39:55.907568 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=e6fd046e-5860-4508-b593-0f004ec3e6d8]
[0m13:39:55.908571 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=e6fd046e-5860-4508-b593-0f004ec3e6d8]
[0m13:39:55.982277 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=46dbf964-a973-41dd-b51b-e0de635b8310]
[0m13:39:55.982277 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=46dbf964-a973-41dd-b51b-e0de635b8310]
[0m13:39:56.059989 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=b2d4d6f2-ad76-42ae-830a-2ac4fb34a708]
[0m13:39:56.060990 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=b2d4d6f2-ad76-42ae-830a-2ac4fb34a708]
[0m13:39:56.138747 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=d36851fb-2dc0-46b8-a63b-605dbf004e3a]
[0m13:39:56.139746 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=d36851fb-2dc0-46b8-a63b-605dbf004e3a]
[0m13:39:56.209224 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=477b8389-9384-4c1f-99d8-b8560f798521]
[0m13:39:56.209224 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=477b8389-9384-4c1f-99d8-b8560f798521]
[0m13:39:56.278276 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=e804cea7-4e0b-4d89-9c10-94fc36b3ccfe]
[0m13:39:56.279276 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=e804cea7-4e0b-4d89-9c10-94fc36b3ccfe]
[0m13:39:56.355752 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=cefa79a2-ad12-4d57-9780-ac26d5166de5]
[0m13:39:56.356754 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=cefa79a2-ad12-4d57-9780-ac26d5166de5]
[0m13:39:56.433905 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=e6a4e278-b1fb-45bd-9e59-48db783401b1]
[0m13:39:56.434905 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=e6a4e278-b1fb-45bd-9e59-48db783401b1]
[0m13:39:56.541385 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=50b403b1-f9a8-4c3f-b156-b5ad0447716f]
[0m13:39:56.541385 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=50b403b1-f9a8-4c3f-b156-b5ad0447716f]
[0m13:39:56.614408 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=d25d97a0-38db-4dcc-b3aa-a602dab44cc2]
[0m13:39:56.615410 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=d25d97a0-38db-4dcc-b3aa-a602dab44cc2]
[0m13:39:56.684757 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=21437cdc-ef03-4330-bc8c-45b96df2c9dd]
[0m13:39:56.684757 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=21437cdc-ef03-4330-bc8c-45b96df2c9dd]
[0m13:39:56.768056 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=511edf06-996e-4003-8437-948f233318b0]
[0m13:39:56.769061 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=511edf06-996e-4003-8437-948f233318b0]
[0m13:39:56.839187 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=b9580127-4c44-4608-9c3d-2e00f1accf95]
[0m13:39:56.839187 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=b9580127-4c44-4608-9c3d-2e00f1accf95]
[0m13:39:56.911922 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=4dfc7ca1-8926-4c53-8d46-4bd98353a0f4]
[0m13:39:56.911922 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=4dfc7ca1-8926-4c53-8d46-4bd98353a0f4]
[0m13:39:56.994477 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=9276ccda-c6e3-4445-92b6-f3061ace18d0]
[0m13:39:56.995461 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=9276ccda-c6e3-4445-92b6-f3061ace18d0]
[0m13:39:57.056820 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=d44974fc-0f19-4dec-a74f-fe78a5b1e7c2]
[0m13:39:57.057821 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=d44974fc-0f19-4dec-a74f-fe78a5b1e7c2]
[0m13:39:57.130002 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=0c6a26b6-849c-49f6-9323-bbad22fc2434]
[0m13:39:57.130002 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=0c6a26b6-849c-49f6-9323-bbad22fc2434]
[0m13:39:57.194984 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=3d88fee4-3c4b-45cd-a1e0-477127d736c6]
[0m13:39:57.195989 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=3d88fee4-3c4b-45cd-a1e0-477127d736c6]
[0m13:39:57.267072 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=c39d6f69-9700-48e5-bac8-34ee011b880d]
[0m13:39:57.268075 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=c39d6f69-9700-48e5-bac8-34ee011b880d]
[0m13:39:57.346342 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=ac5627ea-7c54-4962-845b-7cd7cc2916b8]
[0m13:39:57.347343 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=ac5627ea-7c54-4962-845b-7cd7cc2916b8]
[0m13:39:57.414413 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=ee3e030a-fa93-40b6-af7f-6010db7a8b9f]
[0m13:39:57.415412 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=ee3e030a-fa93-40b6-af7f-6010db7a8b9f]
[0m13:39:57.484313 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=b263e006-6299-4782-b919-9fd344fade16]
[0m13:39:57.485316 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=b263e006-6299-4782-b919-9fd344fade16]
[0m13:39:57.551309 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=f902d9de-b12b-4fb1-8e8a-721bb5661799]
[0m13:39:57.551309 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=f902d9de-b12b-4fb1-8e8a-721bb5661799]
[0m13:39:57.630069 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=ede2ab79-6734-428a-a586-06533d01b081]
[0m13:39:57.630069 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=ede2ab79-6734-428a-a586-06533d01b081]
[0m13:39:57.707366 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=9d5b42bc-0b07-4f3a-a611-96bdd8064161]
[0m13:39:57.708369 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=9d5b42bc-0b07-4f3a-a611-96bdd8064161]
[0m13:39:57.779973 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=5b23b49f-5486-4305-8034-807fe0a706dc]
[0m13:39:57.779973 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=5b23b49f-5486-4305-8034-807fe0a706dc]
[0m13:39:57.862560 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=51ae0f0d-b260-4842-a5a9-4a192bebc3a3]
[0m13:39:57.863560 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=51ae0f0d-b260-4842-a5a9-4a192bebc3a3]
[0m13:39:57.934531 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=1703278a-7050-4ece-a5df-a9450c7c87be]
[0m13:39:57.934531 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=1703278a-7050-4ece-a5df-a9450c7c87be]
[0m13:39:58.009095 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=69b70c65-148f-4035-b65c-e7af21aab4c7]
[0m13:39:58.009095 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=69b70c65-148f-4035-b65c-e7af21aab4c7]
[0m13:39:58.076944 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a3d50f04-d43a-4254-b261-163cce1d7e5e]
[0m13:39:58.076944 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a3d50f04-d43a-4254-b261-163cce1d7e5e]
[0m13:39:58.152501 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=75a4a451-ec5e-4fa5-a656-d2a584b9ab07]
[0m13:39:58.153503 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=75a4a451-ec5e-4fa5-a656-d2a584b9ab07]
[0m13:39:58.221760 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a7674c1f-b83a-4e98-9046-c16ce13f0eee]
[0m13:39:58.221760 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a7674c1f-b83a-4e98-9046-c16ce13f0eee]
[0m13:39:58.306868 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a6392b38-dca7-46a3-a417-6488976f3267]
[0m13:39:58.307873 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a6392b38-dca7-46a3-a417-6488976f3267]
[0m13:39:58.384471 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=e7fc7241-d670-4ccc-ad32-18a30120e477]
[0m13:39:58.385471 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=e7fc7241-d670-4ccc-ad32-18a30120e477]
[0m13:39:58.462774 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=fd81e517-8721-4e7a-aa7b-3cc43beaf0de]
[0m13:39:58.463773 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=fd81e517-8721-4e7a-aa7b-3cc43beaf0de]
[0m13:39:58.532490 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=3be0b405-b015-47dc-b538-cf6f9e9317a9]
[0m13:39:58.532490 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=3be0b405-b015-47dc-b538-cf6f9e9317a9]
[0m13:39:58.612107 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=c0de5839-9b88-4116-ab1d-8d992b37522b]
[0m13:39:58.613126 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=c0de5839-9b88-4116-ab1d-8d992b37522b]
[0m13:39:58.722631 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=5c69b326-9a27-4683-affc-7e2312e1cfc7]
[0m13:39:58.723634 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=5c69b326-9a27-4683-affc-7e2312e1cfc7]
[0m13:39:58.824191 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a194f407-9c32-463f-b39c-45e7ac8c3416]
[0m13:39:58.825198 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a194f407-9c32-463f-b39c-45e7ac8c3416]
[0m13:39:58.899270 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a69d339c-7709-452c-8922-ec60c6bc497e]
[0m13:39:58.900275 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a69d339c-7709-452c-8922-ec60c6bc497e]
[0m13:39:58.985998 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=b8e65eac-23ad-4dba-8276-a01722cde83a]
[0m13:39:58.987002 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=b8e65eac-23ad-4dba-8276-a01722cde83a]
[0m13:39:59.052838 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=5edc48b6-7603-444d-875a-021f31b4106f]
[0m13:39:59.052838 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=5edc48b6-7603-444d-875a-021f31b4106f]
[0m13:39:59.128672 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=ca03ffb7-473b-4b38-98f2-75620e97d135]
[0m13:39:59.129672 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=ca03ffb7-473b-4b38-98f2-75620e97d135]
[0m13:39:59.200286 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=7221dae2-76c2-4c43-890c-ac639a59f9da]
[0m13:39:59.200286 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=7221dae2-76c2-4c43-890c-ac639a59f9da]
[0m13:39:59.281235 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a5c0cba9-32bb-41ab-82b1-ca9007001d62]
[0m13:39:59.281235 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a5c0cba9-32bb-41ab-82b1-ca9007001d62]
[0m13:39:59.346588 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=b67df735-d486-4ad4-a7a3-07b0b2b20ee2]
[0m13:39:59.347591 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=b67df735-d486-4ad4-a7a3-07b0b2b20ee2]
[0m13:39:59.417427 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=60fe6e97-c092-419a-8772-e483b6acb286]
[0m13:39:59.417427 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=60fe6e97-c092-419a-8772-e483b6acb286]
[0m13:39:59.499056 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=5265a323-4957-43f9-b03d-36ebf1d6fb8c]
[0m13:39:59.500060 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=5265a323-4957-43f9-b03d-36ebf1d6fb8c]
[0m13:39:59.569770 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=bc3f0379-4006-4393-8ae5-93aaccda2649]
[0m13:39:59.569770 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=bc3f0379-4006-4393-8ae5-93aaccda2649]
[0m13:39:59.643143 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a2a4c9bb-224d-4f0a-aa9c-42ae4869949e]
[0m13:39:59.643143 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a2a4c9bb-224d-4f0a-aa9c-42ae4869949e]
[0m13:39:59.718880 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=84c7f3c8-57a3-49f2-89c8-68a93eec164e]
[0m13:39:59.718880 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=84c7f3c8-57a3-49f2-89c8-68a93eec164e]
[0m13:39:59.793292 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=e437bf29-d985-445f-8be4-86704ef7ab5e]
[0m13:39:59.793292 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=e437bf29-d985-445f-8be4-86704ef7ab5e]
[0m13:39:59.877473 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=356e28c9-4f87-492b-a1d5-3d298ef58884]
[0m13:39:59.878472 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=356e28c9-4f87-492b-a1d5-3d298ef58884]
[0m13:39:59.959239 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=620b7f1b-c73b-4d67-9a37-ce43e9a0f709]
[0m13:39:59.960241 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=620b7f1b-c73b-4d67-9a37-ce43e9a0f709]
[0m13:40:00.033432 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=55b953e2-eb39-4173-9e62-acb45f1e02e1]
[0m13:40:00.034433 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=55b953e2-eb39-4173-9e62-acb45f1e02e1]
[0m13:40:00.105127 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=e53d14e6-9d0a-42e4-96a7-d8c6965a4527]
[0m13:40:00.105127 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=e53d14e6-9d0a-42e4-96a7-d8c6965a4527]
[0m13:40:00.188834 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=b947aeac-9966-40d2-a740-0ab6d1071c15]
[0m13:40:00.188834 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=b947aeac-9966-40d2-a740-0ab6d1071c15]
[0m13:40:00.259619 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=58e64f0a-427f-419c-9488-b68468cad088]
[0m13:40:00.259619 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=58e64f0a-427f-419c-9488-b68468cad088]
[0m13:40:00.334929 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=dcad66d1-e581-46cb-8b3d-febf63f6b7ae]
[0m13:40:00.334929 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=dcad66d1-e581-46cb-8b3d-febf63f6b7ae]
[0m13:40:00.407458 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=0d041ffb-dc19-4034-b32c-23a66a10b8b2]
[0m13:40:00.408456 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=0d041ffb-dc19-4034-b32c-23a66a10b8b2]
[0m13:40:00.487041 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=fff49e9e-3390-4e11-b49f-97c129608973]
[0m13:40:00.488045 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=fff49e9e-3390-4e11-b49f-97c129608973]
[0m13:40:00.559685 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=1e38e112-91d5-48b0-91a0-f29a2367dc31]
[0m13:40:00.559685 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=1e38e112-91d5-48b0-91a0-f29a2367dc31]
[0m13:40:00.638079 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=aaee72bc-9ac3-433d-bc88-17ac9190a713]
[0m13:40:00.638079 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=aaee72bc-9ac3-433d-bc88-17ac9190a713]
[0m13:40:00.709940 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=12046a12-c3f3-42dd-9e3c-75dffba2d5d9]
[0m13:40:00.710943 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=12046a12-c3f3-42dd-9e3c-75dffba2d5d9]
[0m13:40:00.781118 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=c315093c-855c-4107-bc43-35c948a8d27c]
[0m13:40:00.781118 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=c315093c-855c-4107-bc43-35c948a8d27c]
[0m13:40:00.852466 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=dec58c3d-7644-4665-879a-57d312e409fa]
[0m13:40:00.853469 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=dec58c3d-7644-4665-879a-57d312e409fa]
[0m13:40:00.935960 [debug] [MainThread]: Databricks adapter: Exception while cancelling query: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=1d38ba6f-d368-4a08-886a-403dfeb513c3]
[0m13:40:00.936959 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.DatabaseError'>: Invalid OperationHandle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=1d38ba6f-d368-4a08-886a-403dfeb513c3]
[0m13:40:00.937961 [error] [MainThread]: CANCEL query list_hive_metastore ............................................... [[31mCANCEL[0m]
[0m13:40:00.937961 [error] [MainThread]: CANCEL query list_hive_metastore_snapshots ..................................... [[31mCANCEL[0m]
[0m13:40:00.938966 [error] [MainThread]: CANCEL query snapshot.dbt_spark_modeling.salesorderheader_snapshot ............. [[31mCANCEL[0m]
[0m13:40:00.939962 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (15524, 24048), compute: ``, acquire_release_count: 1, idle time: 85.37501692771912s
[0m13:40:00.940967 [info ] [MainThread]: 
[0m13:40:00.941959 [info ] [MainThread]: [33mExited because of keyboard interrupt[0m
[0m13:40:00.942961 [info ] [MainThread]: 
[0m13:40:00.943962 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=0 SKIP=0 TOTAL=8
[0m13:40:00.943962 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:40:00.944960 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m13:40:00.945965 [debug] [MainThread]: On list_hive_metastore: Close
[0m13:40:01.038049 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m13:40:01.039051 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m13:40:01.040050 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:40:01.040050 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m13:40:01.136053 [debug] [MainThread]: Connection 'snapshot.dbt_spark_modeling.salesorderheader_snapshot' was properly closed.
[0m13:40:01.137058 [debug] [MainThread]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: ROLLBACK
[0m13:40:01.137058 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:40:01.138057 [debug] [MainThread]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: Close
[0m13:40:01.201349 [info ] [MainThread]: 
[0m13:40:01.202349 [info ] [MainThread]: Finished running 8 snapshots in 0 hours 1 minutes and 29.14 seconds (89.14s).
[0m13:40:01.203350 [error] [MainThread]: Encountered an error:

[0m13:40:01.248621 [error] [MainThread]: Traceback (most recent call last):
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\cli\requires.py", line 90, in wrapper
    result, success = func(*args, **kwargs)
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\cli\requires.py", line 75, in wrapper
    return func(*args, **kwargs)
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\cli\requires.py", line 168, in wrapper
    return func(*args, **kwargs)
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\cli\requires.py", line 197, in wrapper
    return func(*args, **kwargs)
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\cli\requires.py", line 244, in wrapper
    return func(*args, **kwargs)
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\cli\requires.py", line 284, in wrapper
    return func(*args, **kwargs)
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\cli\main.py", line 801, in snapshot
    results = task.run()
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\task\runnable.py", line 474, in run
    result = self.execute_with_hooks(selected_uids)
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\task\runnable.py", line 435, in execute_with_hooks
    res = self.execute_nodes()
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\task\runnable.py", line 359, in execute_nodes
    self.run_queue(pool)
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\task\runnable.py", line 291, in run_queue
    self.job_queue.join()
  File "D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\lib\site-packages\dbt\graph\queue.py", line 198, in join
    self.inner.join()
  File "C:\Program Files\Python39\lib\queue.py", line 90, in join
    self.all_tasks_done.wait()
  File "C:\Program Files\Python39\lib\threading.py", line 312, in wait
    waiter.acquire()
KeyboardInterrupt

[0m13:40:01.252621 [debug] [MainThread]: Command `dbt snapshot` failed at 13:40:01.252621 after 91.45 seconds
[0m13:40:01.252621 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DFF3E7130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DA363F310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025DA39734C0>]}
[0m13:40:01.253621 [debug] [MainThread]: Flushing usage events
[0m13:40:19.034275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A67415520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A6A3F25E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A6A3F2F10>]}


============================== 13:40:19.037275 | adfb752b-fe4c-46ff-bd12-66b6178cee92 ==============================
[0m13:40:19.037275 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:40:19.037275 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt snapshot --select productcategory', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m13:40:20.030264 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'adfb752b-fe4c-46ff-bd12-66b6178cee92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A662D9970>]}
[0m13:40:20.105265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'adfb752b-fe4c-46ff-bd12-66b6178cee92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A0B53C580>]}
[0m13:40:20.105265 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m13:40:20.115264 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:40:20.200263 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:40:20.201263 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:40:20.206263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'adfb752b-fe4c-46ff-bd12-66b6178cee92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A0B980F40>]}
[0m13:40:20.218264 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'adfb752b-fe4c-46ff-bd12-66b6178cee92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A0B843D60>]}
[0m13:40:20.218264 [info ] [MainThread]: Found 8 snapshots, 3 models, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m13:40:20.219265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'adfb752b-fe4c-46ff-bd12-66b6178cee92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A0B4D95E0>]}
[0m13:40:20.220267 [info ] [MainThread]: 
[0m13:40:20.222266 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (9360, 15020), compute: ``
[0m13:40:20.222266 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:40:20.223266 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (9360, 15020), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:40:20.223266 [debug] [MainThread]: Databricks adapter: Thread (9360, 15020) using default compute resource.
[0m13:40:20.225265 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore, thread: (9360, 1768), compute: ``
[0m13:40:20.225265 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m13:40:20.226267 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore, thread: (9360, 1768), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:40:20.226267 [debug] [ThreadPool]: Databricks adapter: Thread (9360, 1768) using default compute resource.
[0m13:40:20.227264 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m13:40:20.227264 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m13:40:20.228266 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:40:20.568941 [debug] [ThreadPool]: SQL status: OK in 0.3400000035762787 seconds
[0m13:40:20.572940 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore, thread: (9360, 1768), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:40:20.575941 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (9360, 22388), compute: ``
[0m13:40:20.576944 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m13:40:20.576944 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (9360, 22388), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:40:20.577944 [debug] [ThreadPool]: Databricks adapter: Thread (9360, 22388) using default compute resource.
[0m13:40:20.581940 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:40:20.581940 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m13:40:20.582940 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:40:20.959071 [debug] [ThreadPool]: SQL status: OK in 0.3799999952316284 seconds
[0m13:40:20.973065 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:40:20.973065 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:40:20.974066 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m13:40:21.143637 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m13:40:21.152633 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:40:21.153633 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m13:40:21.319130 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m13:40:21.330137 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:40:21.330137 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show table extended in `hive_metastore`.`saleslt` like '*'
  
[0m13:40:21.860332 [debug] [ThreadPool]: SQL status: OK in 0.5299999713897705 seconds
[0m13:40:21.864332 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (9360, 22388), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:40:21.866334 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m13:40:21.870334 [debug] [ThreadPool]: Databricks adapter: Reusing DatabricksDBTConnection. name: list_hive_metastore_snapshots, thread: (9360, 22388), compute: ``, acquire_release_count: 0, idle time: 0.0050013065338134766s
[0m13:40:21.871332 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_snapshots, thread: (9360, 22388), compute: ``, acquire_release_count: 0, idle time: 0.0050013065338134766s
[0m13:40:21.871332 [debug] [ThreadPool]: Databricks adapter: Thread (9360, 22388) using default compute resource.
[0m13:40:21.873331 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:40:21.874330 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m13:40:21.992732 [debug] [ThreadPool]: SQL status: OK in 0.11999999731779099 seconds
[0m13:40:22.000737 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:40:22.000737 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m13:40:22.266113 [debug] [ThreadPool]: SQL status: OK in 0.27000001072883606 seconds
[0m13:40:22.274114 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:40:22.274114 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m13:40:22.459396 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m13:40:22.467395 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:40:22.468395 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show table extended in `hive_metastore`.`snapshots` like '*'
  
[0m13:40:23.054025 [debug] [ThreadPool]: SQL status: OK in 0.5899999737739563 seconds
[0m13:40:23.058025 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_snapshots, thread: (9360, 22388), compute: ``, acquire_release_count: 1, idle time: 1.192692518234253s
[0m13:40:23.061026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'adfb752b-fe4c-46ff-bd12-66b6178cee92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A0B6C7AC0>]}
[0m13:40:23.062025 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:40:23.063027 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:40:23.063027 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (9360, 15020), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:40:23.064024 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:40:23.064024 [info ] [MainThread]: 
[0m13:40:23.067024 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.productcategory_snapshot
[0m13:40:23.068025 [info ] [Thread-1  ]: 1 of 1 START snapshot snapshots.productcategory_snapshot ....................... [RUN]
[0m13:40:23.069026 [debug] [Thread-1  ]: Databricks adapter: Creating DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.productcategory_snapshot, thread: (9360, 12412), compute: ``
[0m13:40:23.070026 [debug] [Thread-1  ]: Acquiring new databricks connection 'snapshot.dbt_spark_modeling.productcategory_snapshot'
[0m13:40:23.070026 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.productcategory_snapshot, thread: (9360, 12412), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:40:23.071026 [debug] [Thread-1  ]: Databricks adapter: On thread (9360, 12412): `hive_metastore`.`snapshots`.`productcategory_snapshot` using default compute resource.
[0m13:40:23.071026 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.productcategory_snapshot
[0m13:40:23.079024 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.productcategory_snapshot (compile): 13:40:23.072027 => 13:40:23.079024
[0m13:40:23.080028 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.productcategory_snapshot
[0m13:40:23.105026 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:40:23.106026 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productcategory_snapshot"
[0m13:40:23.107026 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productcategory_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productcategory_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productcategory_snapshot`
  
[0m13:40:23.107026 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m13:40:24.167137 [debug] [Thread-1  ]: SQL status: OK in 1.059999942779541 seconds
[0m13:40:24.199136 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productcategory_snapshot"
[0m13:40:24.199136 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productcategory_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productcategory_snapshot"} */
select * from (
        



with product_snapshot as (
    SELECT
        ProductCategoryID,
        Name
    FROM `hive_metastore`.`saleslt`.`productcategory`
    WHERE ParentProductCategoryID is not null
)

select * from product_snapshot

    ) as __dbt_sbq
    where false
    limit 0

    
[0m13:40:24.382894 [debug] [Thread-1  ]: SQL status: OK in 0.18000000715255737 seconds
[0m13:40:24.462896 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productcategory_snapshot"
[0m13:40:24.462896 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productcategory_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productcategory_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productcategory_snapshot`
  
[0m13:40:24.926904 [debug] [Thread-1  ]: SQL status: OK in 0.46000000834465027 seconds
[0m13:40:24.936903 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productcategory_snapshot"
[0m13:40:24.937907 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productcategory_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productcategory_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productcategory_snapshot`
  
[0m13:40:35.663818 [debug] [Thread-1  ]: SQL status: OK in 10.729999542236328 seconds
[0m13:40:35.763100 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productcategory_snapshot"
[0m13:40:35.764096 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productcategory_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productcategory_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`productcategory_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with product_snapshot as (
    SELECT
        ProductCategoryID,
        Name
    FROM `hive_metastore`.`saleslt`.`productcategory`
    WHERE ParentProductCategoryID is not null
)

select * from product_snapshot


    ),

    snapshotted_data as (

        select *,
            ProductCategoryID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`productcategory_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            ProductCategoryID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(ProductCategoryID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            ProductCategoryID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            ProductCategoryID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`ProductCategoryID` != source_data.`ProductCategoryID`
        or
        (
            ((snapshotted_data.`ProductCategoryID` is null) and not (source_data.`ProductCategoryID` is null))
            or
            ((not snapshotted_data.`ProductCategoryID` is null) and (source_data.`ProductCategoryID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`ProductCategoryID` != source_data.`ProductCategoryID`
        or
        (
            ((snapshotted_data.`ProductCategoryID` is null) and not (source_data.`ProductCategoryID` is null))
            or
            ((not snapshotted_data.`ProductCategoryID` is null) and (source_data.`ProductCategoryID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m13:40:36.187144 [debug] [Thread-1  ]: SQL status: OK in 0.41999998688697815 seconds
[0m13:40:36.190148 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productcategory_snapshot"
[0m13:40:36.191149 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productcategory_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productcategory_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productcategory_snapshot__dbt_tmp`
  
[0m13:40:36.342857 [debug] [Thread-1  ]: SQL status: OK in 0.15000000596046448 seconds
[0m13:40:36.348857 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productcategory_snapshot"
[0m13:40:36.349858 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productcategory_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productcategory_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productcategory_snapshot`
  
[0m13:40:36.737017 [debug] [Thread-1  ]: SQL status: OK in 0.38999998569488525 seconds
[0m13:40:36.743017 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productcategory_snapshot"
[0m13:40:36.744018 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productcategory_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productcategory_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productcategory_snapshot__dbt_tmp`
  
[0m13:40:36.933545 [debug] [Thread-1  ]: SQL status: OK in 0.1899999976158142 seconds
[0m13:40:36.939543 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productcategory_snapshot"
[0m13:40:36.940543 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productcategory_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productcategory_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productcategory_snapshot`
  
[0m13:40:37.337658 [debug] [Thread-1  ]: SQL status: OK in 0.4000000059604645 seconds
[0m13:40:37.349657 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productcategory_snapshot"
[0m13:40:37.349657 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productcategory_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productcategory_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productcategory_snapshot__dbt_tmp`
  
[0m13:40:37.517140 [debug] [Thread-1  ]: SQL status: OK in 0.17000000178813934 seconds
[0m13:40:37.527137 [debug] [Thread-1  ]: Writing runtime SQL for node "snapshot.dbt_spark_modeling.productcategory_snapshot"
[0m13:40:37.528138 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productcategory_snapshot"
[0m13:40:37.529138 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productcategory_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productcategory_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`productcategory_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`productcategory_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m13:40:42.609609 [debug] [Thread-1  ]: SQL status: OK in 5.079999923706055 seconds
[0m13:40:42.617590 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productcategory_snapshot"
[0m13:40:42.618589 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productcategory_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productcategory_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`productcategory_snapshot__dbt_tmp`
[0m13:40:43.151904 [debug] [Thread-1  ]: SQL status: OK in 0.5299999713897705 seconds
[0m13:40:43.171902 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m13:40:43.172902 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.productcategory_snapshot (execute): 13:40:23.080028 => 13:40:43.172902
[0m13:40:43.173902 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.productcategory_snapshot, thread: (9360, 12412), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:40:43.173902 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.productcategory_snapshot, thread: (9360, 12412), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m13:40:43.174902 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'adfb752b-fe4c-46ff-bd12-66b6178cee92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A0B9E2940>]}
[0m13:40:43.174902 [info ] [Thread-1  ]: 1 of 1 OK snapshotted snapshots.productcategory_snapshot ....................... [[32mOK[0m in 20.11s]
[0m13:40:43.175903 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.productcategory_snapshot
[0m13:40:43.176902 [debug] [MainThread]: Databricks adapter: Reusing DatabricksDBTConnection. name: master, thread: (9360, 15020), compute: ``, acquire_release_count: 0, idle time: 20.11387586593628s
[0m13:40:43.176902 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (9360, 15020), compute: ``, acquire_release_count: 0, idle time: 20.11387586593628s
[0m13:40:43.177902 [debug] [MainThread]: Databricks adapter: Thread (9360, 15020) using default compute resource.
[0m13:40:43.177902 [debug] [MainThread]: On master: ROLLBACK
[0m13:40:43.177902 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:40:43.404692 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:40:43.404692 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:40:43.405691 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:40:43.405691 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (9360, 15020), compute: ``, acquire_release_count: 1, idle time: 20.34266448020935s
[0m13:40:43.406693 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:40:43.406693 [debug] [MainThread]: On master: ROLLBACK
[0m13:40:43.407692 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:40:43.407692 [debug] [MainThread]: On master: Close
[0m13:40:43.475838 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m13:40:43.476835 [debug] [MainThread]: On list_hive_metastore: Close
[0m13:40:43.576574 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m13:40:43.577574 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m13:40:43.577574 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:40:43.577574 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m13:40:43.655122 [debug] [MainThread]: Connection 'snapshot.dbt_spark_modeling.productcategory_snapshot' was properly closed.
[0m13:40:43.655122 [debug] [MainThread]: On snapshot.dbt_spark_modeling.productcategory_snapshot: ROLLBACK
[0m13:40:43.655122 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:40:43.656121 [debug] [MainThread]: On snapshot.dbt_spark_modeling.productcategory_snapshot: Close
[0m13:40:43.736863 [info ] [MainThread]: 
[0m13:40:43.736863 [info ] [MainThread]: Finished running 1 snapshot in 0 hours 0 minutes and 23.51 seconds (23.51s).
[0m13:40:43.737864 [debug] [MainThread]: Command end result
[0m13:40:43.746865 [info ] [MainThread]: 
[0m13:40:43.747863 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:40:43.747863 [info ] [MainThread]: 
[0m13:40:43.748864 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m13:40:43.749862 [debug] [MainThread]: Command `dbt snapshot` succeeded at 13:40:43.748864 after 24.75 seconds
[0m13:40:43.749862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A67415520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A0B61CC70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A0B955E80>]}
[0m13:40:43.749862 [debug] [MainThread]: Flushing usage events
[0m13:44:35.500650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001616B8A6A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001616E89F4F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001616E89FD00>]}


============================== 13:44:35.503650 | 352a41b7-9949-46d8-82eb-0ad095b5428a ==============================
[0m13:44:35.503650 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:44:35.504648 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select dim_product', 'send_anonymous_usage_stats': 'True'}
[0m13:44:36.504908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '352a41b7-9949-46d8-82eb-0ad095b5428a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001616A7898E0>]}
[0m13:44:36.578903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '352a41b7-9949-46d8-82eb-0ad095b5428a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001610F950B50>]}
[0m13:44:36.579904 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m13:44:36.588903 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:44:36.674903 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:44:36.674903 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_modeling://models\marts\product\dim_product.sql
[0m13:44:36.792903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '352a41b7-9949-46d8-82eb-0ad095b5428a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001610FF6D0D0>]}
[0m13:44:36.804903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '352a41b7-9949-46d8-82eb-0ad095b5428a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001610FCC8C70>]}
[0m13:44:36.805903 [info ] [MainThread]: Found 8 snapshots, 3 models, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m13:44:36.805903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '352a41b7-9949-46d8-82eb-0ad095b5428a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001610FCC8AF0>]}
[0m13:44:36.806903 [info ] [MainThread]: 
[0m13:44:36.807903 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (6980, 23460), compute: ``
[0m13:44:36.807903 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:44:36.808903 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (6980, 23460), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:44:36.808903 [debug] [MainThread]: Databricks adapter: Thread (6980, 23460) using default compute resource.
[0m13:44:36.809903 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore, thread: (6980, 21828), compute: ``
[0m13:44:36.810903 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m13:44:36.810903 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore, thread: (6980, 21828), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:44:36.810903 [debug] [ThreadPool]: Databricks adapter: Thread (6980, 21828) using default compute resource.
[0m13:44:36.811903 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m13:44:36.811903 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m13:44:36.811903 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:44:37.551261 [debug] [ThreadPool]: SQL status: OK in 0.7400000095367432 seconds
[0m13:44:37.556257 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore, thread: (6980, 21828), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:44:37.558260 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_snapshots, thread: (6980, 1684), compute: ``
[0m13:44:37.559258 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m13:44:37.559258 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_snapshots, thread: (6980, 1684), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:44:37.560260 [debug] [ThreadPool]: Databricks adapter: Thread (6980, 1684) using default compute resource.
[0m13:44:37.564260 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:44:37.564260 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m13:44:37.565261 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:44:38.010739 [debug] [ThreadPool]: SQL status: OK in 0.44999998807907104 seconds
[0m13:44:38.023733 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:44:38.023733 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:44:38.023733 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m13:44:38.257115 [debug] [ThreadPool]: SQL status: OK in 0.23000000417232513 seconds
[0m13:44:38.269122 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:44:38.270130 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m13:44:38.561816 [debug] [ThreadPool]: SQL status: OK in 0.28999999165534973 seconds
[0m13:44:38.567817 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:44:38.568817 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show table extended in `hive_metastore`.`snapshots` like '*'
  
[0m13:44:39.238315 [debug] [ThreadPool]: SQL status: OK in 0.6700000166893005 seconds
[0m13:44:39.242313 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_snapshots, thread: (6980, 1684), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:44:39.244315 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m13:44:39.247314 [debug] [ThreadPool]: Databricks adapter: Reusing DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (6980, 1684), compute: ``, acquire_release_count: 0, idle time: 0.003004312515258789s
[0m13:44:39.247314 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (6980, 1684), compute: ``, acquire_release_count: 0, idle time: 0.004000186920166016s
[0m13:44:39.248314 [debug] [ThreadPool]: Databricks adapter: Thread (6980, 1684) using default compute resource.
[0m13:44:39.250311 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:44:39.251311 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m13:44:39.393319 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m13:44:39.399314 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:44:39.400315 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m13:44:39.518064 [debug] [ThreadPool]: SQL status: OK in 0.11999999731779099 seconds
[0m13:44:39.524062 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:44:39.524062 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m13:44:39.677851 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m13:44:39.683850 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:44:39.684851 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show table extended in `hive_metastore`.`saleslt` like '*'
  
[0m13:44:40.207666 [debug] [ThreadPool]: SQL status: OK in 0.5199999809265137 seconds
[0m13:44:40.211665 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (6980, 1684), compute: ``, acquire_release_count: 1, idle time: 0.9683511257171631s
[0m13:44:40.216664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '352a41b7-9949-46d8-82eb-0ad095b5428a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001610FA7A2E0>]}
[0m13:44:40.216664 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:44:40.217668 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:44:40.217668 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (6980, 23460), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:44:40.218667 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:44:40.218667 [info ] [MainThread]: 
[0m13:44:40.222663 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.dim_product
[0m13:44:40.222663 [info ] [Thread-1  ]: 1 of 1 START sql table model saleslt.dim_product ............................... [RUN]
[0m13:44:40.223663 [debug] [Thread-1  ]: Databricks adapter: Creating DatabricksDBTConnection. name: model.dbt_spark_modeling.dim_product, thread: (6980, 4740), compute: ``
[0m13:44:40.224663 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbt_spark_modeling.dim_product'
[0m13:44:40.224663 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.dim_product, thread: (6980, 4740), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:44:40.225663 [debug] [Thread-1  ]: Databricks adapter: On thread (6980, 4740): `hive_metastore`.`saleslt`.`dim_product` using default compute resource.
[0m13:44:40.225663 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.dim_product
[0m13:44:40.233665 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.dim_product"
[0m13:44:40.234664 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (compile): 13:44:40.226663 => 13:44:40.234664
[0m13:44:40.235664 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.dim_product
[0m13:44:40.247665 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:44:40.247665 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_product"
[0m13:44:40.248666 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_product: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
[0m13:44:40.248666 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m13:44:41.039802 [debug] [Thread-1  ]: SQL status: OK in 0.7900000214576721 seconds
[0m13:44:41.082795 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.dim_product"
[0m13:44:41.083794 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_product"
[0m13:44:41.084796 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_product: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by name) as category_id
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  
[0m13:44:41.219774 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by name) as category_id
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  
[0m13:44:41.220774 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by name) as category_id
------------------^^^
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  

[0m13:44:41.221774 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by name) as category_id
------------------^^^
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by name) as category_id
------------------^^^
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:259)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:111)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:541)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more

[0m13:44:41.222773 [debug] [Thread-1  ]: Databricks adapter: operation-id: 55d0b8c6-5e51-429e-b168-3b8b0f91c70d
[0m13:44:41.223776 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (execute): 13:44:40.235664 => 13:44:41.223776
[0m13:44:41.224775 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (6980, 4740), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:44:41.289772 [debug] [Thread-1  ]: Runtime Error in model dim_product (models\marts\product\dim_product.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_product`
        
        
      using delta
        
        
        
        
        
      location '/mnt/gold/products/dim_product'
        
        
        as
        
  
  with product_snapshot as (
      select
          productId,
          name,
          standardCost,
          listPrice,
          size,
          weight,
          productcategoryid,
          productmodelid,
          sellstartdate,
          sellenddate,
          discontinueddate
      from `hive_metastore`.`snapshots`.`product_snapshot`
      where dbt_valid_to is null
  ),
  
  product_model_snapshot as (
      select
          productmodelid,
          name,
          CatalogDescription,
          row_number() over (order by name) as model_id
      from `hive_metastore`.`snapshots`.`productmodel_snapshot`
      where dbt_valid_to is null
  ),
  
  productcategory_snapshot as (
      select
          ProductCategoryID,
          Name
          row_number() over (order by name) as category_id
  ------------------^^^
      from `hive_metastore`.`snapshots`.`productcategory_snapshot`
  ),
  
  transformed as (
      select
          row_number() over (order by p.productId) as product_sk,
          p.productId as productID,
          p.name as product_name,
          p.standardCost,
          p.listPrice,
          p.size,
          p.weight,
          pm.name as model,
          pm.CatalogDescription as description,
          p.sellstartdate,
          p.sellenddate,
          p.discontinueddate
      from product_snapshot p
      left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
      left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
  )
  
  select * from transformed
    
  
[0m13:44:41.290773 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (6980, 4740), compute: ``, acquire_release_count: 0, idle time: 0.06499671936035156s
[0m13:44:41.290773 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '352a41b7-9949-46d8-82eb-0ad095b5428a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001610FDF2850>]}
[0m13:44:41.290773 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model saleslt.dim_product ...................... [[31mERROR[0m in 1.07s]
[0m13:44:41.291772 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.dim_product
[0m13:44:41.292771 [debug] [MainThread]: Databricks adapter: Reusing DatabricksDBTConnection. name: master, thread: (6980, 23460), compute: ``, acquire_release_count: 0, idle time: 1.0751028060913086s
[0m13:44:41.293775 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (6980, 23460), compute: ``, acquire_release_count: 0, idle time: 1.0761072635650635s
[0m13:44:41.293775 [debug] [MainThread]: Databricks adapter: Thread (6980, 23460) using default compute resource.
[0m13:44:41.293775 [debug] [MainThread]: On master: ROLLBACK
[0m13:44:41.294775 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:44:41.524942 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:44:41.525944 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:44:41.525944 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:44:41.526945 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (6980, 23460), compute: ``, acquire_release_count: 1, idle time: 1.309277057647705s
[0m13:44:41.527944 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:44:41.527944 [debug] [MainThread]: On master: ROLLBACK
[0m13:44:41.528945 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:44:41.528945 [debug] [MainThread]: On master: Close
[0m13:44:41.608193 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m13:44:41.608193 [debug] [MainThread]: On list_hive_metastore: Close
[0m13:44:41.767780 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m13:44:41.767780 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m13:44:41.768777 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:44:41.768777 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m13:44:41.859494 [debug] [MainThread]: Connection 'model.dbt_spark_modeling.dim_product' was properly closed.
[0m13:44:41.859494 [debug] [MainThread]: On model.dbt_spark_modeling.dim_product: ROLLBACK
[0m13:44:41.860493 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:44:41.860493 [debug] [MainThread]: On model.dbt_spark_modeling.dim_product: Close
[0m13:44:41.937440 [info ] [MainThread]: 
[0m13:44:41.941441 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 5.13 seconds (5.13s).
[0m13:44:41.942441 [debug] [MainThread]: Command end result
[0m13:44:41.959444 [info ] [MainThread]: 
[0m13:44:41.962440 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:44:41.963442 [info ] [MainThread]: 
[0m13:44:41.963442 [error] [MainThread]:   Runtime Error in model dim_product (models\marts\product\dim_product.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_product`
        
        
      using delta
        
        
        
        
        
      location '/mnt/gold/products/dim_product'
        
        
        as
        
  
  with product_snapshot as (
      select
          productId,
          name,
          standardCost,
          listPrice,
          size,
          weight,
          productcategoryid,
          productmodelid,
          sellstartdate,
          sellenddate,
          discontinueddate
      from `hive_metastore`.`snapshots`.`product_snapshot`
      where dbt_valid_to is null
  ),
  
  product_model_snapshot as (
      select
          productmodelid,
          name,
          CatalogDescription,
          row_number() over (order by name) as model_id
      from `hive_metastore`.`snapshots`.`productmodel_snapshot`
      where dbt_valid_to is null
  ),
  
  productcategory_snapshot as (
      select
          ProductCategoryID,
          Name
          row_number() over (order by name) as category_id
  ------------------^^^
      from `hive_metastore`.`snapshots`.`productcategory_snapshot`
  ),
  
  transformed as (
      select
          row_number() over (order by p.productId) as product_sk,
          p.productId as productID,
          p.name as product_name,
          p.standardCost,
          p.listPrice,
          p.size,
          p.weight,
          pm.name as model,
          pm.CatalogDescription as description,
          p.sellstartdate,
          p.sellenddate,
          p.discontinueddate
      from product_snapshot p
      left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
      left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
  )
  
  select * from transformed
    
  
[0m13:44:41.966441 [info ] [MainThread]: 
[0m13:44:41.967441 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m13:44:41.968443 [debug] [MainThread]: Command `dbt run` failed at 13:44:41.968443 after 6.50 seconds
[0m13:44:41.968443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001616B8A6A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001610FEE0370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001610FE30970>]}
[0m13:44:41.969440 [debug] [MainThread]: Flushing usage events
[0m13:45:05.189437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F263C26670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F266C054F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F266C05E50>]}


============================== 13:45:05.192437 | 0d6ce9bc-9829-4479-97d3-fcac0df32dc8 ==============================
[0m13:45:05.192437 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:45:05.193438 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select dim_product', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:45:06.170530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0d6ce9bc-9829-4479-97d3-fcac0df32dc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F262B09850>]}
[0m13:45:06.246449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0d6ce9bc-9829-4479-97d3-fcac0df32dc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F207D86130>]}
[0m13:45:06.246449 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m13:45:06.256448 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:45:06.338451 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:45:06.338451 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_modeling://models\marts\product\dim_product.sql
[0m13:45:06.456453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0d6ce9bc-9829-4479-97d3-fcac0df32dc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2082ED0D0>]}
[0m13:45:06.467454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0d6ce9bc-9829-4479-97d3-fcac0df32dc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F20804CEB0>]}
[0m13:45:06.468452 [info ] [MainThread]: Found 8 snapshots, 3 models, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m13:45:06.468452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0d6ce9bc-9829-4479-97d3-fcac0df32dc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F20804CCA0>]}
[0m13:45:06.469450 [info ] [MainThread]: 
[0m13:45:06.471451 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (22420, 9016), compute: ``
[0m13:45:06.471451 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:45:06.471451 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (22420, 9016), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:45:06.472453 [debug] [MainThread]: Databricks adapter: Thread (22420, 9016) using default compute resource.
[0m13:45:06.473449 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore, thread: (22420, 11832), compute: ``
[0m13:45:06.473449 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m13:45:06.474450 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore, thread: (22420, 11832), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:45:06.474450 [debug] [ThreadPool]: Databricks adapter: Thread (22420, 11832) using default compute resource.
[0m13:45:06.474450 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m13:45:06.475449 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m13:45:06.475449 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:45:06.825363 [debug] [ThreadPool]: SQL status: OK in 0.3499999940395355 seconds
[0m13:45:06.829363 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore, thread: (22420, 11832), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:45:06.832362 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (22420, 23024), compute: ``
[0m13:45:06.832362 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m13:45:06.833363 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (22420, 23024), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:45:06.833363 [debug] [ThreadPool]: Databricks adapter: Thread (22420, 23024) using default compute resource.
[0m13:45:06.837363 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:45:06.837363 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m13:45:06.838363 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:45:07.165615 [debug] [ThreadPool]: SQL status: OK in 0.33000001311302185 seconds
[0m13:45:07.180613 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:45:07.180613 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:45:07.180613 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m13:45:07.339133 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m13:45:07.349559 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:45:07.350560 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m13:45:07.572042 [debug] [ThreadPool]: SQL status: OK in 0.2199999988079071 seconds
[0m13:45:07.582043 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:45:07.584042 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show table extended in `hive_metastore`.`saleslt` like '*'
  
[0m13:45:08.081642 [debug] [ThreadPool]: SQL status: OK in 0.5 seconds
[0m13:45:08.085644 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (22420, 23024), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:45:08.087642 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m13:45:08.091642 [debug] [ThreadPool]: Databricks adapter: Reusing DatabricksDBTConnection. name: list_hive_metastore_snapshots, thread: (22420, 23024), compute: ``, acquire_release_count: 0, idle time: 0.004997730255126953s
[0m13:45:08.092644 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_snapshots, thread: (22420, 23024), compute: ``, acquire_release_count: 0, idle time: 0.00599980354309082s
[0m13:45:08.092644 [debug] [ThreadPool]: Databricks adapter: Thread (22420, 23024) using default compute resource.
[0m13:45:08.095643 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:45:08.095643 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m13:45:08.211079 [debug] [ThreadPool]: SQL status: OK in 0.11999999731779099 seconds
[0m13:45:08.218081 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:45:08.218081 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m13:45:08.398129 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m13:45:08.405131 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:45:08.405131 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m13:45:08.564821 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m13:45:08.570823 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:45:08.571824 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show table extended in `hive_metastore`.`snapshots` like '*'
  
[0m13:45:09.113150 [debug] [ThreadPool]: SQL status: OK in 0.5400000214576721 seconds
[0m13:45:09.117151 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_snapshots, thread: (22420, 23024), compute: ``, acquire_release_count: 1, idle time: 1.0305066108703613s
[0m13:45:09.122328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0d6ce9bc-9829-4479-97d3-fcac0df32dc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F208302490>]}
[0m13:45:09.123146 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:45:09.123146 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:45:09.123146 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (22420, 9016), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:45:09.124144 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:45:09.125143 [info ] [MainThread]: 
[0m13:45:09.129144 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.dim_product
[0m13:45:09.129144 [info ] [Thread-1  ]: 1 of 1 START sql table model saleslt.dim_product ............................... [RUN]
[0m13:45:09.131144 [debug] [Thread-1  ]: Databricks adapter: Creating DatabricksDBTConnection. name: model.dbt_spark_modeling.dim_product, thread: (22420, 8968), compute: ``
[0m13:45:09.131144 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbt_spark_modeling.dim_product'
[0m13:45:09.132144 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.dim_product, thread: (22420, 8968), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:45:09.132144 [debug] [Thread-1  ]: Databricks adapter: On thread (22420, 8968): `hive_metastore`.`saleslt`.`dim_product` using default compute resource.
[0m13:45:09.133144 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.dim_product
[0m13:45:09.142144 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.dim_product"
[0m13:45:09.143144 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (compile): 13:45:09.133144 => 13:45:09.143144
[0m13:45:09.144145 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.dim_product
[0m13:45:09.157144 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:45:09.158144 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_product"
[0m13:45:09.158144 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_product: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
[0m13:45:09.159143 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m13:45:10.143660 [debug] [Thread-1  ]: SQL status: OK in 0.9900000095367432 seconds
[0m13:45:10.191660 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.dim_product"
[0m13:45:10.192662 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_product"
[0m13:45:10.192662 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_product: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by ProductCategoryID) as category_id
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  
[0m13:45:10.383173 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by ProductCategoryID) as category_id
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  
[0m13:45:10.384173 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by ProductCategoryID) as category_id
------------------^^^
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  

[0m13:45:10.385172 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by ProductCategoryID) as category_id
------------------^^^
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by ProductCategoryID) as category_id
------------------^^^
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:259)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:111)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:541)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more

[0m13:45:10.386172 [debug] [Thread-1  ]: Databricks adapter: operation-id: fc262750-80ef-4bf2-a3c1-af8bd74d1020
[0m13:45:10.387173 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (execute): 13:45:09.144145 => 13:45:10.387173
[0m13:45:10.388174 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (22420, 8968), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:45:10.395172 [debug] [Thread-1  ]: Runtime Error in model dim_product (models\marts\product\dim_product.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_product`
        
        
      using delta
        
        
        
        
        
      location '/mnt/gold/products/dim_product'
        
        
        as
        
  
  with product_snapshot as (
      select
          productId,
          name,
          standardCost,
          listPrice,
          size,
          weight,
          productcategoryid,
          productmodelid,
          sellstartdate,
          sellenddate,
          discontinueddate
      from `hive_metastore`.`snapshots`.`product_snapshot`
      where dbt_valid_to is null
  ),
  
  product_model_snapshot as (
      select
          productmodelid,
          name,
          CatalogDescription,
          row_number() over (order by name) as model_id
      from `hive_metastore`.`snapshots`.`productmodel_snapshot`
      where dbt_valid_to is null
  ),
  
  productcategory_snapshot as (
      select
          ProductCategoryID,
          Name
          row_number() over (order by ProductCategoryID) as category_id
  ------------------^^^
      from `hive_metastore`.`snapshots`.`productcategory_snapshot`
  ),
  
  transformed as (
      select
          row_number() over (order by p.productId) as product_sk,
          p.productId as productID,
          p.name as product_name,
          p.standardCost,
          p.listPrice,
          p.size,
          p.weight,
          pm.name as model,
          pm.CatalogDescription as description,
          p.sellstartdate,
          p.sellenddate,
          p.discontinueddate
      from product_snapshot p
      left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
      left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
  )
  
  select * from transformed
    
  
[0m13:45:10.396172 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (22420, 8968), compute: ``, acquire_release_count: 0, idle time: 0.007998228073120117s
[0m13:45:10.397171 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0d6ce9bc-9829-4479-97d3-fcac0df32dc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2081715B0>]}
[0m13:45:10.397171 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model saleslt.dim_product ...................... [[31mERROR[0m in 1.27s]
[0m13:45:10.398170 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.dim_product
[0m13:45:10.401170 [debug] [MainThread]: Databricks adapter: Reusing DatabricksDBTConnection. name: master, thread: (22420, 9016), compute: ``, acquire_release_count: 0, idle time: 1.2760264873504639s
[0m13:45:10.401170 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (22420, 9016), compute: ``, acquire_release_count: 0, idle time: 1.2770259380340576s
[0m13:45:10.401170 [debug] [MainThread]: Databricks adapter: Thread (22420, 9016) using default compute resource.
[0m13:45:10.401170 [debug] [MainThread]: On master: ROLLBACK
[0m13:45:10.402170 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:45:10.638832 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:45:10.639832 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:45:10.639832 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:45:10.640831 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (22420, 9016), compute: ``, acquire_release_count: 1, idle time: 1.5156874656677246s
[0m13:45:10.641831 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:45:10.641831 [debug] [MainThread]: On master: ROLLBACK
[0m13:45:10.641831 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:45:10.642831 [debug] [MainThread]: On master: Close
[0m13:45:10.747830 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m13:45:10.748831 [debug] [MainThread]: On list_hive_metastore: Close
[0m13:45:10.826395 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m13:45:10.827348 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m13:45:10.828357 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:45:10.828357 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m13:45:10.908165 [debug] [MainThread]: Connection 'model.dbt_spark_modeling.dim_product' was properly closed.
[0m13:45:10.909165 [debug] [MainThread]: On model.dbt_spark_modeling.dim_product: ROLLBACK
[0m13:45:10.909165 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:45:10.910163 [debug] [MainThread]: On model.dbt_spark_modeling.dim_product: Close
[0m13:45:10.994896 [info ] [MainThread]: 
[0m13:45:10.995410 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.52 seconds (4.52s).
[0m13:45:10.996941 [debug] [MainThread]: Command end result
[0m13:45:11.012953 [info ] [MainThread]: 
[0m13:45:11.012953 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:45:11.013950 [info ] [MainThread]: 
[0m13:45:11.013950 [error] [MainThread]:   Runtime Error in model dim_product (models\marts\product\dim_product.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_product`
        
        
      using delta
        
        
        
        
        
      location '/mnt/gold/products/dim_product'
        
        
        as
        
  
  with product_snapshot as (
      select
          productId,
          name,
          standardCost,
          listPrice,
          size,
          weight,
          productcategoryid,
          productmodelid,
          sellstartdate,
          sellenddate,
          discontinueddate
      from `hive_metastore`.`snapshots`.`product_snapshot`
      where dbt_valid_to is null
  ),
  
  product_model_snapshot as (
      select
          productmodelid,
          name,
          CatalogDescription,
          row_number() over (order by name) as model_id
      from `hive_metastore`.`snapshots`.`productmodel_snapshot`
      where dbt_valid_to is null
  ),
  
  productcategory_snapshot as (
      select
          ProductCategoryID,
          Name
          row_number() over (order by ProductCategoryID) as category_id
  ------------------^^^
      from `hive_metastore`.`snapshots`.`productcategory_snapshot`
  ),
  
  transformed as (
      select
          row_number() over (order by p.productId) as product_sk,
          p.productId as productID,
          p.name as product_name,
          p.standardCost,
          p.listPrice,
          p.size,
          p.weight,
          pm.name as model,
          pm.CatalogDescription as description,
          p.sellstartdate,
          p.sellenddate,
          p.discontinueddate
      from product_snapshot p
      left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
      left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
  )
  
  select * from transformed
    
  
[0m13:45:11.015951 [info ] [MainThread]: 
[0m13:45:11.016951 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m13:45:11.018951 [debug] [MainThread]: Command `dbt run` failed at 13:45:11.018951 after 5.86 seconds
[0m13:45:11.018951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F263C26670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2081F7430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F208241460>]}
[0m13:45:11.019951 [debug] [MainThread]: Flushing usage events
[0m13:48:13.564804 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002306AAC6820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002306DABFEE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002306DABF2B0>]}


============================== 13:48:13.567807 | e7156f20-33bf-43d0-8129-41acaca22f4d ==============================
[0m13:48:13.567807 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:48:13.568804 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select dim_product', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m13:48:14.566760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e7156f20-33bf-43d0-8129-41acaca22f4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230699A98B0>]}
[0m13:48:14.638759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e7156f20-33bf-43d0-8129-41acaca22f4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002300EC6D130>]}
[0m13:48:14.639760 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m13:48:14.648283 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:48:14.728282 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:48:14.728282 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_modeling://models\marts\product\dim_product.sql
[0m13:48:14.845279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e7156f20-33bf-43d0-8129-41acaca22f4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002300F187FA0>]}
[0m13:48:14.856279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e7156f20-33bf-43d0-8129-41acaca22f4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002300EEDD070>]}
[0m13:48:14.856279 [info ] [MainThread]: Found 8 snapshots, 3 models, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m13:48:14.857280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e7156f20-33bf-43d0-8129-41acaca22f4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002300ED75B50>]}
[0m13:48:14.858281 [info ] [MainThread]: 
[0m13:48:14.858281 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (11336, 17088), compute: ``
[0m13:48:14.859280 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:48:14.859280 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (11336, 17088), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:48:14.859280 [debug] [MainThread]: Databricks adapter: Thread (11336, 17088) using default compute resource.
[0m13:48:14.860667 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore, thread: (11336, 17144), compute: ``
[0m13:48:14.861690 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m13:48:14.861690 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore, thread: (11336, 17144), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:48:14.861690 [debug] [ThreadPool]: Databricks adapter: Thread (11336, 17144) using default compute resource.
[0m13:48:14.862679 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m13:48:14.862679 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m13:48:14.862679 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:48:15.319127 [debug] [ThreadPool]: SQL status: OK in 0.46000000834465027 seconds
[0m13:48:15.323126 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore, thread: (11336, 17144), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:48:15.326130 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_snapshots, thread: (11336, 20852), compute: ``
[0m13:48:15.326130 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m13:48:15.327130 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_snapshots, thread: (11336, 20852), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:48:15.327130 [debug] [ThreadPool]: Databricks adapter: Thread (11336, 20852) using default compute resource.
[0m13:48:15.332127 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:48:15.332127 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m13:48:15.332127 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:48:15.884302 [debug] [ThreadPool]: SQL status: OK in 0.550000011920929 seconds
[0m13:48:15.900303 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:48:15.901303 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:48:15.901303 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m13:48:16.030237 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m13:48:16.040234 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:48:16.040234 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m13:48:16.381992 [debug] [ThreadPool]: SQL status: OK in 0.3400000035762787 seconds
[0m13:48:16.387990 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:48:16.388990 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show table extended in `hive_metastore`.`snapshots` like '*'
  
[0m13:48:16.914304 [debug] [ThreadPool]: SQL status: OK in 0.5299999713897705 seconds
[0m13:48:16.919305 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_snapshots, thread: (11336, 20852), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:48:16.921305 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m13:48:16.925307 [debug] [ThreadPool]: Databricks adapter: Reusing DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (11336, 20852), compute: ``, acquire_release_count: 0, idle time: 0.005001068115234375s
[0m13:48:16.925307 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (11336, 20852), compute: ``, acquire_release_count: 0, idle time: 0.005001068115234375s
[0m13:48:16.926303 [debug] [ThreadPool]: Databricks adapter: Thread (11336, 20852) using default compute resource.
[0m13:48:16.928301 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:48:16.929301 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m13:48:17.032422 [debug] [ThreadPool]: SQL status: OK in 0.10000000149011612 seconds
[0m13:48:17.037415 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:48:17.037415 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m13:48:17.152552 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m13:48:17.157550 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:48:17.158552 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m13:48:17.337452 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m13:48:17.344450 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:48:17.344450 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show table extended in `hive_metastore`.`saleslt` like '*'
  
[0m13:48:17.899002 [debug] [ThreadPool]: SQL status: OK in 0.550000011920929 seconds
[0m13:48:17.903998 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (11336, 20852), compute: ``, acquire_release_count: 1, idle time: 0.9836926460266113s
[0m13:48:17.908997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e7156f20-33bf-43d0-8129-41acaca22f4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002300F1045B0>]}
[0m13:48:17.908997 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:48:17.909996 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:48:17.909996 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (11336, 17088), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:48:17.910999 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:48:17.910999 [info ] [MainThread]: 
[0m13:48:17.913997 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.dim_product
[0m13:48:17.913997 [info ] [Thread-1  ]: 1 of 1 START sql table model saleslt.dim_product ............................... [RUN]
[0m13:48:17.914995 [debug] [Thread-1  ]: Databricks adapter: Creating DatabricksDBTConnection. name: model.dbt_spark_modeling.dim_product, thread: (11336, 15024), compute: ``
[0m13:48:17.914995 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbt_spark_modeling.dim_product'
[0m13:48:17.915995 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.dim_product, thread: (11336, 15024), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:48:17.915995 [debug] [Thread-1  ]: Databricks adapter: On thread (11336, 15024): `hive_metastore`.`saleslt`.`dim_product` using default compute resource.
[0m13:48:17.916997 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.dim_product
[0m13:48:17.924997 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.dim_product"
[0m13:48:17.926995 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (compile): 13:48:17.916997 => 13:48:17.925994
[0m13:48:17.926995 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.dim_product
[0m13:48:17.938997 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:48:17.938997 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_product"
[0m13:48:17.939998 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_product: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
[0m13:48:17.939998 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m13:48:18.880675 [debug] [Thread-1  ]: SQL status: OK in 0.9399999976158142 seconds
[0m13:48:18.924668 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.dim_product"
[0m13:48:18.925669 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_product"
[0m13:48:18.925669 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_product: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by ProductCategoryID) as category_id
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        pc.name as product_category
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  
[0m13:48:19.068402 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by ProductCategoryID) as category_id
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        pc.name as product_category
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  
[0m13:48:19.070406 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by ProductCategoryID) as category_id
------------------^^^
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        pc.name as product_category
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  

[0m13:48:19.071405 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by ProductCategoryID) as category_id
------------------^^^
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        pc.name as product_category
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by ProductCategoryID) as category_id
------------------^^^
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        pc.name as product_category
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:259)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:111)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:541)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more

[0m13:48:19.072406 [debug] [Thread-1  ]: Databricks adapter: operation-id: ac5bf05e-0bf4-47c2-af79-a93911d71493
[0m13:48:19.073405 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (execute): 13:48:17.927996 => 13:48:19.072406
[0m13:48:19.073405 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (11336, 15024), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:48:19.080401 [debug] [Thread-1  ]: Runtime Error in model dim_product (models\marts\product\dim_product.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_product`
        
        
      using delta
        
        
        
        
        
      location '/mnt/gold/products/dim_product'
        
        
        as
        
  
  with product_snapshot as (
      select
          productId,
          name,
          standardCost,
          listPrice,
          size,
          weight,
          productcategoryid,
          productmodelid,
          sellstartdate,
          sellenddate,
          discontinueddate
      from `hive_metastore`.`snapshots`.`product_snapshot`
      where dbt_valid_to is null
  ),
  
  product_model_snapshot as (
      select
          productmodelid,
          name,
          CatalogDescription,
          row_number() over (order by name) as model_id
      from `hive_metastore`.`snapshots`.`productmodel_snapshot`
      where dbt_valid_to is null
  ),
  
  productcategory_snapshot as (
      select
          ProductCategoryID,
          Name
          row_number() over (order by ProductCategoryID) as category_id
  ------------------^^^
      from `hive_metastore`.`snapshots`.`productcategory_snapshot`
  ),
  
  transformed as (
      select
          row_number() over (order by p.productId) as product_sk,
          p.productId as productID,
          p.name as product_name,
          pc.name as product_category
          p.standardCost,
          p.listPrice,
          p.size,
          p.weight,
          pm.name as model,
          pm.CatalogDescription as description,
          p.sellstartdate,
          p.sellenddate,
          p.discontinueddate
      from product_snapshot p
      left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
      left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
  )
  
  select * from transformed
    
  
[0m13:48:19.081402 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (11336, 15024), compute: ``, acquire_release_count: 0, idle time: 0.00699615478515625s
[0m13:48:19.081402 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e7156f20-33bf-43d0-8129-41acaca22f4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002300EEDD790>]}
[0m13:48:19.082402 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model saleslt.dim_product ...................... [[31mERROR[0m in 1.17s]
[0m13:48:19.083398 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.dim_product
[0m13:48:19.084397 [debug] [MainThread]: Databricks adapter: Reusing DatabricksDBTConnection. name: master, thread: (11336, 17088), compute: ``, acquire_release_count: 0, idle time: 1.1744003295898438s
[0m13:48:19.084397 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (11336, 17088), compute: ``, acquire_release_count: 0, idle time: 1.1744003295898438s
[0m13:48:19.085401 [debug] [MainThread]: Databricks adapter: Thread (11336, 17088) using default compute resource.
[0m13:48:19.086399 [debug] [MainThread]: On master: ROLLBACK
[0m13:48:19.086399 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:48:19.325261 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:48:19.325261 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:48:19.326263 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:48:19.326263 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (11336, 17088), compute: ``, acquire_release_count: 1, idle time: 1.4162662029266357s
[0m13:48:19.327263 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:48:19.328263 [debug] [MainThread]: On master: ROLLBACK
[0m13:48:19.328263 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:48:19.329263 [debug] [MainThread]: On master: Close
[0m13:48:19.408556 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m13:48:19.408556 [debug] [MainThread]: On list_hive_metastore: Close
[0m13:48:19.503521 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m13:48:19.503521 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m13:48:19.504521 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:48:19.504521 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m13:48:19.645003 [debug] [MainThread]: Connection 'model.dbt_spark_modeling.dim_product' was properly closed.
[0m13:48:19.645003 [debug] [MainThread]: On model.dbt_spark_modeling.dim_product: ROLLBACK
[0m13:48:19.646009 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:48:19.646009 [debug] [MainThread]: On model.dbt_spark_modeling.dim_product: Close
[0m13:48:19.721757 [info ] [MainThread]: 
[0m13:48:19.722755 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.86 seconds (4.86s).
[0m13:48:19.723760 [debug] [MainThread]: Command end result
[0m13:48:19.739752 [info ] [MainThread]: 
[0m13:48:19.739752 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:48:19.740752 [info ] [MainThread]: 
[0m13:48:19.741753 [error] [MainThread]:   Runtime Error in model dim_product (models\marts\product\dim_product.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_product`
        
        
      using delta
        
        
        
        
        
      location '/mnt/gold/products/dim_product'
        
        
        as
        
  
  with product_snapshot as (
      select
          productId,
          name,
          standardCost,
          listPrice,
          size,
          weight,
          productcategoryid,
          productmodelid,
          sellstartdate,
          sellenddate,
          discontinueddate
      from `hive_metastore`.`snapshots`.`product_snapshot`
      where dbt_valid_to is null
  ),
  
  product_model_snapshot as (
      select
          productmodelid,
          name,
          CatalogDescription,
          row_number() over (order by name) as model_id
      from `hive_metastore`.`snapshots`.`productmodel_snapshot`
      where dbt_valid_to is null
  ),
  
  productcategory_snapshot as (
      select
          ProductCategoryID,
          Name
          row_number() over (order by ProductCategoryID) as category_id
  ------------------^^^
      from `hive_metastore`.`snapshots`.`productcategory_snapshot`
  ),
  
  transformed as (
      select
          row_number() over (order by p.productId) as product_sk,
          p.productId as productID,
          p.name as product_name,
          pc.name as product_category
          p.standardCost,
          p.listPrice,
          p.size,
          p.weight,
          pm.name as model,
          pm.CatalogDescription as description,
          p.sellstartdate,
          p.sellenddate,
          p.discontinueddate
      from product_snapshot p
      left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
      left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
  )
  
  select * from transformed
    
  
[0m13:48:19.743751 [info ] [MainThread]: 
[0m13:48:19.745752 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m13:48:19.746751 [debug] [MainThread]: Command `dbt run` failed at 13:48:19.746751 after 6.22 seconds
[0m13:48:19.747751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002306AAC6820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002300ED468B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002300F31EB80>]}
[0m13:48:19.747751 [debug] [MainThread]: Flushing usage events
[0m13:50:50.238843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC99727130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC9C705C10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC9C705EE0>]}


============================== 13:50:50.241843 | 5c61fc4c-ff43-4c56-b922-4e2597e54100 ==============================
[0m13:50:50.241843 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:50:50.242842 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run --select dim_product', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m13:50:51.347304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5c61fc4c-ff43-4c56-b922-4e2597e54100', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC98609880>]}
[0m13:50:51.422306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5c61fc4c-ff43-4c56-b922-4e2597e54100', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCBD7E1400>]}
[0m13:50:51.423306 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m13:50:51.432306 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:50:51.524309 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:50:51.525309 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:50:51.530308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5c61fc4c-ff43-4c56-b922-4e2597e54100', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCBDC7E0D0>]}
[0m13:50:51.546307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5c61fc4c-ff43-4c56-b922-4e2597e54100', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCBDC59F10>]}
[0m13:50:51.547307 [info ] [MainThread]: Found 8 snapshots, 3 models, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m13:50:51.547307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5c61fc4c-ff43-4c56-b922-4e2597e54100', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCBDC59FA0>]}
[0m13:50:51.548306 [info ] [MainThread]: 
[0m13:50:51.549305 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (21680, 16584), compute: ``
[0m13:50:51.549305 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:50:51.550307 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (21680, 16584), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:50:51.550307 [debug] [MainThread]: Databricks adapter: Thread (21680, 16584) using default compute resource.
[0m13:50:51.551308 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore, thread: (21680, 5028), compute: ``
[0m13:50:51.552305 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m13:50:51.552305 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore, thread: (21680, 5028), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:50:51.552305 [debug] [ThreadPool]: Databricks adapter: Thread (21680, 5028) using default compute resource.
[0m13:50:51.552305 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m13:50:51.553309 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m13:50:51.553309 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:50:52.272789 [debug] [ThreadPool]: SQL status: OK in 0.7200000286102295 seconds
[0m13:50:52.276785 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore, thread: (21680, 5028), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:50:52.279785 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (21680, 17052), compute: ``
[0m13:50:52.280785 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m13:50:52.280785 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (21680, 17052), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:50:52.281785 [debug] [ThreadPool]: Databricks adapter: Thread (21680, 17052) using default compute resource.
[0m13:50:52.286783 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:50:52.286783 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m13:50:52.286783 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:50:52.709826 [debug] [ThreadPool]: SQL status: OK in 0.41999998688697815 seconds
[0m13:50:52.724819 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:52.724819 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:50:52.724819 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m13:50:52.890475 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m13:50:52.902472 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:50:52.902472 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m13:50:53.141799 [debug] [ThreadPool]: SQL status: OK in 0.23999999463558197 seconds
[0m13:50:53.151786 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:50:53.151786 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show table extended in `hive_metastore`.`saleslt` like '*'
  
[0m13:50:53.661061 [debug] [ThreadPool]: SQL status: OK in 0.5099999904632568 seconds
[0m13:50:53.666063 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (21680, 17052), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:50:53.667062 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m13:50:53.672065 [debug] [ThreadPool]: Databricks adapter: Reusing DatabricksDBTConnection. name: list_hive_metastore_snapshots, thread: (21680, 17052), compute: ``, acquire_release_count: 0, idle time: 0.006002187728881836s
[0m13:50:53.672065 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_snapshots, thread: (21680, 17052), compute: ``, acquire_release_count: 0, idle time: 0.006002187728881836s
[0m13:50:53.672065 [debug] [ThreadPool]: Databricks adapter: Thread (21680, 17052) using default compute resource.
[0m13:50:53.675060 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:50:53.675060 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m13:50:53.818111 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m13:50:53.824109 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:50:53.825110 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m13:50:53.947312 [debug] [ThreadPool]: SQL status: OK in 0.11999999731779099 seconds
[0m13:50:53.954305 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:50:53.955311 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m13:50:54.109256 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m13:50:54.117254 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:50:54.117254 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show table extended in `hive_metastore`.`snapshots` like '*'
  
[0m13:50:54.748681 [debug] [ThreadPool]: SQL status: OK in 0.6299999952316284 seconds
[0m13:50:54.752680 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_snapshots, thread: (21680, 17052), compute: ``, acquire_release_count: 1, idle time: 1.085618257522583s
[0m13:50:54.755685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5c61fc4c-ff43-4c56-b922-4e2597e54100', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCBDA136A0>]}
[0m13:50:54.756681 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:54.756681 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:50:54.757681 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (21680, 16584), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:50:54.757681 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:50:54.758680 [info ] [MainThread]: 
[0m13:50:54.761677 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.dim_product
[0m13:50:54.761677 [info ] [Thread-1  ]: 1 of 1 START sql table model saleslt.dim_product ............................... [RUN]
[0m13:50:54.763678 [debug] [Thread-1  ]: Databricks adapter: Creating DatabricksDBTConnection. name: model.dbt_spark_modeling.dim_product, thread: (21680, 25104), compute: ``
[0m13:50:54.763678 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbt_spark_modeling.dim_product'
[0m13:50:54.763678 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.dim_product, thread: (21680, 25104), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:50:54.764678 [debug] [Thread-1  ]: Databricks adapter: On thread (21680, 25104): `hive_metastore`.`saleslt`.`dim_product` using default compute resource.
[0m13:50:54.764678 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.dim_product
[0m13:50:54.771680 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.dim_product"
[0m13:50:54.772681 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (compile): 13:50:54.764678 => 13:50:54.772681
[0m13:50:54.773679 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.dim_product
[0m13:50:54.787680 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:54.787680 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_product"
[0m13:50:54.788682 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_product: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
[0m13:50:54.788682 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m13:50:55.539275 [debug] [Thread-1  ]: SQL status: OK in 0.75 seconds
[0m13:50:55.583273 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.dim_product"
[0m13:50:55.584271 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_product"
[0m13:50:55.584271 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_product: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by ProductCategoryID) as category_id
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        pc.name as product_category
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  
[0m13:50:55.744920 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by ProductCategoryID) as category_id
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        pc.name as product_category
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  
[0m13:50:55.745922 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by ProductCategoryID) as category_id
------------------^^^
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        pc.name as product_category
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  

[0m13:50:55.746920 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by ProductCategoryID) as category_id
------------------^^^
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        pc.name as product_category
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name
        row_number() over (order by ProductCategoryID) as category_id
------------------^^^
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        pc.name as product_category
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:259)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:111)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:541)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more

[0m13:50:55.747921 [debug] [Thread-1  ]: Databricks adapter: operation-id: 76c5d19a-1109-4313-9215-008fa82fae24
[0m13:50:55.748921 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (execute): 13:50:54.773679 => 13:50:55.748921
[0m13:50:55.748921 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (21680, 25104), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:50:55.755917 [debug] [Thread-1  ]: Runtime Error in model dim_product (models\marts\product\dim_product.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_product`
        
        
      using delta
        
        
        
        
        
      location '/mnt/gold/products/dim_product'
        
        
        as
        
  
  with product_snapshot as (
      select
          productId,
          name,
          standardCost,
          listPrice,
          size,
          weight,
          productcategoryid,
          productmodelid,
          sellstartdate,
          sellenddate,
          discontinueddate
      from `hive_metastore`.`snapshots`.`product_snapshot`
      where dbt_valid_to is null
  ),
  
  product_model_snapshot as (
      select
          productmodelid,
          name,
          CatalogDescription,
          row_number() over (order by name) as model_id
      from `hive_metastore`.`snapshots`.`productmodel_snapshot`
      where dbt_valid_to is null
  ),
  
  productcategory_snapshot as (
      select
          ProductCategoryID,
          Name
          row_number() over (order by ProductCategoryID) as category_id
  ------------------^^^
      from `hive_metastore`.`snapshots`.`productcategory_snapshot`
  ),
  
  transformed as (
      select
          row_number() over (order by p.productId) as product_sk,
          p.productId as productID,
          p.name as product_name,
          pc.name as product_category
          p.standardCost,
          p.listPrice,
          p.size,
          p.weight,
          pm.name as model,
          pm.CatalogDescription as description,
          p.sellstartdate,
          p.sellenddate,
          p.discontinueddate
      from product_snapshot p
      left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
      left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
  )
  
  select * from transformed
    
  
[0m13:50:55.755917 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (21680, 25104), compute: ``, acquire_release_count: 0, idle time: 0.005995512008666992s
[0m13:50:55.756918 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5c61fc4c-ff43-4c56-b922-4e2597e54100', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCBDD645E0>]}
[0m13:50:55.756918 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model saleslt.dim_product ...................... [[31mERROR[0m in 0.99s]
[0m13:50:55.757917 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.dim_product
[0m13:50:55.758915 [debug] [MainThread]: Databricks adapter: Reusing DatabricksDBTConnection. name: master, thread: (21680, 16584), compute: ``, acquire_release_count: 0, idle time: 1.001234531402588s
[0m13:50:55.759916 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (21680, 16584), compute: ``, acquire_release_count: 0, idle time: 1.0022351741790771s
[0m13:50:55.759916 [debug] [MainThread]: Databricks adapter: Thread (21680, 16584) using default compute resource.
[0m13:50:55.759916 [debug] [MainThread]: On master: ROLLBACK
[0m13:50:55.760916 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:50:55.991971 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:50:55.991971 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:55.992972 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:50:55.992972 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (21680, 16584), compute: ``, acquire_release_count: 1, idle time: 1.235290765762329s
[0m13:50:55.993972 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:50:55.994972 [debug] [MainThread]: On master: ROLLBACK
[0m13:50:55.995974 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:50:55.995974 [debug] [MainThread]: On master: Close
[0m13:50:56.083917 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m13:50:56.084916 [debug] [MainThread]: On list_hive_metastore: Close
[0m13:50:56.170872 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m13:50:56.170872 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m13:50:56.171872 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:50:56.171872 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m13:50:56.247410 [debug] [MainThread]: Connection 'model.dbt_spark_modeling.dim_product' was properly closed.
[0m13:50:56.248402 [debug] [MainThread]: On model.dbt_spark_modeling.dim_product: ROLLBACK
[0m13:50:56.248402 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:50:56.249402 [debug] [MainThread]: On model.dbt_spark_modeling.dim_product: Close
[0m13:50:56.336495 [info ] [MainThread]: 
[0m13:50:56.337498 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.79 seconds (4.79s).
[0m13:50:56.339500 [debug] [MainThread]: Command end result
[0m13:50:56.434494 [info ] [MainThread]: 
[0m13:50:56.435495 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:50:56.435495 [info ] [MainThread]: 
[0m13:50:56.436496 [error] [MainThread]:   Runtime Error in model dim_product (models\marts\product\dim_product.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '(': extra input '('.(line 51, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_product`
        
        
      using delta
        
        
        
        
        
      location '/mnt/gold/products/dim_product'
        
        
        as
        
  
  with product_snapshot as (
      select
          productId,
          name,
          standardCost,
          listPrice,
          size,
          weight,
          productcategoryid,
          productmodelid,
          sellstartdate,
          sellenddate,
          discontinueddate
      from `hive_metastore`.`snapshots`.`product_snapshot`
      where dbt_valid_to is null
  ),
  
  product_model_snapshot as (
      select
          productmodelid,
          name,
          CatalogDescription,
          row_number() over (order by name) as model_id
      from `hive_metastore`.`snapshots`.`productmodel_snapshot`
      where dbt_valid_to is null
  ),
  
  productcategory_snapshot as (
      select
          ProductCategoryID,
          Name
          row_number() over (order by ProductCategoryID) as category_id
  ------------------^^^
      from `hive_metastore`.`snapshots`.`productcategory_snapshot`
  ),
  
  transformed as (
      select
          row_number() over (order by p.productId) as product_sk,
          p.productId as productID,
          p.name as product_name,
          pc.name as product_category
          p.standardCost,
          p.listPrice,
          p.size,
          p.weight,
          pm.name as model,
          pm.CatalogDescription as description,
          p.sellstartdate,
          p.sellenddate,
          p.discontinueddate
      from product_snapshot p
      left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
      left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
  )
  
  select * from transformed
    
  
[0m13:50:56.438495 [info ] [MainThread]: 
[0m13:50:56.438495 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m13:50:56.439494 [debug] [MainThread]: Command `dbt run` failed at 13:50:56.439494 after 6.24 seconds
[0m13:50:56.440496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC99727130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCBDD1CA00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCBDD1C0A0>]}
[0m13:50:56.441496 [debug] [MainThread]: Flushing usage events
[0m13:51:19.916442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AFC1C26820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AFC4C1EEE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AFC4C1E2B0>]}


============================== 13:51:19.919442 | e158ac63-4ad9-4040-9449-cae7f92c910c ==============================
[0m13:51:19.919442 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:51:19.920438 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run --select dim_product', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m13:51:20.963473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e158ac63-4ad9-4040-9449-cae7f92c910c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AFC0B098B0>]}
[0m13:51:21.040471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e158ac63-4ad9-4040-9449-cae7f92c910c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AFE5E0B0A0>]}
[0m13:51:21.041472 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m13:51:21.050476 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:51:21.128471 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:51:21.129471 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_modeling://models\marts\product\dim_product.sql
[0m13:51:21.253476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e158ac63-4ad9-4040-9449-cae7f92c910c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AFE62ED0D0>]}
[0m13:51:21.266474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e158ac63-4ad9-4040-9449-cae7f92c910c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AFE60499D0>]}
[0m13:51:21.266474 [info ] [MainThread]: Found 8 snapshots, 3 models, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m13:51:21.267475 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e158ac63-4ad9-4040-9449-cae7f92c910c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AFE60498E0>]}
[0m13:51:21.268477 [info ] [MainThread]: 
[0m13:51:21.268477 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (3336, 10372), compute: ``
[0m13:51:21.269473 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:51:21.269473 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (3336, 10372), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:51:21.269473 [debug] [MainThread]: Databricks adapter: Thread (3336, 10372) using default compute resource.
[0m13:51:21.271473 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore, thread: (3336, 17132), compute: ``
[0m13:51:21.272474 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m13:51:21.272474 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore, thread: (3336, 17132), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:51:21.272474 [debug] [ThreadPool]: Databricks adapter: Thread (3336, 17132) using default compute resource.
[0m13:51:21.272474 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m13:51:21.273473 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m13:51:21.273473 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:51:21.766324 [debug] [ThreadPool]: SQL status: OK in 0.49000000953674316 seconds
[0m13:51:21.771318 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore, thread: (3336, 17132), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:51:21.773662 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_snapshots, thread: (3336, 22136), compute: ``
[0m13:51:21.774688 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m13:51:21.774688 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_snapshots, thread: (3336, 22136), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:51:21.774688 [debug] [ThreadPool]: Databricks adapter: Thread (3336, 22136) using default compute resource.
[0m13:51:21.779681 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:51:21.779681 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m13:51:21.779681 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:51:22.126506 [debug] [ThreadPool]: SQL status: OK in 0.3499999940395355 seconds
[0m13:51:22.140504 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:22.140504 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:51:22.140504 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m13:51:22.265488 [debug] [ThreadPool]: SQL status: OK in 0.11999999731779099 seconds
[0m13:51:22.277482 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:51:22.277482 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m13:51:22.497192 [debug] [ThreadPool]: SQL status: OK in 0.2199999988079071 seconds
[0m13:51:22.503191 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:51:22.504192 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show table extended in `hive_metastore`.`snapshots` like '*'
  
[0m13:51:23.032348 [debug] [ThreadPool]: SQL status: OK in 0.5299999713897705 seconds
[0m13:51:23.036347 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_snapshots, thread: (3336, 22136), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:51:23.037348 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m13:51:23.042348 [debug] [ThreadPool]: Databricks adapter: Reusing DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (3336, 22136), compute: ``, acquire_release_count: 0, idle time: 0.005000591278076172s
[0m13:51:23.042348 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (3336, 22136), compute: ``, acquire_release_count: 0, idle time: 0.006001472473144531s
[0m13:51:23.042348 [debug] [ThreadPool]: Databricks adapter: Thread (3336, 22136) using default compute resource.
[0m13:51:23.045347 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:51:23.045347 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m13:51:23.148265 [debug] [ThreadPool]: SQL status: OK in 0.10000000149011612 seconds
[0m13:51:23.155818 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:51:23.155818 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m13:51:23.273055 [debug] [ThreadPool]: SQL status: OK in 0.11999999731779099 seconds
[0m13:51:23.279055 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:51:23.280059 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m13:51:23.432171 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m13:51:23.439174 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:51:23.439174 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show table extended in `hive_metastore`.`saleslt` like '*'
  
[0m13:51:23.914693 [debug] [ThreadPool]: SQL status: OK in 0.47999998927116394 seconds
[0m13:51:23.918697 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (3336, 22136), compute: ``, acquire_release_count: 1, idle time: 0.8823497295379639s
[0m13:51:23.923696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e158ac63-4ad9-4040-9449-cae7f92c910c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AFE5ECA2B0>]}
[0m13:51:23.923696 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:23.923696 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:51:23.924690 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (3336, 10372), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:51:23.924690 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:51:23.925689 [info ] [MainThread]: 
[0m13:51:23.928689 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.dim_product
[0m13:51:23.928689 [info ] [Thread-1  ]: 1 of 1 START sql table model saleslt.dim_product ............................... [RUN]
[0m13:51:23.930691 [debug] [Thread-1  ]: Databricks adapter: Creating DatabricksDBTConnection. name: model.dbt_spark_modeling.dim_product, thread: (3336, 21988), compute: ``
[0m13:51:23.931688 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbt_spark_modeling.dim_product'
[0m13:51:23.931688 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.dim_product, thread: (3336, 21988), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:51:23.931688 [debug] [Thread-1  ]: Databricks adapter: On thread (3336, 21988): `hive_metastore`.`saleslt`.`dim_product` using default compute resource.
[0m13:51:23.931688 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.dim_product
[0m13:51:23.940689 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.dim_product"
[0m13:51:23.941688 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (compile): 13:51:23.932688 => 13:51:23.941688
[0m13:51:23.942690 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.dim_product
[0m13:51:23.953688 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:23.954688 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_product"
[0m13:51:23.954688 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_product: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
[0m13:51:23.954688 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m13:51:24.598311 [debug] [Thread-1  ]: SQL status: OK in 0.6399999856948853 seconds
[0m13:51:24.640307 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.dim_product"
[0m13:51:24.641309 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_product"
[0m13:51:24.642308 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_product: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name,
        row_number() over (order by ProductCategoryID) as category_id
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        pc.name as product_category
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  
[0m13:51:24.777776 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name,
        row_number() over (order by ProductCategoryID) as category_id
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        pc.name as product_category
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  
[0m13:51:24.778778 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'p'.(line 61, pos 8)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name,
        row_number() over (order by ProductCategoryID) as category_id
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        pc.name as product_category
        p.standardCost,
--------^^^
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  

[0m13:51:24.779777 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'p'.(line 61, pos 8)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name,
        row_number() over (order by ProductCategoryID) as category_id
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        pc.name as product_category
        p.standardCost,
--------^^^
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'p'.(line 61, pos 8)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name,
        row_number() over (order by ProductCategoryID) as category_id
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        pc.name as product_category
        p.standardCost,
--------^^^
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:259)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:111)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:541)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more

[0m13:51:24.781777 [debug] [Thread-1  ]: Databricks adapter: operation-id: 28117e66-b321-4c91-9258-375cb7653c52
[0m13:51:24.781777 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (execute): 13:51:23.942690 => 13:51:24.781777
[0m13:51:24.782778 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (3336, 21988), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:51:24.790772 [debug] [Thread-1  ]: Runtime Error in model dim_product (models\marts\product\dim_product.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'p'.(line 61, pos 8)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_product`
        
        
      using delta
        
        
        
        
        
      location '/mnt/gold/products/dim_product'
        
        
        as
        
  
  with product_snapshot as (
      select
          productId,
          name,
          standardCost,
          listPrice,
          size,
          weight,
          productcategoryid,
          productmodelid,
          sellstartdate,
          sellenddate,
          discontinueddate
      from `hive_metastore`.`snapshots`.`product_snapshot`
      where dbt_valid_to is null
  ),
  
  product_model_snapshot as (
      select
          productmodelid,
          name,
          CatalogDescription,
          row_number() over (order by name) as model_id
      from `hive_metastore`.`snapshots`.`productmodel_snapshot`
      where dbt_valid_to is null
  ),
  
  productcategory_snapshot as (
      select
          ProductCategoryID,
          Name,
          row_number() over (order by ProductCategoryID) as category_id
      from `hive_metastore`.`snapshots`.`productcategory_snapshot`
  ),
  
  transformed as (
      select
          row_number() over (order by p.productId) as product_sk,
          p.productId as productID,
          p.name as product_name,
          pc.name as product_category
          p.standardCost,
  --------^^^
          p.listPrice,
          p.size,
          p.weight,
          pm.name as model,
          pm.CatalogDescription as description,
          p.sellstartdate,
          p.sellenddate,
          p.discontinueddate
      from product_snapshot p
      left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
      left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
  )
  
  select * from transformed
    
  
[0m13:51:24.790772 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (3336, 21988), compute: ``, acquire_release_count: 0, idle time: 0.007994413375854492s
[0m13:51:24.791775 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e158ac63-4ad9-4040-9449-cae7f92c910c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AFE6067910>]}
[0m13:51:24.791775 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model saleslt.dim_product ...................... [[31mERROR[0m in 0.86s]
[0m13:51:24.792773 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.dim_product
[0m13:51:24.793771 [debug] [MainThread]: Databricks adapter: Reusing DatabricksDBTConnection. name: master, thread: (3336, 10372), compute: ``, acquire_release_count: 0, idle time: 0.8690814971923828s
[0m13:51:24.794773 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (3336, 10372), compute: ``, acquire_release_count: 0, idle time: 0.8700833320617676s
[0m13:51:24.794773 [debug] [MainThread]: Databricks adapter: Thread (3336, 10372) using default compute resource.
[0m13:51:24.794773 [debug] [MainThread]: On master: ROLLBACK
[0m13:51:24.795772 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:51:25.045403 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:51:25.046401 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:25.047402 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:51:25.047402 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (3336, 10372), compute: ``, acquire_release_count: 1, idle time: 1.1227118968963623s
[0m13:51:25.049400 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:51:25.049400 [debug] [MainThread]: On master: ROLLBACK
[0m13:51:25.050402 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:51:25.050402 [debug] [MainThread]: On master: Close
[0m13:51:25.145960 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m13:51:25.146959 [debug] [MainThread]: On list_hive_metastore: Close
[0m13:51:25.444751 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m13:51:25.444751 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m13:51:25.445752 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:51:25.445752 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m13:51:25.515326 [debug] [MainThread]: Connection 'model.dbt_spark_modeling.dim_product' was properly closed.
[0m13:51:25.516322 [debug] [MainThread]: On model.dbt_spark_modeling.dim_product: ROLLBACK
[0m13:51:25.517322 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:51:25.518321 [debug] [MainThread]: On model.dbt_spark_modeling.dim_product: Close
[0m13:51:25.594694 [info ] [MainThread]: 
[0m13:51:25.595702 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.33 seconds (4.33s).
[0m13:51:25.596697 [debug] [MainThread]: Command end result
[0m13:51:25.614694 [info ] [MainThread]: 
[0m13:51:25.615695 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:51:25.615695 [info ] [MainThread]: 
[0m13:51:25.616697 [error] [MainThread]:   Runtime Error in model dim_product (models\marts\product\dim_product.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'p'.(line 61, pos 8)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_product`
        
        
      using delta
        
        
        
        
        
      location '/mnt/gold/products/dim_product'
        
        
        as
        
  
  with product_snapshot as (
      select
          productId,
          name,
          standardCost,
          listPrice,
          size,
          weight,
          productcategoryid,
          productmodelid,
          sellstartdate,
          sellenddate,
          discontinueddate
      from `hive_metastore`.`snapshots`.`product_snapshot`
      where dbt_valid_to is null
  ),
  
  product_model_snapshot as (
      select
          productmodelid,
          name,
          CatalogDescription,
          row_number() over (order by name) as model_id
      from `hive_metastore`.`snapshots`.`productmodel_snapshot`
      where dbt_valid_to is null
  ),
  
  productcategory_snapshot as (
      select
          ProductCategoryID,
          Name,
          row_number() over (order by ProductCategoryID) as category_id
      from `hive_metastore`.`snapshots`.`productcategory_snapshot`
  ),
  
  transformed as (
      select
          row_number() over (order by p.productId) as product_sk,
          p.productId as productID,
          p.name as product_name,
          pc.name as product_category
          p.standardCost,
  --------^^^
          p.listPrice,
          p.size,
          p.weight,
          pm.name as model,
          pm.CatalogDescription as description,
          p.sellstartdate,
          p.sellenddate,
          p.discontinueddate
      from product_snapshot p
      left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
      left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
  )
  
  select * from transformed
    
  
[0m13:51:25.618695 [info ] [MainThread]: 
[0m13:51:25.620695 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m13:51:25.621695 [debug] [MainThread]: Command `dbt run` failed at 13:51:25.621695 after 5.74 seconds
[0m13:51:25.621695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AFC1C26820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AFE5EF53D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AFE62417F0>]}
[0m13:51:25.622697 [debug] [MainThread]: Flushing usage events
[0m13:51:51.871689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E48FD26820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E492D1FEE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E492D1F2B0>]}


============================== 13:51:51.874689 | 138527e0-0e78-4a8c-adea-4eb361bc802f ==============================
[0m13:51:51.874689 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:51:51.874689 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --select dim_product', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m13:51:52.928964 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '138527e0-0e78-4a8c-adea-4eb361bc802f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E48EC098B0>]}
[0m13:51:53.006966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '138527e0-0e78-4a8c-adea-4eb361bc802f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E492D04940>]}
[0m13:51:53.007966 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m13:51:53.016965 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:51:53.103965 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:51:53.104966 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_modeling://models\marts\product\dim_product.sql
[0m13:51:53.233482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '138527e0-0e78-4a8c-adea-4eb361bc802f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4B43EF0D0>]}
[0m13:51:53.247482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '138527e0-0e78-4a8c-adea-4eb361bc802f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4B4149B80>]}
[0m13:51:53.247482 [info ] [MainThread]: Found 8 snapshots, 3 models, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m13:51:53.248479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '138527e0-0e78-4a8c-adea-4eb361bc802f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4B4149970>]}
[0m13:51:53.249479 [info ] [MainThread]: 
[0m13:51:53.250481 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (8828, 13712), compute: ``
[0m13:51:53.251481 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:51:53.251481 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (8828, 13712), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:51:53.252480 [debug] [MainThread]: Databricks adapter: Thread (8828, 13712) using default compute resource.
[0m13:51:53.253478 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore, thread: (8828, 15072), compute: ``
[0m13:51:53.254480 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m13:51:53.254480 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore, thread: (8828, 15072), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:51:53.254480 [debug] [ThreadPool]: Databricks adapter: Thread (8828, 15072) using default compute resource.
[0m13:51:53.255478 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m13:51:53.255478 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m13:51:53.255478 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:51:53.594073 [debug] [ThreadPool]: SQL status: OK in 0.3400000035762787 seconds
[0m13:51:53.598072 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore, thread: (8828, 15072), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:51:53.601070 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (8828, 17108), compute: ``
[0m13:51:53.602073 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m13:51:53.602073 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (8828, 17108), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:51:53.602073 [debug] [ThreadPool]: Databricks adapter: Thread (8828, 17108) using default compute resource.
[0m13:51:53.607071 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:51:53.607071 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m13:51:53.607071 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:51:54.041081 [debug] [ThreadPool]: SQL status: OK in 0.4300000071525574 seconds
[0m13:51:54.057080 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:54.057080 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:51:54.058081 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m13:51:54.243230 [debug] [ThreadPool]: SQL status: OK in 0.1899999976158142 seconds
[0m13:51:54.253224 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:51:54.254225 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m13:51:54.486240 [debug] [ThreadPool]: SQL status: OK in 0.23000000417232513 seconds
[0m13:51:54.494237 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m13:51:54.494237 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show table extended in `hive_metastore`.`saleslt` like '*'
  
[0m13:51:54.972526 [debug] [ThreadPool]: SQL status: OK in 0.47999998927116394 seconds
[0m13:51:54.976503 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (8828, 17108), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:51:54.978503 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m13:51:54.982559 [debug] [ThreadPool]: Databricks adapter: Reusing DatabricksDBTConnection. name: list_hive_metastore_snapshots, thread: (8828, 17108), compute: ``, acquire_release_count: 0, idle time: 0.005056619644165039s
[0m13:51:54.982559 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_snapshots, thread: (8828, 17108), compute: ``, acquire_release_count: 0, idle time: 0.005056619644165039s
[0m13:51:54.983581 [debug] [ThreadPool]: Databricks adapter: Thread (8828, 17108) using default compute resource.
[0m13:51:54.985578 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:51:54.985578 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m13:51:55.139028 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m13:51:55.145025 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:51:55.146025 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m13:51:55.286026 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m13:51:55.292024 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:51:55.293024 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m13:51:55.449157 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m13:51:55.455140 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m13:51:55.457141 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show table extended in `hive_metastore`.`snapshots` like '*'
  
[0m13:51:56.256732 [debug] [ThreadPool]: SQL status: OK in 0.800000011920929 seconds
[0m13:51:56.260729 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_snapshots, thread: (8828, 17108), compute: ``, acquire_release_count: 1, idle time: 1.283226490020752s
[0m13:51:56.264728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '138527e0-0e78-4a8c-adea-4eb361bc802f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4B3F8C580>]}
[0m13:51:56.265733 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:56.265733 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:51:56.265733 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (8828, 13712), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:51:56.266727 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:51:56.267728 [info ] [MainThread]: 
[0m13:51:56.269817 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.dim_product
[0m13:51:56.270831 [info ] [Thread-1  ]: 1 of 1 START sql table model saleslt.dim_product ............................... [RUN]
[0m13:51:56.270831 [debug] [Thread-1  ]: Databricks adapter: Creating DatabricksDBTConnection. name: model.dbt_spark_modeling.dim_product, thread: (8828, 24304), compute: ``
[0m13:51:56.271830 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbt_spark_modeling.dim_product'
[0m13:51:56.271830 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.dim_product, thread: (8828, 24304), compute: ``, acquire_release_count: 0, idle time: 0s
[0m13:51:56.272830 [debug] [Thread-1  ]: Databricks adapter: On thread (8828, 24304): `hive_metastore`.`saleslt`.`dim_product` using default compute resource.
[0m13:51:56.272830 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.dim_product
[0m13:51:56.280828 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.dim_product"
[0m13:51:56.280828 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (compile): 13:51:56.272830 => 13:51:56.280828
[0m13:51:56.281830 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.dim_product
[0m13:51:56.293829 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:56.293829 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_product"
[0m13:51:56.293829 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_product: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
[0m13:51:56.294829 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m13:51:57.219675 [debug] [Thread-1  ]: SQL status: OK in 0.9300000071525574 seconds
[0m13:51:57.264827 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.dim_product"
[0m13:51:57.264827 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_product"
[0m13:51:57.265826 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_product: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name,
        row_number() over (order by ProductCategoryID) as category_id
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        pc.name as product_category,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  
[0m13:52:01.300251 [debug] [Thread-1  ]: SQL status: OK in 4.03000020980835 seconds
[0m13:52:01.328778 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (execute): 13:51:56.281830 => 13:52:01.328778
[0m13:52:01.328778 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (8828, 24304), compute: ``, acquire_release_count: 1, idle time: 0s
[0m13:52:01.329778 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (8828, 24304), compute: ``, acquire_release_count: 0, idle time: 0.0009996891021728516s
[0m13:52:01.329778 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '138527e0-0e78-4a8c-adea-4eb361bc802f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4B4168790>]}
[0m13:52:01.330779 [info ] [Thread-1  ]: 1 of 1 OK created sql table model saleslt.dim_product .......................... [[32mOK[0m in 5.06s]
[0m13:52:01.330779 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.dim_product
[0m13:52:01.332777 [debug] [MainThread]: Databricks adapter: Reusing DatabricksDBTConnection. name: master, thread: (8828, 13712), compute: ``, acquire_release_count: 0, idle time: 5.066049814224243s
[0m13:52:01.332777 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (8828, 13712), compute: ``, acquire_release_count: 0, idle time: 5.066049814224243s
[0m13:52:01.332777 [debug] [MainThread]: Databricks adapter: Thread (8828, 13712) using default compute resource.
[0m13:52:01.333778 [debug] [MainThread]: On master: ROLLBACK
[0m13:52:01.333778 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:52:01.635235 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:52:01.635235 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:52:01.636235 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:52:01.636235 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (8828, 13712), compute: ``, acquire_release_count: 1, idle time: 5.369508266448975s
[0m13:52:01.637235 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:52:01.638236 [debug] [MainThread]: On master: ROLLBACK
[0m13:52:01.638236 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:52:01.639235 [debug] [MainThread]: On master: Close
[0m13:52:01.712761 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m13:52:01.712761 [debug] [MainThread]: On list_hive_metastore: Close
[0m13:52:01.795211 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m13:52:01.795211 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m13:52:01.796210 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:52:01.796210 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m13:52:01.885145 [debug] [MainThread]: Connection 'model.dbt_spark_modeling.dim_product' was properly closed.
[0m13:52:01.885961 [debug] [MainThread]: On model.dbt_spark_modeling.dim_product: ROLLBACK
[0m13:52:01.885961 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:52:01.886961 [debug] [MainThread]: On model.dbt_spark_modeling.dim_product: Close
[0m13:52:01.966309 [info ] [MainThread]: 
[0m13:52:01.966309 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 8.72 seconds (8.72s).
[0m13:52:01.968310 [debug] [MainThread]: Command end result
[0m13:52:01.983411 [info ] [MainThread]: 
[0m13:52:01.983411 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:52:01.984309 [info ] [MainThread]: 
[0m13:52:01.984309 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m13:52:01.985313 [debug] [MainThread]: Command `dbt run` succeeded at 13:52:01.985313 after 10.15 seconds
[0m13:52:01.986309 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E48FD26820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4B42E3F40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4B436A9D0>]}
[0m13:52:01.986309 [debug] [MainThread]: Flushing usage events
[0m11:48:15.630406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B2D20C6820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B2D50A59A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B2D50A59D0>]}


============================== 11:48:15.635402 | fa72aa68-2abb-4e52-b618-9f52dd115d9f ==============================
[0m11:48:15.635402 [info ] [MainThread]: Running with dbt=1.7.4
[0m11:48:15.636404 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:48:19.475729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fa72aa68-2abb-4e52-b618-9f52dd115d9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B2D50BF0D0>]}
[0m11:48:19.603882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fa72aa68-2abb-4e52-b618-9f52dd115d9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B2F61DC070>]}
[0m11:48:19.603882 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m11:48:19.628895 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m11:48:20.917406 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:48:20.917406 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:48:20.926407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fa72aa68-2abb-4e52-b618-9f52dd115d9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B2F661D0D0>]}
[0m11:48:20.946616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fa72aa68-2abb-4e52-b618-9f52dd115d9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B2F65F8E80>]}
[0m11:48:20.946616 [info ] [MainThread]: Found 8 snapshots, 3 models, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m11:48:20.947704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fa72aa68-2abb-4e52-b618-9f52dd115d9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B2F65F8E50>]}
[0m11:48:20.949731 [info ] [MainThread]: 
[0m11:48:20.950729 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (21124, 27448), compute: ``
[0m11:48:20.951732 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:48:20.952727 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (21124, 27448), compute: ``, acquire_release_count: 0, idle time: 0s
[0m11:48:20.952727 [debug] [MainThread]: Databricks adapter: Thread (21124, 27448) using default compute resource.
[0m11:48:20.955085 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore, thread: (21124, 10232), compute: ``
[0m11:48:20.956083 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m11:48:20.956083 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore, thread: (21124, 10232), compute: ``, acquire_release_count: 0, idle time: 0s
[0m11:48:20.957086 [debug] [ThreadPool]: Databricks adapter: Thread (21124, 10232) using default compute resource.
[0m11:48:20.957086 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m11:48:20.958085 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m11:48:20.958085 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:51:18.562260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002836E396A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002837137E880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002837137ED90>]}


============================== 11:51:18.567261 | 0bc9ebd2-0a08-4c29-9ae8-40a4169b9867 ==============================
[0m11:51:18.567261 [info ] [MainThread]: Running with dbt=1.7.4
[0m11:51:18.569261 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:51:18.569261 [info ] [MainThread]: dbt version: 1.7.4
[0m11:51:18.570261 [info ] [MainThread]: python version: 3.9.0
[0m11:51:18.571260 [info ] [MainThread]: python path: D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\venv\Scripts\python.exe
[0m11:51:18.572261 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m11:51:20.154004 [info ] [MainThread]: Using profiles dir at C:\Users\EliteSniper\.dbt
[0m11:51:20.154826 [info ] [MainThread]: Using profiles.yml file at C:\Users\EliteSniper\.dbt\profiles.yml
[0m11:51:20.155852 [info ] [MainThread]: Using dbt_project.yml file at D:\DataEngineering\Data_Engineering_Projects\azure-dbt-spark\dbt_spark_modeling\dbt_project.yml
[0m11:51:20.156858 [info ] [MainThread]: adapter type: databricks
[0m11:51:20.156858 [info ] [MainThread]: adapter version: 1.7.3
[0m11:51:20.272940 [info ] [MainThread]: Configuration:
[0m11:51:20.273956 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:51:20.274944 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:51:20.275934 [info ] [MainThread]: Required dependencies:
[0m11:51:20.276952 [debug] [MainThread]: Executing "git --help"
[0m11:51:20.332332 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:51:20.333332 [debug] [MainThread]: STDERR: "b''"
[0m11:51:20.334332 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:51:20.335323 [info ] [MainThread]: Connection:
[0m11:51:20.336332 [info ] [MainThread]:   host: adb-6516581332475033.13.azuredatabricks.net
[0m11:51:20.337332 [info ] [MainThread]:   http_path: sql/protocolv1/o/6516581332475033/1219-172427-cg5hgyuy
[0m11:51:20.338332 [info ] [MainThread]:   catalog: hive_metastore
[0m11:51:20.339317 [info ] [MainThread]:   schema: saleslt
[0m11:51:20.340332 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m11:51:20.341320 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: debug, thread: (20224, 412), compute: ``
[0m11:51:20.342332 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m11:51:20.343332 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: debug, thread: (20224, 412), compute: ``, acquire_release_count: 0, idle time: 0s
[0m11:51:20.344488 [debug] [MainThread]: Databricks adapter: Thread (20224, 412) using default compute resource.
[0m11:51:20.344488 [debug] [MainThread]: Using databricks connection "debug"
[0m11:51:20.345514 [debug] [MainThread]: On debug: select 1 as id
[0m11:51:20.345514 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:52:02.787503 [debug] [MainThread]: SQL status: OK in 42.439998626708984 seconds
[0m11:52:03.073921 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: debug, thread: (20224, 412), compute: ``, acquire_release_count: 1, idle time: 0s
[0m11:52:03.074917 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:52:03.075917 [info ] [MainThread]: [32mAll checks passed![0m
[0m11:52:03.077919 [debug] [MainThread]: Command `dbt debug` succeeded at 11:52:03.077919 after 44.58 seconds
[0m11:52:03.078918 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:52:03.078918 [debug] [MainThread]: On debug: Close
[0m11:52:03.308728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002836E396A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002837135D520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283125B8610>]}
[0m11:52:03.309722 [debug] [MainThread]: Flushing usage events
[0m11:58:41.908536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F10C8E72E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F10F8DF2B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F10F8DF730>]}


============================== 11:58:41.913558 | de815c2a-a852-499e-a47d-e123e1ac7ed1 ==============================
[0m11:58:41.913558 [info ] [MainThread]: Running with dbt=1.7.4
[0m11:58:41.914558 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt snapshot', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:58:43.656356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'de815c2a-a852-499e-a47d-e123e1ac7ed1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F10B7CA910>]}
[0m11:58:43.780930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'de815c2a-a852-499e-a47d-e123e1ac7ed1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F10F8F6340>]}
[0m11:58:43.781930 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m11:58:43.797909 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m11:58:43.926486 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:58:43.927995 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:58:43.936007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'de815c2a-a852-499e-a47d-e123e1ac7ed1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F130E3D0D0>]}
[0m11:58:43.956006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'de815c2a-a852-499e-a47d-e123e1ac7ed1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F130C67CD0>]}
[0m11:58:43.957022 [info ] [MainThread]: Found 8 snapshots, 3 models, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m11:58:43.957022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'de815c2a-a852-499e-a47d-e123e1ac7ed1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F130C67EB0>]}
[0m11:58:43.960006 [info ] [MainThread]: 
[0m11:58:43.961759 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (27292, 3432), compute: ``
[0m11:58:43.961759 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:58:43.962759 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (27292, 3432), compute: ``, acquire_release_count: 0, idle time: 0s
[0m11:58:43.962759 [debug] [MainThread]: Databricks adapter: Thread (27292, 3432) using default compute resource.
[0m11:58:43.965768 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore, thread: (27292, 27268), compute: ``
[0m11:58:43.966752 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m11:58:43.966752 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore, thread: (27292, 27268), compute: ``, acquire_release_count: 0, idle time: 0s
[0m11:58:43.967768 [debug] [ThreadPool]: Databricks adapter: Thread (27292, 27268) using default compute resource.
[0m11:58:43.967768 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m11:58:43.968772 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m11:58:43.968772 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:58:44.823305 [debug] [ThreadPool]: SQL status: OK in 0.8500000238418579 seconds
[0m11:58:44.835556 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore, thread: (27292, 27268), compute: ``, acquire_release_count: 1, idle time: 0s
[0m11:58:44.837543 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_snapshots)
[0m11:58:44.837543 [debug] [ThreadPool]: Databricks adapter: Reusing DatabricksDBTConnection. name: create_hive_metastore_snapshots, thread: (27292, 27268), compute: ``, acquire_release_count: 0, idle time: 0.0009822845458984375s
[0m11:58:44.839068 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: create_hive_metastore_snapshots, thread: (27292, 27268), compute: ``, acquire_release_count: 0, idle time: 0.0009822845458984375s
[0m11:58:44.839068 [debug] [ThreadPool]: Databricks adapter: Thread (27292, 27268) using default compute resource.
[0m11:58:44.840100 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: create_hive_metastore_snapshots, thread: (27292, 27268), compute: ``, acquire_release_count: 1, idle time: 0.0035393238067626953s
[0m11:58:44.840100 [debug] [ThreadPool]: Databricks adapter: Thread (27292, 27268) using default compute resource.
[0m11:58:44.841100 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "snapshots"
"
[0m11:58:44.859085 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:58:44.859085 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_snapshots"
[0m11:58:44.860083 [debug] [ThreadPool]: On create_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "create_hive_metastore_snapshots"} */
create schema if not exists `hive_metastore`.`snapshots`
  
[0m11:58:45.415427 [debug] [ThreadPool]: SQL status: OK in 0.550000011920929 seconds
[0m11:58:45.417444 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m11:58:45.418444 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: create_hive_metastore_snapshots, thread: (27292, 27268), compute: ``, acquire_release_count: 2, idle time: 0.5818839073181152s
[0m11:58:45.419443 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: create_hive_metastore_snapshots, thread: (27292, 27268), compute: ``, acquire_release_count: 1, idle time: 0.5828821659088135s
[0m11:58:45.422436 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_snapshots, thread: (27292, 6492), compute: ``
[0m11:58:45.423452 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m11:58:45.423452 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_snapshots, thread: (27292, 6492), compute: ``, acquire_release_count: 0, idle time: 0s
[0m11:58:45.424438 [debug] [ThreadPool]: Databricks adapter: Thread (27292, 6492) using default compute resource.
[0m11:58:45.431958 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:58:45.431958 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m11:58:45.432959 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:58:45.939786 [debug] [ThreadPool]: SQL status: OK in 0.5099999904632568 seconds
[0m11:58:45.945772 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_snapshots, thread: (27292, 6492), compute: ``, acquire_release_count: 1, idle time: 0s
[0m11:58:45.947772 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m11:58:45.948771 [debug] [ThreadPool]: Databricks adapter: Reusing DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (27292, 6492), compute: ``, acquire_release_count: 0, idle time: 0.0019996166229248047s
[0m11:58:45.948771 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (27292, 6492), compute: ``, acquire_release_count: 0, idle time: 0.0019996166229248047s
[0m11:58:45.949771 [debug] [ThreadPool]: Databricks adapter: Thread (27292, 6492) using default compute resource.
[0m11:58:45.952768 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:58:45.953771 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m11:58:46.083884 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m11:58:46.096862 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:58:46.097862 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:58:46.097862 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m11:58:46.312507 [debug] [ThreadPool]: SQL status: OK in 0.20999999344348907 seconds
[0m11:58:46.324531 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:58:46.324531 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m11:58:46.591755 [debug] [ThreadPool]: SQL status: OK in 0.27000001072883606 seconds
[0m11:58:46.605763 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:58:46.605763 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show table extended in `hive_metastore`.`saleslt` like '*'
  
[0m11:58:47.088217 [debug] [ThreadPool]: SQL status: OK in 0.47999998927116394 seconds
[0m11:58:47.094057 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (27292, 6492), compute: ``, acquire_release_count: 1, idle time: 1.1472859382629395s
[0m11:58:47.099040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'de815c2a-a852-499e-a47d-e123e1ac7ed1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F130E3DA90>]}
[0m11:58:47.099040 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:58:47.100042 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:58:47.100042 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (27292, 3432), compute: ``, acquire_release_count: 1, idle time: 0s
[0m11:58:47.101042 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:58:47.102043 [info ] [MainThread]: 
[0m11:58:47.108045 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.address_snapshot
[0m11:58:47.109038 [info ] [Thread-1  ]: 1 of 8 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m11:58:47.110053 [debug] [Thread-1  ]: Databricks adapter: Creating DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.address_snapshot, thread: (27292, 28040), compute: ``
[0m11:58:47.112042 [debug] [Thread-1  ]: Acquiring new databricks connection 'snapshot.dbt_spark_modeling.address_snapshot'
[0m11:58:47.113042 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.address_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0s
[0m11:58:47.114042 [debug] [Thread-1  ]: Databricks adapter: On thread (27292, 28040): `hive_metastore`.`snapshots`.`address_snapshot` using default compute resource.
[0m11:58:47.115039 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.address_snapshot
[0m11:58:47.131050 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.address_snapshot (compile): 11:58:47.116043 => 11:58:47.130051
[0m11:58:47.132054 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.address_snapshot
[0m11:58:47.296677 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.dbt_spark_modeling.address_snapshot"
[0m11:58:47.298656 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m11:58:47.298656 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.address_snapshot"
[0m11:58:47.299657 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.address_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`address_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/address/address_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) sbq



  
      
[0m11:58:47.299657 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m11:58:59.969981 [debug] [Thread-1  ]: SQL status: OK in 12.670000076293945 seconds
[0m11:59:00.080587 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m11:59:00.082588 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.address_snapshot (execute): 11:58:47.132054 => 11:59:00.082588
[0m11:59:00.083588 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.address_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 1, idle time: 0s
[0m11:59:00.084588 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.address_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.0009999275207519531s
[0m11:59:00.085589 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'de815c2a-a852-499e-a47d-e123e1ac7ed1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F130EF8DF0>]}
[0m11:59:00.086588 [info ] [Thread-1  ]: 1 of 8 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 12.98s]
[0m11:59:00.087587 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.address_snapshot
[0m11:59:00.088599 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.customer_snapshot
[0m11:59:00.089599 [info ] [Thread-1  ]: 2 of 8 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m11:59:00.090588 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.address_snapshot, now snapshot.dbt_spark_modeling.customer_snapshot)
[0m11:59:00.091582 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.customer_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.006993770599365234s
[0m11:59:00.092584 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.customer_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.00799560546875s
[0m11:59:00.093580 [debug] [Thread-1  ]: Databricks adapter: On thread (27292, 28040): `hive_metastore`.`snapshots`.`customer_snapshot` using default compute resource.
[0m11:59:00.093580 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.customer_snapshot
[0m11:59:00.099582 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.customer_snapshot (compile): 11:59:00.094586 => 11:59:00.098583
[0m11:59:00.099582 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.customer_snapshot
[0m11:59:00.112581 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.dbt_spark_modeling.customer_snapshot"
[0m11:59:00.121093 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customer_snapshot"
[0m11:59:00.122121 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customer_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`customer_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/customer_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) sbq



  
      
[0m11:59:05.078382 [debug] [Thread-1  ]: SQL status: OK in 4.949999809265137 seconds
[0m11:59:05.082382 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m11:59:05.083393 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.customer_snapshot (execute): 11:59:00.100581 => 11:59:05.083393
[0m11:59:05.084393 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.customer_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 1, idle time: 4.999804496765137s
[0m11:59:05.085397 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.customer_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.0010042190551757812s
[0m11:59:05.086397 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'de815c2a-a852-499e-a47d-e123e1ac7ed1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F130FF6A00>]}
[0m11:59:05.087397 [info ] [Thread-1  ]: 2 of 8 OK snapshotted snapshots.customer_snapshot .............................. [[32mOK[0m in 5.00s]
[0m11:59:05.088397 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.customer_snapshot
[0m11:59:05.089397 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.customeraddress_snapshot
[0m11:59:05.090382 [info ] [Thread-1  ]: 3 of 8 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m11:59:05.091378 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.customer_snapshot, now snapshot.dbt_spark_modeling.customeraddress_snapshot)
[0m11:59:05.092378 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.customeraddress_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.006981372833251953s
[0m11:59:05.093377 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.customeraddress_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.0079803466796875s
[0m11:59:05.094379 [debug] [Thread-1  ]: Databricks adapter: On thread (27292, 28040): `hive_metastore`.`snapshots`.`customeraddress_snapshot` using default compute resource.
[0m11:59:05.095378 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.customeraddress_snapshot
[0m11:59:05.102376 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.customeraddress_snapshot (compile): 11:59:05.095378 => 11:59:05.101375
[0m11:59:05.102376 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.customeraddress_snapshot
[0m11:59:05.116393 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.dbt_spark_modeling.customeraddress_snapshot"
[0m11:59:05.117397 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.customeraddress_snapshot"
[0m11:59:05.118380 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.customeraddress_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customeraddress/customeraddress_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) sbq



  
      
[0m11:59:08.968290 [debug] [Thread-1  ]: SQL status: OK in 3.8499999046325684 seconds
[0m11:59:08.972289 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m11:59:08.974287 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.customeraddress_snapshot (execute): 11:59:05.103393 => 11:59:08.973289
[0m11:59:08.974287 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.customeraddress_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 1, idle time: 3.888889789581299s
[0m11:59:08.975287 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.customeraddress_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m11:59:08.976306 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'de815c2a-a852-499e-a47d-e123e1ac7ed1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F130EC05B0>]}
[0m11:59:08.977291 [info ] [Thread-1  ]: 3 of 8 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 3.88s]
[0m11:59:08.979320 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.customeraddress_snapshot
[0m11:59:08.980307 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.product_snapshot
[0m11:59:08.981306 [info ] [Thread-1  ]: 4 of 8 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m11:59:08.982290 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.customeraddress_snapshot, now snapshot.dbt_spark_modeling.product_snapshot)
[0m11:59:08.983306 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.product_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.005984067916870117s
[0m11:59:08.983306 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.product_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.006999969482421875s
[0m11:59:08.984311 [debug] [Thread-1  ]: Databricks adapter: On thread (27292, 28040): `hive_metastore`.`snapshots`.`product_snapshot` using default compute resource.
[0m11:59:08.984311 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.product_snapshot
[0m11:59:08.991310 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.product_snapshot (compile): 11:59:08.985310 => 11:59:08.990306
[0m11:59:08.991310 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.product_snapshot
[0m11:59:09.005311 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.dbt_spark_modeling.product_snapshot"
[0m11:59:09.007301 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.product_snapshot"
[0m11:59:09.007301 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.product_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`product_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/product/product_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(ProductID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) sbq



  
      
[0m11:59:13.086380 [debug] [Thread-1  ]: SQL status: OK in 4.079999923706055 seconds
[0m11:59:13.090385 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m11:59:13.092383 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.product_snapshot (execute): 11:59:08.992310 => 11:59:13.092383
[0m11:59:13.093383 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.product_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 1, idle time: 4.116077184677124s
[0m11:59:13.094383 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.product_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m11:59:13.094383 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'de815c2a-a852-499e-a47d-e123e1ac7ed1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F131033D90>]}
[0m11:59:13.096384 [info ] [Thread-1  ]: 4 of 8 OK snapshotted snapshots.product_snapshot ............................... [[32mOK[0m in 4.11s]
[0m11:59:13.097383 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.product_snapshot
[0m11:59:13.098382 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.productcategory_snapshot
[0m11:59:13.099399 [info ] [Thread-1  ]: 5 of 8 START snapshot snapshots.productcategory_snapshot ....................... [RUN]
[0m11:59:13.100383 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.product_snapshot, now snapshot.dbt_spark_modeling.productcategory_snapshot)
[0m11:59:13.101383 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.productcategory_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.006000041961669922s
[0m11:59:13.101383 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.productcategory_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.006999969482421875s
[0m11:59:13.102383 [debug] [Thread-1  ]: Databricks adapter: On thread (27292, 28040): `hive_metastore`.`snapshots`.`productcategory_snapshot` using default compute resource.
[0m11:59:13.103383 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.productcategory_snapshot
[0m11:59:13.110383 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.productcategory_snapshot (compile): 11:59:13.103383 => 11:59:13.110383
[0m11:59:13.111383 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.productcategory_snapshot
[0m11:59:13.310514 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.dbt_spark_modeling.productcategory_snapshot"
[0m11:59:13.311514 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productcategory_snapshot"
[0m11:59:13.312514 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productcategory_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productcategory_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`productcategory_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/productcategory/productcategory_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(ProductCategoryID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with product_snapshot as (
    SELECT
        ProductCategoryID,
        Name
    FROM `hive_metastore`.`saleslt`.`productcategory`
    WHERE ParentProductCategoryID is not null
)

select * from product_snapshot

    ) sbq



  
      
[0m11:59:17.018300 [debug] [Thread-1  ]: SQL status: OK in 3.7100000381469727 seconds
[0m11:59:17.024314 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m11:59:17.027300 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.productcategory_snapshot (execute): 11:59:13.111383 => 11:59:17.026319
[0m11:59:17.027300 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.productcategory_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 1, idle time: 3.9329171180725098s
[0m11:59:17.029302 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.productcategory_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.0009937286376953125s
[0m11:59:17.030299 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'de815c2a-a852-499e-a47d-e123e1ac7ed1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F1310332B0>]}
[0m11:59:17.032297 [info ] [Thread-1  ]: 5 of 8 OK snapshotted snapshots.productcategory_snapshot ....................... [[32mOK[0m in 3.93s]
[0m11:59:17.034319 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.productcategory_snapshot
[0m11:59:17.035298 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.productmodel_snapshot
[0m11:59:17.036298 [info ] [Thread-1  ]: 6 of 8 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m11:59:17.037295 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.productcategory_snapshot, now snapshot.dbt_spark_modeling.productmodel_snapshot)
[0m11:59:17.038298 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.productmodel_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.007999181747436523s
[0m11:59:17.039296 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.productmodel_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.007999181747436523s
[0m11:59:17.039296 [debug] [Thread-1  ]: Databricks adapter: On thread (27292, 28040): `hive_metastore`.`snapshots`.`productmodel_snapshot` using default compute resource.
[0m11:59:17.040304 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.productmodel_snapshot
[0m11:59:17.049296 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.productmodel_snapshot (compile): 11:59:17.040304 => 11:59:17.048298
[0m11:59:17.050298 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.productmodel_snapshot
[0m11:59:17.066329 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.dbt_spark_modeling.productmodel_snapshot"
[0m11:59:17.068329 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.productmodel_snapshot"
[0m11:59:17.069311 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.productmodel_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`productmodel_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/productmodel/productmodel_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) sbq



  
      
[0m11:59:20.949369 [debug] [Thread-1  ]: SQL status: OK in 3.880000114440918 seconds
[0m11:59:20.953368 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m11:59:20.954369 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.productmodel_snapshot (execute): 11:59:17.050298 => 11:59:20.954369
[0m11:59:20.955368 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.productmodel_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 1, idle time: 3.9250693321228027s
[0m11:59:20.957372 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.productmodel_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.002003192901611328s
[0m11:59:20.958370 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'de815c2a-a852-499e-a47d-e123e1ac7ed1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F130EF8EB0>]}
[0m11:59:20.960368 [info ] [Thread-1  ]: 6 of 8 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 3.92s]
[0m11:59:20.961369 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.productmodel_snapshot
[0m11:59:20.962370 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.salesorderdetail_snapshot
[0m11:59:20.963368 [info ] [Thread-1  ]: 7 of 8 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m11:59:20.964372 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.productmodel_snapshot, now snapshot.dbt_spark_modeling.salesorderdetail_snapshot)
[0m11:59:20.965370 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.salesorderdetail_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.0070002079010009766s
[0m11:59:20.965370 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.salesorderdetail_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.0070002079010009766s
[0m11:59:20.966369 [debug] [Thread-1  ]: Databricks adapter: On thread (27292, 28040): `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` using default compute resource.
[0m11:59:20.966369 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.salesorderdetail_snapshot
[0m11:59:20.973382 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.salesorderdetail_snapshot (compile): 11:59:20.967369 => 11:59:20.972381
[0m11:59:20.974381 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.salesorderdetail_snapshot
[0m11:59:20.996241 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"
[0m11:59:20.997241 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"
[0m11:59:20.998243 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderdetail_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/salesorderdetail/salesorderdetail_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) sbq



  
      
[0m11:59:24.358327 [debug] [Thread-1  ]: SQL status: OK in 3.359999895095825 seconds
[0m11:59:24.361327 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m11:59:24.363330 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.salesorderdetail_snapshot (execute): 11:59:20.975380 => 11:59:24.363330
[0m11:59:24.364331 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.salesorderdetail_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 1, idle time: 3.4049606323242188s
[0m11:59:24.365330 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.salesorderdetail_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.0009989738464355469s
[0m11:59:24.365330 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'de815c2a-a852-499e-a47d-e123e1ac7ed1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F130F244F0>]}
[0m11:59:24.367330 [info ] [Thread-1  ]: 7 of 8 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 3.40s]
[0m11:59:24.368340 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.salesorderdetail_snapshot
[0m11:59:24.369340 [debug] [Thread-1  ]: Began running node snapshot.dbt_spark_modeling.salesorderheader_snapshot
[0m11:59:24.370328 [info ] [Thread-1  ]: 8 of 8 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m11:59:24.371330 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.dbt_spark_modeling.salesorderdetail_snapshot, now snapshot.dbt_spark_modeling.salesorderheader_snapshot)
[0m11:59:24.372340 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: snapshot.dbt_spark_modeling.salesorderheader_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.007010698318481445s
[0m11:59:24.372340 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: snapshot.dbt_spark_modeling.salesorderheader_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.007010698318481445s
[0m11:59:24.373344 [debug] [Thread-1  ]: Databricks adapter: On thread (27292, 28040): `hive_metastore`.`snapshots`.`salesorderheader_snapshot` using default compute resource.
[0m11:59:24.374344 [debug] [Thread-1  ]: Began compiling node snapshot.dbt_spark_modeling.salesorderheader_snapshot
[0m11:59:24.381357 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.salesorderheader_snapshot (compile): 11:59:24.374344 => 11:59:24.381357
[0m11:59:24.382372 [debug] [Thread-1  ]: Began executing node snapshot.dbt_spark_modeling.salesorderheader_snapshot
[0m11:59:24.393869 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.dbt_spark_modeling.salesorderheader_snapshot"
[0m11:59:24.394874 [debug] [Thread-1  ]: Using databricks connection "snapshot.dbt_spark_modeling.salesorderheader_snapshot"
[0m11:59:24.394874 [debug] [Thread-1  ]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "snapshot.dbt_spark_modeling.salesorderheader_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/salesorderheader/salesorderheader_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) sbq



  
      
[0m11:59:28.116978 [debug] [Thread-1  ]: SQL status: OK in 3.7200000286102295 seconds
[0m11:59:28.120978 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m11:59:28.121978 [debug] [Thread-1  ]: Timing info for snapshot.dbt_spark_modeling.salesorderheader_snapshot (execute): 11:59:24.383355 => 11:59:28.121978
[0m11:59:28.122978 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.salesorderheader_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 1, idle time: 3.75764799118042s
[0m11:59:28.123978 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: snapshot.dbt_spark_modeling.salesorderheader_snapshot, thread: (27292, 28040), compute: ``, acquire_release_count: 0, idle time: 0.0010004043579101562s
[0m11:59:28.124978 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'de815c2a-a852-499e-a47d-e123e1ac7ed1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F130FE0A00>]}
[0m11:59:28.125978 [info ] [Thread-1  ]: 8 of 8 OK snapshotted snapshots.salesorderheader_snapshot ...................... [[32mOK[0m in 3.75s]
[0m11:59:28.126959 [debug] [Thread-1  ]: Finished running node snapshot.dbt_spark_modeling.salesorderheader_snapshot
[0m11:59:28.129852 [debug] [MainThread]: Databricks adapter: Reusing DatabricksDBTConnection. name: master, thread: (27292, 3432), compute: ``, acquire_release_count: 0, idle time: 41.02778124809265s
[0m11:59:28.129852 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (27292, 3432), compute: ``, acquire_release_count: 0, idle time: 41.028809547424316s
[0m11:59:28.130851 [debug] [MainThread]: Databricks adapter: Thread (27292, 3432) using default compute resource.
[0m11:59:28.130851 [debug] [MainThread]: On master: ROLLBACK
[0m11:59:28.131851 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:59:28.533348 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:59:28.534347 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:59:28.535349 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:59:28.535349 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (27292, 3432), compute: ``, acquire_release_count: 1, idle time: 41.434306621551514s
[0m11:59:28.537363 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:59:28.537363 [debug] [MainThread]: On master: ROLLBACK
[0m11:59:28.538348 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:59:28.538348 [debug] [MainThread]: On master: Close
[0m11:59:28.651369 [debug] [MainThread]: Connection 'create_hive_metastore_snapshots' was properly closed.
[0m11:59:28.652366 [debug] [MainThread]: On create_hive_metastore_snapshots: ROLLBACK
[0m11:59:28.653367 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:59:28.653367 [debug] [MainThread]: On create_hive_metastore_snapshots: Close
[0m11:59:28.832528 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m11:59:28.833528 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m11:59:28.834532 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:59:28.834532 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m11:59:28.918319 [debug] [MainThread]: Connection 'snapshot.dbt_spark_modeling.salesorderheader_snapshot' was properly closed.
[0m11:59:28.919317 [debug] [MainThread]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: ROLLBACK
[0m11:59:28.920317 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:59:28.920317 [debug] [MainThread]: On snapshot.dbt_spark_modeling.salesorderheader_snapshot: Close
[0m11:59:29.039607 [info ] [MainThread]: 
[0m11:59:29.040588 [info ] [MainThread]: Finished running 8 snapshots in 0 hours 0 minutes and 45.08 seconds (45.08s).
[0m11:59:29.044620 [debug] [MainThread]: Command end result
[0m11:59:29.072192 [info ] [MainThread]: 
[0m11:59:29.073194 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:59:29.074192 [info ] [MainThread]: 
[0m11:59:29.075210 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=0 SKIP=0 TOTAL=8
[0m11:59:29.076195 [debug] [MainThread]: Command `dbt snapshot` succeeded at 11:59:29.076195 after 47.23 seconds
[0m11:59:29.077192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F10C8E72E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F131033D90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F130E2DE20>]}
[0m11:59:29.078195 [debug] [MainThread]: Flushing usage events
[0m11:59:43.194839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001673C836820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001673F814B50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001673F814C10>]}


============================== 11:59:43.199857 | fa0a0d20-514c-4274-97fb-6cc56afba930 ==============================
[0m11:59:43.199857 [info ] [MainThread]: Running with dbt=1.7.4
[0m11:59:43.200844 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:59:44.884745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fa0a0d20-514c-4274-97fb-6cc56afba930', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001673B7198B0>]}
[0m11:59:45.008324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fa0a0d20-514c-4274-97fb-6cc56afba930', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000167608F9A60>]}
[0m11:59:45.009327 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m11:59:45.024749 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m11:59:45.149297 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:59:45.150295 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:59:45.158296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fa0a0d20-514c-4274-97fb-6cc56afba930', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016760D8D0D0>]}
[0m11:59:45.181292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fa0a0d20-514c-4274-97fb-6cc56afba930', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016760D69F70>]}
[0m11:59:45.182294 [info ] [MainThread]: Found 8 snapshots, 3 models, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m11:59:45.183292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fa0a0d20-514c-4274-97fb-6cc56afba930', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016760D69B80>]}
[0m11:59:45.185308 [info ] [MainThread]: 
[0m11:59:45.187301 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (7412, 17968), compute: ``
[0m11:59:45.187301 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:59:45.188298 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (7412, 17968), compute: ``, acquire_release_count: 0, idle time: 0s
[0m11:59:45.188298 [debug] [MainThread]: Databricks adapter: Thread (7412, 17968) using default compute resource.
[0m11:59:45.191309 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore, thread: (7412, 23564), compute: ``
[0m11:59:45.192292 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m11:59:45.192292 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore, thread: (7412, 23564), compute: ``, acquire_release_count: 0, idle time: 0s
[0m11:59:45.193313 [debug] [ThreadPool]: Databricks adapter: Thread (7412, 23564) using default compute resource.
[0m11:59:45.194314 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m11:59:45.195294 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m11:59:45.196296 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:59:45.735483 [debug] [ThreadPool]: SQL status: OK in 0.5400000214576721 seconds
[0m11:59:45.741482 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore, thread: (7412, 23564), compute: ``, acquire_release_count: 1, idle time: 0s
[0m11:59:45.744478 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (7412, 28992), compute: ``
[0m11:59:45.745480 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m11:59:45.746494 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (7412, 28992), compute: ``, acquire_release_count: 0, idle time: 0s
[0m11:59:45.746494 [debug] [ThreadPool]: Databricks adapter: Thread (7412, 28992) using default compute resource.
[0m11:59:45.753498 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:59:45.753498 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m11:59:45.754498 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:59:46.350349 [debug] [ThreadPool]: SQL status: OK in 0.6000000238418579 seconds
[0m11:59:46.371350 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:59:46.372350 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:59:46.372350 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m11:59:46.652783 [debug] [ThreadPool]: SQL status: OK in 0.2800000011920929 seconds
[0m11:59:46.664777 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:59:46.665778 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m11:59:46.858288 [debug] [ThreadPool]: SQL status: OK in 0.1899999976158142 seconds
[0m11:59:46.870288 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m11:59:46.871288 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show table extended in `hive_metastore`.`saleslt` like '*'
  
[0m11:59:47.487866 [debug] [ThreadPool]: SQL status: OK in 0.6200000047683716 seconds
[0m11:59:47.492859 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (7412, 28992), compute: ``, acquire_release_count: 1, idle time: 0s
[0m11:59:47.494860 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m11:59:47.494860 [debug] [ThreadPool]: Databricks adapter: Reusing DatabricksDBTConnection. name: list_hive_metastore_snapshots, thread: (7412, 28992), compute: ``, acquire_release_count: 0, idle time: 0.002001523971557617s
[0m11:59:47.498859 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_snapshots, thread: (7412, 28992), compute: ``, acquire_release_count: 0, idle time: 0.005999565124511719s
[0m11:59:47.499866 [debug] [ThreadPool]: Databricks adapter: Thread (7412, 28992) using default compute resource.
[0m11:59:47.502862 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:59:47.503863 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m11:59:47.682613 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m11:59:47.692617 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:59:47.693617 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m11:59:47.880879 [debug] [ThreadPool]: SQL status: OK in 0.1899999976158142 seconds
[0m11:59:47.888885 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:59:47.888885 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m11:59:48.087448 [debug] [ThreadPool]: SQL status: OK in 0.20000000298023224 seconds
[0m11:59:48.097447 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m11:59:48.097447 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show table extended in `hive_metastore`.`snapshots` like '*'
  
[0m11:59:48.812314 [debug] [ThreadPool]: SQL status: OK in 0.7099999785423279 seconds
[0m11:59:48.818322 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_snapshots, thread: (7412, 28992), compute: ``, acquire_release_count: 1, idle time: 1.3254625797271729s
[0m11:59:48.821321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fa0a0d20-514c-4274-97fb-6cc56afba930', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016760AAC490>]}
[0m11:59:48.822325 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:59:48.823325 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:59:48.823325 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (7412, 17968), compute: ``, acquire_release_count: 1, idle time: 0s
[0m11:59:48.824323 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:59:48.825341 [info ] [MainThread]: 
[0m11:59:48.830669 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.dim_customer
[0m11:59:48.831694 [info ] [Thread-1  ]: 1 of 3 START sql table model saleslt.dim_customer .............................. [RUN]
[0m11:59:48.833694 [debug] [Thread-1  ]: Databricks adapter: Creating DatabricksDBTConnection. name: model.dbt_spark_modeling.dim_customer, thread: (7412, 10120), compute: ``
[0m11:59:48.834698 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbt_spark_modeling.dim_customer'
[0m11:59:48.834698 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.dim_customer, thread: (7412, 10120), compute: ``, acquire_release_count: 0, idle time: 0s
[0m11:59:48.835697 [debug] [Thread-1  ]: Databricks adapter: On thread (7412, 10120): `hive_metastore`.`saleslt`.`dim_customer` using default compute resource.
[0m11:59:48.835697 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.dim_customer
[0m11:59:48.848695 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.dim_customer"
[0m11:59:48.850694 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_customer (compile): 11:59:48.836697 => 11:59:48.849696
[0m11:59:48.850694 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.dim_customer
[0m11:59:48.928304 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.dim_customer"
[0m11:59:48.930284 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m11:59:48.931287 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_customer"
[0m11:59:48.931287 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_customer: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerID,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
),

, transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId as CustomerID,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)

select *
from transformed
  
[0m11:59:48.932304 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m11:59:49.622671 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerID,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
),

, transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId as CustomerID,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)

select *
from transformed
  
[0m11:59:49.624194 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ','.(line 47, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerID,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
),

, transformed as (
^^^
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId as CustomerID,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)

select *
from transformed
  

[0m11:59:49.624708 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ','.(line 47, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerID,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
),

, transformed as (
^^^
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId as CustomerID,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)

select *
from transformed
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ','.(line 47, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerID,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
),

, transformed as (
^^^
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId as CustomerID,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)

select *
from transformed
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:259)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:111)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:541)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more

[0m11:59:49.625722 [debug] [Thread-1  ]: Databricks adapter: operation-id: 81b0c11a-74ef-42bb-b736-814bead016f6
[0m11:59:49.626806 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_customer (execute): 11:59:48.851697 => 11:59:49.626806
[0m11:59:49.628318 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_customer, thread: (7412, 10120), compute: ``, acquire_release_count: 1, idle time: 0s
[0m11:59:49.793848 [debug] [Thread-1  ]: Runtime Error in model dim_customer (models\marts\customer\dim_customer.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ','.(line 47, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_customer"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_customer`
        
        
      using delta
        
        
        
        
        
      location '/mnt/gold/customers/dim_customer'
        
        
        as
        
  
  with address_snapshot as (
      select
          AddressID,
          AddressLine1,
          AddressLine2,
          City,
          StateProvince,
          CountryRegion,
          PostalCode
      from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
  )
  
  , customeraddress_snapshot as (
      select
          CustomerID,
          AddressId,
          AddressType
      from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
  )
  
  , customer_snapshot as (
      select
          CustomerId,
          concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
      from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
  ),
  
  , transformed as (
  ^^^
      select
      row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
      customer_snapshot.CustomerId as CustomerID,
      customer_snapshot.fullname,
      customeraddress_snapshot.AddressID,
      customeraddress_snapshot.AddressType,
      address_snapshot.AddressLine1,
      address_snapshot.City,
      address_snapshot.StateProvince,
      address_snapshot.CountryRegion,
      address_snapshot.PostalCode
      from customer_snapshot
      inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
      inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
  )
  
  select *
  from transformed
    
  
[0m11:59:49.794846 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_customer, thread: (7412, 10120), compute: ``, acquire_release_count: 0, idle time: 0.16652870178222656s
[0m11:59:49.795847 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fa0a0d20-514c-4274-97fb-6cc56afba930', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016760E129A0>]}
[0m11:59:49.796849 [error] [Thread-1  ]: 1 of 3 ERROR creating sql table model saleslt.dim_customer ..................... [[31mERROR[0m in 0.96s]
[0m11:59:49.799847 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.dim_customer
[0m11:59:49.800851 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.dim_product
[0m11:59:49.801843 [info ] [Thread-1  ]: 2 of 3 START sql table model saleslt.dim_product ............................... [RUN]
[0m11:59:49.802846 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.dbt_spark_modeling.dim_customer, now model.dbt_spark_modeling.dim_product)
[0m11:59:49.803846 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: model.dbt_spark_modeling.dim_product, thread: (7412, 10120), compute: ``, acquire_release_count: 0, idle time: 0.00799870491027832s
[0m11:59:49.804845 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.dim_product, thread: (7412, 10120), compute: ``, acquire_release_count: 0, idle time: 0.00799870491027832s
[0m11:59:49.804845 [debug] [Thread-1  ]: Databricks adapter: On thread (7412, 10120): `hive_metastore`.`saleslt`.`dim_product` using default compute resource.
[0m11:59:49.805845 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.dim_product
[0m11:59:49.826860 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.dim_product"
[0m11:59:49.835288 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (compile): 11:59:49.805845 => 11:59:49.834778
[0m11:59:49.836303 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.dim_product
[0m11:59:49.868298 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.dim_product"
[0m11:59:49.870301 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_product"
[0m11:59:49.871305 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_product: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),

productcategory_snapshot as (
    select
        ProductCategoryID,
        Name,
        row_number() over (order by ProductCategoryID) as category_id
    from `hive_metastore`.`snapshots`.`productcategory_snapshot`
),

transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.productId as productID,
        p.name as product_name,
        pc.name as product_category,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
    left join productcategory_snapshot pc on p.productcategoryid=pc.ProductCategoryID
)

select * from transformed
  
[0m11:59:56.279434 [debug] [Thread-1  ]: SQL status: OK in 6.409999847412109 seconds
[0m11:59:56.580021 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_product (execute): 11:59:49.836303 => 11:59:56.579020
[0m11:59:56.580021 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (7412, 10120), compute: ``, acquire_release_count: 1, idle time: 6.784174203872681s
[0m11:59:56.581025 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_product, thread: (7412, 10120), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m11:59:56.582021 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fa0a0d20-514c-4274-97fb-6cc56afba930', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001676102D0A0>]}
[0m11:59:56.583025 [info ] [Thread-1  ]: 2 of 3 OK created sql table model saleslt.dim_product .......................... [[32mOK[0m in 6.78s]
[0m11:59:56.584008 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.dim_product
[0m11:59:56.585006 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.sales
[0m11:59:56.586008 [info ] [Thread-1  ]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m11:59:56.586781 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.dbt_spark_modeling.dim_product, now model.dbt_spark_modeling.sales)
[0m11:59:56.587808 [debug] [Thread-1  ]: Databricks adapter: Reusing DatabricksDBTConnection. name: model.dbt_spark_modeling.sales, thread: (7412, 10120), compute: ``, acquire_release_count: 0, idle time: 0.0057866573333740234s
[0m11:59:56.588806 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.sales, thread: (7412, 10120), compute: ``, acquire_release_count: 0, idle time: 0.0057866573333740234s
[0m11:59:56.588806 [debug] [Thread-1  ]: Databricks adapter: On thread (7412, 10120): `hive_metastore`.`saleslt`.`sales` using default compute resource.
[0m11:59:56.589806 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.sales
[0m11:59:56.595806 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.sales"
[0m11:59:56.596805 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.sales (compile): 11:59:56.589806 => 11:59:56.596805
[0m11:59:56.597806 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.sales
[0m11:59:56.607826 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.sales"
[0m11:59:56.608826 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.sales"
[0m11:59:56.609812 [debug] [Thread-1  ]: On model.dbt_spark_modeling.sales: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.ProductID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        soh.CustomerID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
[0m12:00:02.216785 [debug] [Thread-1  ]: SQL status: OK in 5.610000133514404 seconds
[0m12:00:02.322135 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.sales (execute): 11:59:56.598810 => 12:00:02.321124
[0m12:00:02.323135 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.sales, thread: (7412, 10120), compute: ``, acquire_release_count: 1, idle time: 5.7401134967803955s
[0m12:00:02.324135 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.sales, thread: (7412, 10120), compute: ``, acquire_release_count: 0, idle time: 0.0010004043579101562s
[0m12:00:02.324135 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fa0a0d20-514c-4274-97fb-6cc56afba930', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016760E85D30>]}
[0m12:00:02.325135 [info ] [Thread-1  ]: 3 of 3 OK created sql table model saleslt.sales ................................ [[32mOK[0m in 5.74s]
[0m12:00:02.327141 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.sales
[0m12:00:02.330136 [debug] [MainThread]: Databricks adapter: Reusing DatabricksDBTConnection. name: master, thread: (7412, 17968), compute: ``, acquire_release_count: 0, idle time: 13.504810810089111s
[0m12:00:02.330136 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (7412, 17968), compute: ``, acquire_release_count: 0, idle time: 13.505812883377075s
[0m12:00:02.331176 [debug] [MainThread]: Databricks adapter: Thread (7412, 17968) using default compute resource.
[0m12:00:02.331176 [debug] [MainThread]: On master: ROLLBACK
[0m12:00:02.332140 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:00:02.831218 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:00:02.831218 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:00:02.832219 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:00:02.832219 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (7412, 17968), compute: ``, acquire_release_count: 1, idle time: 14.007895946502686s
[0m12:00:02.834216 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:00:02.835220 [debug] [MainThread]: On master: ROLLBACK
[0m12:00:02.835220 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:00:02.835220 [debug] [MainThread]: On master: Close
[0m12:00:02.954071 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m12:00:02.954071 [debug] [MainThread]: On list_hive_metastore: Close
[0m12:00:03.034174 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m12:00:03.034174 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m12:00:03.035197 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:00:03.035197 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m12:00:03.136545 [debug] [MainThread]: Connection 'model.dbt_spark_modeling.sales' was properly closed.
[0m12:00:03.137557 [debug] [MainThread]: On model.dbt_spark_modeling.sales: ROLLBACK
[0m12:00:03.138558 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:00:03.138558 [debug] [MainThread]: On model.dbt_spark_modeling.sales: Close
[0m12:00:03.221331 [info ] [MainThread]: 
[0m12:00:03.222342 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 18.03 seconds (18.03s).
[0m12:00:03.224346 [debug] [MainThread]: Command end result
[0m12:00:03.245867 [info ] [MainThread]: 
[0m12:00:03.249865 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m12:00:03.251378 [info ] [MainThread]: 
[0m12:00:03.252905 [error] [MainThread]:   Runtime Error in model dim_customer (models\marts\customer\dim_customer.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ','.(line 47, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_customer"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_customer`
        
        
      using delta
        
        
        
        
        
      location '/mnt/gold/customers/dim_customer'
        
        
        as
        
  
  with address_snapshot as (
      select
          AddressID,
          AddressLine1,
          AddressLine2,
          City,
          StateProvince,
          CountryRegion,
          PostalCode
      from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
  )
  
  , customeraddress_snapshot as (
      select
          CustomerID,
          AddressId,
          AddressType
      from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
  )
  
  , customer_snapshot as (
      select
          CustomerId,
          concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
      from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
  ),
  
  , transformed as (
  ^^^
      select
      row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
      customer_snapshot.CustomerId as CustomerID,
      customer_snapshot.fullname,
      customeraddress_snapshot.AddressID,
      customeraddress_snapshot.AddressType,
      address_snapshot.AddressLine1,
      address_snapshot.City,
      address_snapshot.StateProvince,
      address_snapshot.CountryRegion,
      address_snapshot.PostalCode
      from customer_snapshot
      inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
      inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
  )
  
  select *
  from transformed
    
  
[0m12:00:03.254426 [info ] [MainThread]: 
[0m12:00:03.255433 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
[0m12:00:03.257434 [debug] [MainThread]: Command `dbt run` failed at 12:00:03.256430 after 20.12 seconds
[0m12:00:03.257434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001673C836820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000167609E4790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016760E51340>]}
[0m12:00:03.258433 [debug] [MainThread]: Flushing usage events
[0m12:00:37.414228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205E92862E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205EC272760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205EC2726D0>]}


============================== 12:00:37.419245 | 2c4771b1-a37f-4d70-ab60-ca230a2d0412 ==============================
[0m12:00:37.419245 [info ] [MainThread]: Running with dbt=1.7.4
[0m12:00:37.420229 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\EliteSniper\\.dbt', 'log_path': 'D:\\DataEngineering\\Data_Engineering_Projects\\azure-dbt-spark\\dbt_spark_modeling\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select dim_customer.sql', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:00:39.116747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2c4771b1-a37f-4d70-ab60-ca230a2d0412', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205E8169910>]}
[0m12:00:39.241243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2c4771b1-a37f-4d70-ab60-ca230a2d0412', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002058D4A5430>]}
[0m12:00:39.242243 [info ] [MainThread]: Registered adapter: databricks=1.7.3
[0m12:00:39.258243 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m12:00:39.382369 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:00:39.383365 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_modeling://models\marts\customer\dim_customer.sql
[0m12:00:39.580481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2c4771b1-a37f-4d70-ab60-ca230a2d0412', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002058D96E0A0>]}
[0m12:00:39.599482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2c4771b1-a37f-4d70-ab60-ca230a2d0412', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002058D6C9A60>]}
[0m12:00:39.600483 [info ] [MainThread]: Found 8 snapshots, 3 models, 9 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m12:00:39.601492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c4771b1-a37f-4d70-ab60-ca230a2d0412', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002058D6C9850>]}
[0m12:00:39.603496 [info ] [MainThread]: 
[0m12:00:39.604483 [debug] [MainThread]: Databricks adapter: Creating DatabricksDBTConnection. name: master, thread: (13976, 3220), compute: ``
[0m12:00:39.605481 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:00:39.605481 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (13976, 3220), compute: ``, acquire_release_count: 0, idle time: 0s
[0m12:00:39.606481 [debug] [MainThread]: Databricks adapter: Thread (13976, 3220) using default compute resource.
[0m12:00:39.608492 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore, thread: (13976, 25808), compute: ``
[0m12:00:39.609492 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m12:00:39.609492 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore, thread: (13976, 25808), compute: ``, acquire_release_count: 0, idle time: 0s
[0m12:00:39.609492 [debug] [ThreadPool]: Databricks adapter: Thread (13976, 25808) using default compute resource.
[0m12:00:39.610496 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m12:00:39.610496 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m12:00:39.611496 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:00:40.212989 [debug] [ThreadPool]: SQL status: OK in 0.6000000238418579 seconds
[0m12:00:40.217995 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore, thread: (13976, 25808), compute: ``, acquire_release_count: 1, idle time: 0s
[0m12:00:40.221993 [debug] [ThreadPool]: Databricks adapter: Creating DatabricksDBTConnection. name: list_hive_metastore_snapshots, thread: (13976, 10228), compute: ``
[0m12:00:40.221993 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m12:00:40.222993 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_snapshots, thread: (13976, 10228), compute: ``, acquire_release_count: 0, idle time: 0s
[0m12:00:40.223995 [debug] [ThreadPool]: Databricks adapter: Thread (13976, 10228) using default compute resource.
[0m12:00:40.229989 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m12:00:40.230990 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m12:00:40.230990 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:00:40.724139 [debug] [ThreadPool]: SQL status: OK in 0.49000000953674316 seconds
[0m12:00:40.750143 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:00:40.751145 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m12:00:40.751145 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m12:00:40.927870 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m12:00:40.940876 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m12:00:40.940876 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m12:00:41.310138 [debug] [ThreadPool]: SQL status: OK in 0.3700000047683716 seconds
[0m12:00:41.323140 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m12:00:41.324141 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show table extended in `hive_metastore`.`snapshots` like '*'
  
[0m12:00:41.967522 [debug] [ThreadPool]: SQL status: OK in 0.6399999856948853 seconds
[0m12:00:41.972532 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_snapshots, thread: (13976, 10228), compute: ``, acquire_release_count: 1, idle time: 0s
[0m12:00:41.974532 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m12:00:41.977537 [debug] [ThreadPool]: Databricks adapter: Reusing DatabricksDBTConnection. name: list_hive_metastore_saleslt, thread: (13976, 10228), compute: ``, acquire_release_count: 0, idle time: 0.004004716873168945s
[0m12:00:41.979535 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._acquire: name: list_hive_metastore_saleslt, thread: (13976, 10228), compute: ``, acquire_release_count: 0, idle time: 0.0050029754638671875s
[0m12:00:41.980536 [debug] [ThreadPool]: Databricks adapter: Thread (13976, 10228) using default compute resource.
[0m12:00:41.986059 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m12:00:41.987060 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m12:00:42.125881 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m12:00:42.135880 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m12:00:42.135880 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m12:00:42.267254 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m12:00:42.274266 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m12:00:42.275270 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m12:00:42.461747 [debug] [ThreadPool]: SQL status: OK in 0.1899999976158142 seconds
[0m12:00:42.469756 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m12:00:42.470757 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show table extended in `hive_metastore`.`saleslt` like '*'
  
[0m12:00:42.978364 [debug] [ThreadPool]: SQL status: OK in 0.5099999904632568 seconds
[0m12:00:42.983358 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection._release: name: list_hive_metastore_saleslt, thread: (13976, 10228), compute: ``, acquire_release_count: 1, idle time: 1.0098252296447754s
[0m12:00:42.988878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c4771b1-a37f-4d70-ab60-ca230a2d0412', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002058D41CE50>]}
[0m12:00:42.988878 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:00:42.989884 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:00:42.990881 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (13976, 3220), compute: ``, acquire_release_count: 1, idle time: 0s
[0m12:00:42.991885 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:00:42.992881 [info ] [MainThread]: 
[0m12:00:42.996878 [debug] [Thread-1  ]: Began running node model.dbt_spark_modeling.dim_customer
[0m12:00:42.997879 [info ] [Thread-1  ]: 1 of 1 START sql table model saleslt.dim_customer .............................. [RUN]
[0m12:00:42.999885 [debug] [Thread-1  ]: Databricks adapter: Creating DatabricksDBTConnection. name: model.dbt_spark_modeling.dim_customer, thread: (13976, 26572), compute: ``
[0m12:00:42.999885 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbt_spark_modeling.dim_customer'
[0m12:00:43.000885 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._acquire: name: model.dbt_spark_modeling.dim_customer, thread: (13976, 26572), compute: ``, acquire_release_count: 0, idle time: 0s
[0m12:00:43.001882 [debug] [Thread-1  ]: Databricks adapter: On thread (13976, 26572): `hive_metastore`.`saleslt`.`dim_customer` using default compute resource.
[0m12:00:43.002878 [debug] [Thread-1  ]: Began compiling node model.dbt_spark_modeling.dim_customer
[0m12:00:43.022884 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_spark_modeling.dim_customer"
[0m12:00:43.024880 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_customer (compile): 12:00:43.002878 => 12:00:43.024880
[0m12:00:43.025881 [debug] [Thread-1  ]: Began executing node model.dbt_spark_modeling.dim_customer
[0m12:00:43.108404 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_spark_modeling.dim_customer"
[0m12:00:43.110406 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m12:00:43.110406 [debug] [Thread-1  ]: Using databricks connection "model.dbt_spark_modeling.dim_customer"
[0m12:00:43.111407 [debug] [Thread-1  ]: On model.dbt_spark_modeling.dim_customer: /* {"app": "dbt", "dbt_version": "1.7.4", "dbt_databricks_version": "1.7.3", "databricks_sql_connector_version": "2.9.3", "profile_name": "dbt_spark_modeling", "target_name": "dev", "node_id": "model.dbt_spark_modeling.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerID,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId as CustomerID,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)

select *
from transformed
  
[0m12:00:43.112408 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m12:00:49.526716 [debug] [Thread-1  ]: SQL status: OK in 6.420000076293945 seconds
[0m12:00:49.661334 [debug] [Thread-1  ]: Timing info for model.dbt_spark_modeling.dim_customer (execute): 12:00:43.026882 => 12:00:49.661334
[0m12:00:49.662334 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_customer, thread: (13976, 26572), compute: ``, acquire_release_count: 1, idle time: 0s
[0m12:00:49.663334 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection._release: name: model.dbt_spark_modeling.dim_customer, thread: (13976, 26572), compute: ``, acquire_release_count: 0, idle time: 0.0s
[0m12:00:49.664334 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2c4771b1-a37f-4d70-ab60-ca230a2d0412', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002058D7F4BB0>]}
[0m12:00:49.665334 [info ] [Thread-1  ]: 1 of 1 OK created sql table model saleslt.dim_customer ......................... [[32mOK[0m in 6.67s]
[0m12:00:49.666315 [debug] [Thread-1  ]: Finished running node model.dbt_spark_modeling.dim_customer
[0m12:00:49.668348 [debug] [MainThread]: Databricks adapter: Reusing DatabricksDBTConnection. name: master, thread: (13976, 3220), compute: ``, acquire_release_count: 0, idle time: 6.677467584609985s
[0m12:00:49.669349 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._acquire: name: master, thread: (13976, 3220), compute: ``, acquire_release_count: 0, idle time: 6.678467750549316s
[0m12:00:49.669349 [debug] [MainThread]: Databricks adapter: Thread (13976, 3220) using default compute resource.
[0m12:00:49.670350 [debug] [MainThread]: On master: ROLLBACK
[0m12:00:49.670350 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:00:50.040826 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:00:50.040826 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:00:50.041826 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:00:50.041826 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection._release: name: master, thread: (13976, 3220), compute: ``, acquire_release_count: 1, idle time: 7.050945281982422s
[0m12:00:50.042826 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:00:50.043832 [debug] [MainThread]: On master: ROLLBACK
[0m12:00:50.043832 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:00:50.044833 [debug] [MainThread]: On master: Close
[0m12:00:50.141595 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m12:00:50.141595 [debug] [MainThread]: On list_hive_metastore: Close
[0m12:00:50.242094 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m12:00:50.243095 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m12:00:50.243095 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:00:50.244095 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m12:00:50.448298 [debug] [MainThread]: Connection 'model.dbt_spark_modeling.dim_customer' was properly closed.
[0m12:00:50.448298 [debug] [MainThread]: On model.dbt_spark_modeling.dim_customer: ROLLBACK
[0m12:00:50.449298 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:00:50.449298 [debug] [MainThread]: On model.dbt_spark_modeling.dim_customer: Close
[0m12:00:50.530685 [info ] [MainThread]: 
[0m12:00:50.531740 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 10.93 seconds (10.93s).
[0m12:00:50.532688 [debug] [MainThread]: Command end result
[0m12:00:50.559692 [info ] [MainThread]: 
[0m12:00:50.560689 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:00:50.561688 [info ] [MainThread]: 
[0m12:00:50.562686 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m12:00:50.565685 [debug] [MainThread]: Command `dbt run` succeeded at 12:00:50.565685 after 13.21 seconds
[0m12:00:50.566690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205E92862E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002058D38C250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002058D981040>]}
[0m12:00:50.567692 [debug] [MainThread]: Flushing usage events
